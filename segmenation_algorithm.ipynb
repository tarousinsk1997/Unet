{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Описание подходов для решения задачи сегментации фасадов жилых домов и подсчёта количества окон</h1>\n",
    "\n",
    "Для решения задачи будем использовать архитектуру Resnet18, хорошо зарекомендовавшуюся себя в задачах семантической сегментации изображений.\n",
    "\n",
    "1. Характеристики входного изображения (С x W x H) - 1 x 128 x 128 (grayscale)\n",
    "2. Размеченные данные - (С x W x H) - 2 x 128 x 128 (два класса [0,1], 0 - background, 1 - windows)\n",
    "3. При достаточной хорошей обучаемости Unet, для определения контуров окон можно использовать The marching squares algorithm. (The marching squares algorithm is a special case of the marching cubes algorithm (Lorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170).)\n",
    "\n",
    "The marching squares algorithm возвращает требуемое количество контуров, соответствующих окнам, а также индексы пикселей, соответствующие центрам контуров, чтобы можно было посчитать сетку колонн/рядов окон.\n",
    "\n",
    "Работа алгоритма:\n",
    "\n",
    "<image src=\"https://scikit-image.org/docs/stable/_images/sphx_glr_plot_contours_001.png\" alt=\"Описание картинки\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary\n",
    "import torchmetrics.classification as tmc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import cv2 \n",
    "import albumentations as A\n",
    "from skimage import measure # measure.find_contours() - The marching squares algorithm\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import segmentation_models_pytorch as smp\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dataset: CMP_facade_DB_base\n",
    "https://cmp.felk.cvut.cz/~tylecr1/facade/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IMAGE_WIDTH = 128\n",
    "TARGET_IMAGE_HEIGHT = 128\n",
    "NUM_CLASSES = 2 \n",
    "NUM_COLORS = 1\n",
    "BATCH_SIZE = 64\n",
    "EXECUTION_IMAGE_DATA = True\n",
    "EXECUTION_MODEL_TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "img_trainA_path = os.path.join(cwd, r'dataset\\trainA')\n",
    "img_testA_path = os.path.join(cwd, r'dataset\\testA')\n",
    "img_trainB_path = os.path.join(cwd, r'dataset\\trainB')\n",
    "img_testB_path = os.path.join(cwd, r'dataset\\testB')\n",
    "npy_path = os.path.join(cwd, r'dataset\\np.array_targets')\n",
    "images_path = os.path.join(cwd, r'dataset\\images')\n",
    "masks_path = os.path.join(cwd, r'dataset\\masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(img_folder, mask_folder, num_of_augmentaions):\n",
    "\n",
    "    \"\"\"image transformations\"\"\"\n",
    "    \n",
    "    transform = A.Compose([A.VerticalFlip(),\n",
    "                            A.Rotate(),\n",
    "                            A.HorizontalFlip(),\n",
    "                            A.GridDistortion(p=0.33),\n",
    "                            ])\n",
    "\n",
    "    tree = os.walk(img_folder)\n",
    "    cwd = os.getcwd()\n",
    "    for root, dirs, files in tree:\n",
    "        for file in files:\n",
    "            image_base_path = os.path.join(root, file)\n",
    "            mask_base_path = os.path.join(mask_folder, file) \n",
    "            mask_base_path = mask_base_path.replace('.jpg', '.png')\n",
    "            image_base = np.array(Image.open(image_base_path).convert('RGB'))\n",
    "            mask_base = np.array(Image.open(mask_base_path).convert('RGB'))\n",
    "            for i in range(num_of_augmentaions):\n",
    "                transformed = transform(image=image_base, mask=mask_base)\n",
    "                transform_image_save_path = image_base_path.replace('.jpg',f'__{i}.jpg')\n",
    "                transform_mask_save_path = mask_base_path.replace('.png',f'__{i}.png')\n",
    "\n",
    "                transformed_image = Image.fromarray(transformed['image'])\n",
    "                transformed_image.save(transform_image_save_path)\n",
    "                transformed_mask = Image.fromarray(transformed['mask'])\n",
    "                transformed_mask.save(transform_mask_save_path)\n",
    "\n",
    "\n",
    "num_of_augs = 15          \n",
    "#data_augmentation(images_path, masks_path, num_of_augs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_():\n",
    "\n",
    "    for file1, file2, file3, file4 in zip(os.scandir(img_trainA_path), os.scandir(img_trainB_path),os.scandir(img_testA_path),os.scandir(img_testB_path)):\n",
    "        os.remove(file1.path)\n",
    "        os.remove(file2.path)\n",
    "        os.remove(file3.path)\n",
    "        os.remove(file4.path)\n",
    "\n",
    "    masks_list = []\n",
    "    images_list = []\n",
    "    images_tree = os.walk(images_path)\n",
    "    masks_tree = os.walk(masks_path)\n",
    "    for root, dirs, files in images_tree:\n",
    "        for filename in files:\n",
    "            images_list.append(filename)\n",
    "\n",
    "    for root, dirs, files in masks_tree:\n",
    "        for filename in files:\n",
    "            masks_list.append(filename)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(images_list, masks_list, test_size=0.3)\n",
    "\n",
    "\n",
    "    for x_t, y_t in zip(X_train, y_train):\n",
    "        shutil.copy(os.path.join(images_path, x_t), os.path.join(img_trainA_path, x_t))\n",
    "        shutil.copy(os.path.join(masks_path, y_t), os.path.join(img_trainB_path, y_t))\n",
    "\n",
    "    for x_val, y_val in zip(X_val, y_val):\n",
    "        shutil.copy(os.path.join(images_path, x_val), os.path.join(img_testA_path, x_val))\n",
    "        shutil.copy(os.path.join(masks_path, y_val), os.path.join(img_testB_path, y_val))\n",
    "\n",
    "#train_test_split_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode labeled masks to classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\test_case_cv\\Unet\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:488: UserWarning: Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\n",
      "  warnings.warn(\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\")\n"
     ]
    }
   ],
   "source": [
    "for file1, file2 in zip(os.scandir(os.path.join(npy_path, r'npy_trainB')), os.scandir(os.path.join(npy_path, r'npy_testB'))):\n",
    "    os.remove(file1.path)\n",
    "    os.remove(file2.path)\n",
    "\n",
    "\n",
    "\"\"\"resizing pictures without antialiasing\"\"\"\n",
    "\n",
    "transform_mask = transforms.Resize((TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT), interpolation=transforms.InterpolationMode.NEAREST_EXACT, antialias=False)\n",
    "\n",
    "\n",
    "# colormap12_gray = {19 : 0, #background #PIL\n",
    "#             29 : 1, #facade\n",
    "#             79: 2, #window\n",
    "#             129: 3, #door\n",
    "#             126: 4, #cornice\n",
    "#             194: 5, #sill\n",
    "#             210: 6, #balcony\n",
    "#             226: 7, #blind\n",
    "#             176: 8, #deco\n",
    "#             179: 9, #molding\n",
    "#             76: 10, #pillar\n",
    "#             51: 11 #shop \n",
    "#                               }\n",
    "\n",
    "colormap2_gray = {   19 : 0, #background\n",
    "                    79: 1 #window    \n",
    "                                }\n",
    "                                \n",
    "\n",
    "def to_categorical(y: np.array, num_classes: int) -> np.array:\n",
    "    \"\"\"one-hot encoding\"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y.astype('uint8')]\n",
    "\n",
    "def encode_binary_gray(img_array: np.array, colormap):\n",
    "\n",
    "    \"\"\"2 class segmentation encoding for sigmoid activation\"\"\"\n",
    "    class_array = np.zeros((img_array.shape[0], img_array.shape[1]))\n",
    "    image_trans_array = np.zeros((img_array.shape[0], img_array.shape[1]))\n",
    "\n",
    "    for W in range(img_array.shape[0]):\n",
    "        for H in range(img_array.shape[1]):\n",
    "            class_array[W,H] = colormap.get(img_array[W,H], 0)\n",
    "            gray_color = [k for k,v in colormap.items() if v==class_array[W,H]][0]\n",
    "            image_trans_array[W,H] = gray_color\n",
    "    return class_array.astype(np.uint8), image_trans_array.astype(np.uint8)\n",
    "\n",
    "\n",
    "def encode_to_classes2(img_array: np.array, colormap):\n",
    "    \"\"\"2 class segmentation encoding for softmax activation\"\"\"\n",
    "    class_array = np.zeros((img_array.shape[0], img_array.shape[1]))\n",
    "    image_trans_array = np.zeros((img_array.shape[0], img_array.shape[1]))\n",
    "\n",
    "    for W in range(img_array.shape[0]):\n",
    "        for H in range(img_array.shape[1]):\n",
    "            #pixel_color = tuple(img_array[W,H])\n",
    "            class_array[W,H] = colormap.get(img_array[W,H], 0)\n",
    "            image_trans_array[W,H] = img_array[W,H]\n",
    "    class_array = to_categorical(class_array,2)\n",
    "    return class_array.astype(np.uint8), image_trans_array.astype(np.uint8)\n",
    "    \n",
    "\n",
    "def create_image_data(execute) -> np.array:\n",
    "    \"\"\"saving encoded class arrays as .npy on disk\"\"\"\n",
    "    if execute:\n",
    "        tree = os.walk(os.path.join(cwd, r'dataset\\trainB'))\n",
    "        for root, dirs, files in tree:\n",
    "            for filename in files:\n",
    "                mask_filename = os.path.join(root, filename)\n",
    "                mask = Image.open(mask_filename)\n",
    "                mask = ImageOps.grayscale(mask)\n",
    "                mask = transform_mask(mask)\n",
    "                mask = np.array(mask)\n",
    "                if NUM_CLASSES == 2:\n",
    "                    class_array, img_trans_array = encode_to_classes2(mask, colormap2_gray)\n",
    "                else:\n",
    "                    class_array, img_trans_array = encode_binary_gray(mask, colormap2_gray)\n",
    "                    #Image.fromarray(img_trans_array).save(os.path.join(r'D:\\Coding\\test_case_cv\\Unet\\dataset\\check_masks', filename))\n",
    "                np.save(os.path.join(npy_path, r'npy_trainB', filename.replace('.png', '.npy')), class_array)\n",
    "\n",
    "        tree = os.walk(os.path.join(cwd, r'dataset\\testB'))\n",
    "        for root, dirs, files in tree:\n",
    "            for filename in files:\n",
    "                mask_filename = os.path.join(root, filename)\n",
    "                mask = Image.open(mask_filename)\n",
    "                mask = ImageOps.grayscale(mask)\n",
    "                mask = transform_mask(mask)\n",
    "                mask = np.array(mask)\n",
    "                if NUM_CLASSES == 2:\n",
    "                    class_array, img_trans_array = encode_to_classes2(mask, colormap2_gray)\n",
    "                else:\n",
    "                    class_array, img_trans_array = encode_binary_gray(mask, colormap2_gray)\n",
    "                    \n",
    "                np.save(os.path.join(npy_path, r'npy_testB', filename.replace('.png', '.npy')), class_array)\n",
    "                \n",
    "#create_image_data(EXECUTION_IMAGE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m      7\u001b[0m                 mask_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, filename)\n\u001b[1;32m----> 8\u001b[0m                 mask_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_filename\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m tree \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwalk(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cwd, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnp.array_targets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnpy_testB\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     11\u001b[0m mask_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Coding\\test_case_cv\\Unet\\venv\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Balancing weights for loss function\"\"\"\n",
    "\n",
    "# tree = os.walk(os.path.join(cwd, r'dataset\\np.array_targets\\npy_trainB'))\n",
    "# mask_list = []\n",
    "# for root, dirs, files in tree:\n",
    "#             for filename in files:\n",
    "#                 mask_filename = os.path.join(root, filename)\n",
    "#                 mask_list.append(np.load(mask_filename))\n",
    "\n",
    "# tree = os.walk(os.path.join(cwd, r'dataset\\np.array_targets\\npy_testB'))\n",
    "# mask_list = []\n",
    "# for root, dirs, files in tree:\n",
    "#             for filename in files:\n",
    "#                 mask_filename = os.path.join(root, filename)\n",
    "#                 mask_list.append(np.load(mask_filename))\n",
    "\n",
    "# masks_encoded = np.array(mask_list)\n",
    "# masks_reshaped_encoded = masks_encoded.reshape(-1,1).flatten()\n",
    "\n",
    "# # class_weights = compute_class_weight(class_weight='balanced', \n",
    "# #                                     classes = np.unique(masks_reshaped_encoded), \n",
    "# #                                     y=masks_reshaped_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  class_weights = torch.Tensor(class_weights).to('cuda')\n",
    "#  print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2_gray():\n",
    "    tree_train = os.walk(img_trainA_path)\n",
    "    tree_test = os.walk(img_testA_path)\n",
    "\n",
    "    all_images_pathnames = []\n",
    "    for root,dirs, files in tree_train:\n",
    "        for filename in files:\n",
    "            all_images_pathnames.append(os.path.join(root,filename))\n",
    "\n",
    "    for root,dirs, files in tree_test:\n",
    "        for filename in files:\n",
    "            all_images_pathnames.append(os.path.join(root,filename))\n",
    "    \n",
    "    mean = np.array([0.])\n",
    "    stdTemp = np.array([0.])\n",
    "    std = np.array([0.])\n",
    "    \n",
    "    numSamples = len(all_images_pathnames)\n",
    "    for i in range(numSamples):\n",
    "        image = Image.open(all_images_pathnames[i])\n",
    "        image= ImageOps.grayscale(image)\n",
    "        image = np.array(image).astype(np.float32) / 255\n",
    "        mean[0] += np.mean(image)\n",
    "        \n",
    "    mean_gray = mean[0] / numSamples\n",
    "    mean_gray = [mean_gray]\n",
    "\n",
    "    for i in range(numSamples):\n",
    "        image = Image.open(all_images_pathnames[i])\n",
    "        image= ImageOps.grayscale(image)\n",
    "        image = np.array(image).astype(np.float32) / 255\n",
    "        stdTemp[0] += (image - mean_gray[0]**2).sum()/(image.shape[0]*image.shape[1])\n",
    "    \n",
    "    std = np.sqrt(stdTemp/numSamples)\n",
    "    std_gray=[std[0]]\n",
    "    print(mean_gray, std_gray)\n",
    "    return mean_gray, std_gray\n",
    "\n",
    "\n",
    "mean = [0.45470184212915166]\n",
    "std = [0.49794385661480856]\n",
    "\n",
    "#mean, std = normalize2_gray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset class and instances #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 128])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_image = transforms.Compose(\n",
    "    [   transforms.Resize((TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT)),\n",
    "        transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "    ]\n",
    ")\n",
    "transform_label = transforms.ToTensor() # unused\n",
    "\n",
    "image_train_ids = [] # pass to Pytorch Dataset class\n",
    "image_train_path_list = [] # pass to Pytorch Dataset class\n",
    "mask_train_path_list = [] # pass to Pytorch Dataset class\n",
    "\n",
    "image_test_ids = [] # pass to Pytorch Dataset class\n",
    "image_test_path_list = [] # pass to Pytorch Dataset class\n",
    "mask_test_path_list = [] # pass to Pytorch Dataset class\n",
    "\n",
    "# get python lists with image and maskspaths\n",
    "tree_train_img = os.walk(os.path.join(cwd, r'dataset\\trainA')) \n",
    "for root, dirs, files in tree_train_img:\n",
    "            for filename in files:\n",
    "                img_id = filename.replace('.jpg','')\n",
    "                image_path = os.path.join(root, filename)\n",
    "                image_train_ids.append(img_id)\n",
    "                image_train_path_list.append(os.path.join(root, filename))\n",
    "\n",
    "\n",
    "tree_test_img = os.walk(os.path.join(cwd, r'dataset\\testA')) \n",
    "for root, dirs, files in tree_test_img:\n",
    "            for filename in files:\n",
    "                img_id = filename.replace('.jpg','')\n",
    "                image_path = os.path.join(root, filename)\n",
    "                image_test_ids.append(img_id)\n",
    "                image_test_path_list.append(os.path.join(root, filename))\n",
    "\n",
    "\n",
    "tree_train_npy = os.walk(os.path.join(npy_path, r'npy_trainB'))\n",
    "for root, dirs, files in tree_train_npy:\n",
    "            for filename in files:\n",
    "                mask_train_path_list.append(os.path.join(root, filename))\n",
    "\n",
    "tree_test_npy = os.walk(os.path.join(npy_path, r'npy_testB'))\n",
    "for root, dirs, files in tree_test_npy:\n",
    "            for filename in files:\n",
    "                mask_test_path_list.append(os.path.join(root, filename))\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for pytorch dataloader\"\"\"\n",
    "    def __init__(self, ids_list, image_path_list, mask_path_list, transform_image, transform_label):\n",
    "        self.image_ids = ids_list\n",
    "        self.image_path_list = image_path_list\n",
    "        self.mask_path_list = mask_path_list\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_path_list[idx]\n",
    "        img_label_path = self.mask_path_list[idx]\n",
    "        image = Image.open(self.image_path_list[idx])\n",
    "        image= ImageOps.grayscale(image)\n",
    "        image_label = np.load(self.mask_path_list[idx])\n",
    "        text_label = self.image_ids[idx]\n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label:\n",
    "            image_label = torch.Tensor(image_label)\n",
    "            image_label = torch.permute(image_label, dims=(2,0,1))\n",
    "        return image, image_label, text_label\n",
    "\n",
    "\n",
    "training_dataset = CustomImageDataset(ids_list=image_train_ids,\n",
    "                                        image_path_list=image_train_path_list,\n",
    "                                        mask_path_list=mask_train_path_list,\n",
    "                                   transform_image=transform_image,\n",
    "                                   transform_label=transform_label,\n",
    "                                   )\n",
    "\n",
    "validation_dataset = CustomImageDataset(ids_list=image_test_ids,\n",
    "                                        image_path_list=image_test_path_list,\n",
    "                                        mask_path_list=mask_test_path_list,\n",
    "                                   transform_image=transform_image,\n",
    "                                   transform_label=transform_label,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader instances #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, pin_memory=(torch.cuda.is_available()), shuffle=True, drop_last=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, pin_memory=(torch.cuda.is_available()), shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Model implementation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)         \n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)         \n",
    "        self.relu = nn.ReLU()  \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)    \n",
    "        return x\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        self.dropout = nn.Dropout(p=0.3) \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)    \n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, num_channels, num_of_colors):\n",
    "        super().__init__() \n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(num_of_colors, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)        \n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)         \n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)        \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.outputs = nn.Conv2d(64, num_channels, kernel_size=1, padding=0)     \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)         \n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)         \n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)         \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and run model #\n",
    "\n",
    "### Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(preds_batch, labels_batch):\n",
    "    N = TARGET_IMAGE_HEIGHT * TARGET_IMAGE_HEIGHT\n",
    "    preds = torch.softmax(preds_batch.squeeze(0), dim=0)\n",
    "    preds = torch.argmax(preds, dim=0).to(torch.int32)\n",
    "    labels = torch.argmax(labels_batch.squeeze(0), dim=0).to(torch.int32)\n",
    "    return torch.sum(torch.eq(preds,labels)) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.9395301938056946\n",
      "  batch 2 loss: 0.808032751083374\n",
      "  batch 3 loss: 0.6969606876373291\n",
      "  batch 4 loss: 0.6363489031791687\n",
      "  batch 5 loss: 0.5753549337387085\n",
      "  batch 6 loss: 0.5363470911979675\n",
      "  batch 7 loss: 0.4965018630027771\n",
      "  batch 8 loss: 0.4739192724227905\n",
      "  batch 9 loss: 0.45043110847473145\n",
      "  batch 10 loss: 0.4240991175174713\n",
      "  batch 11 loss: 0.40859001874923706\n",
      "  batch 12 loss: 0.37980687618255615\n",
      "  batch 13 loss: 0.3762441575527191\n",
      "  batch 14 loss: 0.357334166765213\n",
      "  batch 15 loss: 0.3528655767440796\n",
      "  batch 16 loss: 0.3413981795310974\n",
      "  batch 17 loss: 0.3198183476924896\n",
      "  batch 18 loss: 0.3368654251098633\n",
      "  batch 19 loss: 0.31707096099853516\n",
      "  batch 20 loss: 0.31928908824920654\n",
      "  batch 21 loss: 0.2894871234893799\n",
      "  batch 22 loss: 0.3048691749572754\n",
      "  batch 23 loss: 0.2903040647506714\n",
      "  batch 24 loss: 0.3055132031440735\n",
      "  batch 25 loss: 0.2766028940677643\n",
      "  batch 26 loss: 0.2870015501976013\n",
      "  batch 27 loss: 0.27847447991371155\n",
      "  batch 28 loss: 0.27190685272216797\n",
      "  batch 29 loss: 0.2764056622982025\n",
      "  batch 30 loss: 0.24595560133457184\n",
      "  batch 31 loss: 0.2614791691303253\n",
      "  batch 32 loss: 0.2652939260005951\n",
      "  batch 33 loss: 0.25669392943382263\n",
      "  batch 34 loss: 0.26922380924224854\n",
      "  batch 35 loss: 0.24854624271392822\n",
      "  batch 36 loss: 0.2553413212299347\n",
      "  batch 37 loss: 0.2449607104063034\n",
      "  batch 38 loss: 0.239106684923172\n",
      "  batch 39 loss: 0.24812839925289154\n",
      "  batch 40 loss: 0.2476671040058136\n",
      "  batch 41 loss: 0.24762630462646484\n",
      "  batch 42 loss: 0.2614639103412628\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS =250\n",
    "writer_step = 2\n",
    "WEIGHT_DECAY = 0\n",
    "\n",
    "\n",
    "model =smp.Unet(encoder_name='resnet18', \n",
    "                        encoder_weights='imagenet',\n",
    "                        in_channels=1,\n",
    "                        classes=2).to(device)\n",
    "        \n",
    "# model = Unet(num_channels=NUM_CLASSES, num_of_colors=NUM_COLORS).to(device)\n",
    "# print(summary(model))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=LEARNING_RATE, \n",
    "                                #weight_decay=WEIGHT_DECAY, \n",
    "                                )\n",
    "\n",
    "\n",
    "def main(execute):\n",
    "    if execute:\n",
    "        def train_one_epoch(epoch_index, tb_writer):\n",
    "            running_loss = 0\n",
    "            last_loss = 0\n",
    "            running_acc = 0\n",
    "            last_acc = 0\n",
    "            for i, batch in enumerate(train_dataloader):\n",
    "                inputs, labels, _ = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(inputs)\n",
    "                loss = loss_fn(preds, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                running_acc = get_acc(preds, labels)\n",
    "                last_acc = running_acc\n",
    "                last_loss = running_loss\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "                tb_writer.add_scalars('Training Accuracy/Loss', {'Training Loss': last_loss, 'Training Accuracy': last_acc*100}, tb_x) \n",
    "                running_loss = 0.\n",
    "                running_acc = 0.\n",
    "                #scheduler.step(last_loss)\n",
    "            return last_loss\n",
    "\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "        epoch_number = 0\n",
    "\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "            model.train(True)\n",
    "            avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "            model.train(False)\n",
    "            with torch.no_grad():\n",
    "                running_vloss = 0.0\n",
    "                for i, vdata in enumerate(validation_dataloader):\n",
    "                    vinputs, vlabels, _ = vdata\n",
    "                    vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "                    voutputs = model(vinputs)\n",
    "                    vloss = loss_fn(voutputs, vlabels)\n",
    "                    running_vloss += vloss\n",
    "                    avg_vloss = running_vloss / (i + 1)\n",
    "                    \n",
    "                    running_acc = get_acc(voutputs, vlabels)\n",
    "                    avg_acc = running_acc / (i + 1)\n",
    "                    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "                    print(f'Validation accuracy = {avg_acc*100} %')\n",
    "                    # Log the running loss averaged per batch\n",
    "                    # for both training and validation\n",
    "                    writer.add_scalars('Validation Loss/Accuracy',\n",
    "                                    { 'Validation Loss' : avg_loss, 'Validation Accuracy' : avg_acc*100},\n",
    "                                    epoch_number + 1)\n",
    "                    writer.flush()\n",
    "                \n",
    "            epoch_number += 1\n",
    "\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        tree = os.walk(cwd, topdown=True)\n",
    "\n",
    "        for root, dirs, files in tree:\n",
    "            for name in files:\n",
    "                if '.pth' in os.path.join(cwd, name):\n",
    "                    os.remove(os.path.join(cwd, name))\n",
    "                    break\n",
    "            break\n",
    "\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path + '.pth')\n",
    "\n",
    "main(EXECUTION_MODEL_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre trained Unet encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions on single Image #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "tree = os.walk(cwd, topdown=True)\n",
    "for root, dirs, files in tree:\n",
    "    for name in files:\n",
    "        if '.pth' in os.path.join(cwd, name):\n",
    "            model_path = os.path.join(cwd, name)\n",
    "            break\n",
    "    break\n",
    "\n",
    "model =smp.Unet(encoder_name='resnet50', \n",
    "                encoder_weights='imagenet',\n",
    "                in_channels=1,\n",
    "                classes=2).to(device)\n",
    "\n",
    "# model = Unet(num_channels=NUM_CLASSES, num_of_colors=NUM_COLORS).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "def decode_to_RGB2(model, image_path, save_path, colormap): #gray\n",
    "    \"\"\"segmentation predicton for 2 class segmentation\"\"\"\n",
    "    image_test_path = os.path.join(cwd, image_path) \n",
    "    image_test = Image.open(image_test_path)\n",
    "    image_test = ImageOps.grayscale(image_test)\n",
    "    image_test = transform_image(image_test)\n",
    "    image_test = image_test.clone().detach().to(device)\n",
    "    image_test .to(device)\n",
    "\n",
    "    output = model(image_test .unsqueeze(0)).squeeze(0)\n",
    "    output = torch.permute(output, (1,2,0)) #swap axes of tensor\n",
    "    output = torch.argmax(output, dim=2)\n",
    "    image_classes = output.cpu().detach().numpy()\n",
    "    image_output = np.zeros(shape=(TARGET_IMAGE_WIDTH,TARGET_IMAGE_HEIGHT)).astype(np.uint8)\n",
    "    for W in range(image_classes.shape[0]):\n",
    "        for H in range(image_classes.shape[1]):\n",
    "            image_output[W,H] = [k for k, v in colormap.items() if v == image_classes[W,H]][0]\n",
    "    image_output = Image.fromarray(image_output.astype(np.uint8))\n",
    "    image_output.save(save_path)\n",
    "\n",
    "\n",
    "def decode_binary_gray(model, image_path, save_path, colormap):\n",
    "    image_test_path = os.path.join(cwd, image_path) \n",
    "    image_test = Image.open(image_test_path)\n",
    "    image_test = ImageOps.grayscale(image_test)\n",
    "    image_test = transform_image(image_test)\n",
    "    image_test = image_test.clone().detach().to(device)\n",
    "\n",
    "    output = model(image_test .unsqueeze(0))\n",
    "    output = torch.sigmoid(output).squeeze(0)\n",
    "    output = torch.permute(output, dims=(1,2,0))\n",
    "\n",
    "    image_classes = output.cpu().detach().numpy()\n",
    "    image_output = np.zeros((TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT))\n",
    "    for W in range(image_classes.shape[0]):\n",
    "        for H in range(image_classes.shape[1]):\n",
    "            print(image_classes[W,H])\n",
    "            if image_classes[W,H] >0.5:\n",
    "                image_output[W,H] = 79\n",
    "            else:\n",
    "                image_output[W,H] = 19\n",
    "\n",
    "    image_output = Image.fromarray(image_output.astype(np.uint8)).save(save_path)\n",
    "\n",
    "if NUM_CLASSES == 2:\n",
    "    decode_to_RGB2(model, 'test_image_validation.jpg', 'test_mask_validation_predicted.png', colormap2_gray)\n",
    "    decode_to_RGB2(model, 'test_image_train.jpg', 'train_mask_validation_predicted.png', colormap2_gray)\n",
    "elif NUM_CLASSES == 1:\n",
    "    decode_binary_gray(model, 'test_image_validation.jpg', 'test_mask_validation_predicted.png', colormap2_gray)\n",
    "    decode_binary_gray(model, 'test_image_train.jpg', 'train_mask_validation_predicted.png', colormap2_gray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of windows #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество окон - None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The marching squares algorithm for counting windows\"\"\"\n",
    "\n",
    "test_image_path = os.path.join(cwd, 'test_mask_predicted.png')\n",
    "\n",
    "def count_windows(test_img_path):\n",
    "    if os.path.exists(test_image_path):\n",
    "        imgray = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        imgrgb = cv2.imread(test_image_path)\n",
    "\n",
    "        # get contours\n",
    "        contours = measure.find_contours(imgray)\n",
    "\n",
    "        # get contours length\n",
    "        contour_length_list = []\n",
    "        for contour in contours:\n",
    "            contour_length = 0\n",
    "            for i in range(len(contour)-1):\n",
    "                contour_length += cv2.norm(contour[i], contour[i+1], cv2.NORM_L2)\n",
    "            contour_length_list.append(contour_length)\n",
    "\n",
    "        max_countour_length = max(contour_length_list)\n",
    "        # remove countours with length less than 20% of mean length of all countours\n",
    "        windows_count = len([length for length in contour_length_list if length > max_countour_length/3])\n",
    "        return windows_count\n",
    "\n",
    "windows = count_windows(test_image_path)\n",
    "print(f\"Количество окон - {windows}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "876187d4a2c995a73645928887c465872e44a13f37aecb236f2b7209f9d185bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
