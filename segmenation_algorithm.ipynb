{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Описание подходов для решения задачи сегментации фасадов жилых домов</h1>\n",
    "\n",
    "Для решения задачи будем использовать архитектуру Unet, хорошо зарекомендовавшуюся себя в задачах семантической сегментации изображений.\n",
    "\n",
    "1. Характеристики входного изображения (С x W x H) - 1 x 128 x 128 (grayscale)\n",
    "2. Размеченные данные - (С x W x H) - 2 x 128 x 128 (два класса [0,1], 0 - background, 1 - windows)\n",
    "3. При достаточной хорошей обучаемости Unet, для определения контуров окон можно использовать The marching squares algorithm. (The marching squares algorithm is a special case of the marching cubes algorithm (Lorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170).)\n",
    "\n",
    "The marching squares algorithm возвращает требуемое количество контуров, соответствующих окнам, а также индексы пикселей, соответствующие центрам контуров, чтобы можно было посчитать сетку колонн/рядов окон.\n",
    "\n",
    "Работа алгоритма:\n",
    "\n",
    "<image src=\"https://scikit-image.org/docs/stable/_images/sphx_glr_plot_contours_001.png\" alt=\"Описание картинки\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision import datasets\n",
    "#from torchsummary import summary\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import cv2 \n",
    "import albumentations as A\n",
    "from skimage import measure # measure.find_contours() - The marching squares algorithm\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.utils import compute_class_weight\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dataset: CMP_facade_DB_base\n",
    "https://cmp.felk.cvut.cz/~tylecr1/facade/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IMAGE_WIDTH = 128\n",
    "TARGET_IMAGE_HEIGHT = 128\n",
    "NUM_CHANNELS = 2\n",
    "BATCH_SIZE = 16\n",
    "EXECUTION_IMAGE_DATA = True\n",
    "EXECUTION_MODEL_TRAIN = True\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "mean_gray=[0.485]\n",
    "std_gray=[0.229]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(img_folder, mask_folder, num_of_augmentaions):\n",
    "\n",
    "    \"\"\"Additional image transformations\"\"\"\n",
    "    \n",
    "    transform = A.Compose([A.VerticalFlip(),\n",
    "                            A.Rotate(),\n",
    "                            A.HorizontalFlip(),\n",
    "                            A.GridDistortion(p=0.2),\n",
    "                            ])\n",
    "\n",
    "    tree = os.walk(img_folder)\n",
    "    cwd = os.getcwd()\n",
    "    for root, dirs, files in tree:\n",
    "        for file in files:\n",
    "            image_base_path = os.path.join(root, file)\n",
    "            mask_base_path = os.path.join(mask_folder, file) \n",
    "            mask_base_path = mask_base_path.replace('.jpg', '.png')\n",
    "            image_base = np.array(Image.open(image_base_path).convert('RGB'))\n",
    "            mask_base = np.array(Image.open(mask_base_path).convert('RGB'))\n",
    "            for i in range(num_of_augmentaions):\n",
    "                transformed = transform(image=image_base, mask=mask_base)\n",
    "                transform_image_save_path = image_base_path.replace('.jpg',f'__{i}.jpg')\n",
    "                transform_mask_save_path = mask_base_path.replace('.png',f'__{i}.png')\n",
    "\n",
    "                transformed_image = Image.fromarray(transformed['image'])\n",
    "                transformed_image.save(transform_image_save_path)\n",
    "                transformed_mask = Image.fromarray(transformed['mask'])\n",
    "                transformed_mask.save(transform_mask_save_path)\n",
    "\n",
    "\n",
    "num_of_augs = 4            \n",
    "\n",
    "#data_augmentation(r'D:\\Coding\\test_case_cv\\Unet\\dataset\\trainA', r'D:\\Coding\\test_case_cv\\Unet\\dataset\\trainB', num_of_augs)\n",
    "#data_augmentation(r'D:\\Coding\\test_case_cv\\Unet\\dataset\\testA', r'D:\\Coding\\test_case_cv\\Unet\\dataset\\testB', num_of_augs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode segmentation masks to classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"resizing pictures without antialiasing\"\"\"\n",
    "\n",
    "transform_mask = transforms.Resize((TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT), interpolation=transforms.InterpolationMode.NEAREST_EXACT, antialias=False)\n",
    "\n",
    "\"\"\"Diferent colormaps for testing (RGB, Grayscale)\"\"\"\n",
    "colormap12 = {(0,0,170) : 0, #background\n",
    "            (0,0,255) : 1, #facade\n",
    "            (0,85,255): 2, #window\n",
    "            (0,170,255): 3, #door\n",
    "            (255,85,0): 4, #cornice\n",
    "            (85,255,170): 5, #sill\n",
    "            (170,255,85): 6, #balcony\n",
    "            (255,255,0): 7, #blind\n",
    "            (255,170,0): 8, #deco\n",
    "            (0,255,255): 9, #molding\n",
    "            (255,0,0): 10, #pillar\n",
    "            (170,0,0): 11 #shop \n",
    "                            }\n",
    "\n",
    "\n",
    "colormap12_gray = {19 : 0, #background #PIL\n",
    "            29 : 1, #facade\n",
    "            79: 2, #window\n",
    "            129: 3, #door\n",
    "            126: 4, #cornice\n",
    "            194: 5, #sill\n",
    "            210: 6, #balcony\n",
    "            226: 7, #blind\n",
    "            176: 8, #deco\n",
    "            179: 9, #molding\n",
    "            76: 10, #pillar\n",
    "            51: 11 #shop \n",
    "                            }\n",
    "\n",
    "colormap2 = {   (0,0,170) : 0, #background\n",
    "                (0,85,255): 1 #window    \n",
    "                           }  \n",
    "\n",
    "colormap2_gray = {   19 : 0, #background\n",
    "                    79: 1 #window    \n",
    "                                }\n",
    "                                \n",
    "  \n",
    "cwd = os.getcwd()\n",
    "img_trainA_path = os.path.join(cwd, r'dataset\\trainA')\n",
    "img_testA_path = os.path.join(cwd, r'dataset\\testA')\n",
    "img_trainB_path = os.path.join(cwd, r'dataset\\trainB')\n",
    "img_testB_path = os.path.join(cwd, r'dataset\\testB')\n",
    "npy_path = os.path.join(cwd, r'dataset\\np.array_targets')\n",
    "img_test_trans = cwd\n",
    "\n",
    "\n",
    "def to_categorical(y: np.array, num_classes: int) -> np.array:\n",
    "    \"\"\"one-hot encoding\"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y.astype('uint8')]\n",
    "\n",
    "\n",
    "\n",
    "def encode_to_classes2(img_array: np.array, colormap):\n",
    "    \"\"\"2 class segmentation encoding\"\"\"\n",
    "    class_array = np.zeros((img_array.shape[0], img_array.shape[1]))\n",
    "    image_trans_array = np.zeros((img_array.shape[0], img_array.shape[1]))\n",
    "\n",
    "    for W in range(img_array.shape[0]):\n",
    "        for H in range(img_array.shape[1]):\n",
    "            #pixel_color = tuple(img_array[W,H])\n",
    "            class_array[W,H] = colormap.get(img_array[W,H], 0)\n",
    "            image_trans_array[W,H] = img_array[W,H]\n",
    "    class_array = to_categorical(class_array,2)\n",
    "    return class_array.astype(np.uint8), image_trans_array.astype(np.uint8)\n",
    "\n",
    "\n",
    "def encode_to_classes12(img_array: np.array, colormap: dict):\n",
    "    \"\"\"2 class segmentation encoding (for testing purposes)\"\"\"\n",
    "\n",
    "    class_array = np.zeros((img_array.shape[0], img_array.shape[1]))\n",
    "    image_trans_array = np.zeros((img_array.shape[0], img_array.shape[1], 3))\n",
    "    for W in range(img_array.shape[0]):\n",
    "        for H in range(img_array.shape[1]):\n",
    "            class_array[W,H] = colormap[tuple(img_array[W,H])]\n",
    "            image_trans_array[W,H] = img_array[W,H]\n",
    "    class_array = to_categorical(class_array, 12)\n",
    "    return class_array.astype(np.uint8), image_trans_array.astype(np.uint8)\n",
    "    \n",
    "\n",
    "def create_image_data(execute) -> np.array:\n",
    "    \"\"\"save encoded class arrays as .npy on disk for faster work of pytorch dataloaer\"\"\"\n",
    "    if execute:\n",
    "        tree = os.walk(os.path.join(cwd, r'dataset\\trainB'))\n",
    "        for root, dirs, files in tree:\n",
    "            for filename in files:\n",
    "                mask_filename = os.path.join(root, filename)\n",
    "\n",
    "                mask = Image.open(mask_filename)\n",
    "                mask = ImageOps.grayscale(mask)\n",
    "                mask = transform_mask(mask)\n",
    "                mask = np.array(mask)\n",
    "                # mask = Image.open(mask_filename).convert('RGB')\n",
    "                # mask = transform_mask(mask)\n",
    "                # mask = np.array(mask)\n",
    "\n",
    "                if NUM_CHANNELS == 2:\n",
    "                    class_array, img_trans_array = encode_to_classes2(mask, colormap2_gray)\n",
    "                else:\n",
    "                    class_array, img_trans_array = encode_to_classes12(mask, colormap12)\n",
    "                    img_trans_array = Image.fromarray(img_trans_array).save(os.path.join(r'D:\\Coding\\test_case_cv\\Unet\\dataset\\check_masks', filename))\n",
    "                np.save(os.path.join(npy_path, r'npy_trainB', filename.replace('.png', '.npy')), class_array)\n",
    "\n",
    "        tree = os.walk(os.path.join(cwd, r'dataset\\testB'))\n",
    "        for root, dirs, files in tree:\n",
    "            for filename in files:\n",
    "                mask_filename = os.path.join(root, filename)\n",
    "                mask = Image.open(mask_filename)\n",
    "                mask = ImageOps.grayscale(mask)\n",
    "                mask = transform_mask(mask)\n",
    "                mask = np.array(mask)\n",
    "\n",
    "                # mask = Image.open(mask_filename).convert('RGB')\n",
    "                # mask = transform_mask(mask)\n",
    "                # mask = np.array(mask)\n",
    "                if NUM_CHANNELS == 2:\n",
    "                    class_array, img_trans_array = encode_to_classes2(mask, colormap2_gray)\n",
    "                else:\n",
    "                    class_array,_ = encode_to_classes12(mask, colormap12)\n",
    "                np.save(os.path.join(npy_path, r'npy_testB', filename.replace('.png', '.npy')), class_array)\n",
    "                \n",
    "#create_image_data(EXECUTION_IMAGE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Balancing weights for loss function\"\"\"\n",
    "\n",
    "tree = os.walk(os.path.join(cwd, r'dataset\\np.array_targets\\npy_trainB'))\n",
    "mask_list = []\n",
    "for root, dirs, files in tree:\n",
    "            for filename in files:\n",
    "                mask_filename = os.path.join(root, filename)\n",
    "                mask_list.append(np.load(mask_filename))\n",
    "\n",
    "tree = os.walk(os.path.join(cwd, r'dataset\\np.array_targets\\npy_testB'))\n",
    "mask_list = []\n",
    "for root, dirs, files in tree:\n",
    "            for filename in files:\n",
    "                mask_filename = os.path.join(root, filename)\n",
    "                mask_list.append(np.load(mask_filename))\n",
    "\n",
    "masks_encoded = np.argmax(np.array(mask_list), axis=3)\n",
    "masks_reshaped_encoded = masks_encoded.reshape(-1,1).flatten()\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                    classes = np.unique(masks_reshaped_encoded), \n",
    "                                    y=masks_reshaped_encoded)\n",
    "# class_weights = [0.56486709,  0.17596369,  0.66243338,  7.8157498,   1.16416553,  5.06901417,\n",
    "#   1.56538023,  1.88223472, 27.06131078,  4.66247519, 11.34772731,  2.91352604]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.Tensor(class_weights).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset class and instances #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_image = transforms.Compose(\n",
    "    [   transforms.Resize((TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT)),\n",
    "        transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean_gray, std=std_gray)\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_label = transforms.ToTensor()\n",
    "\n",
    "\n",
    "image_train_ids = [] # pass to Pytorch Dataset class\n",
    "image_train_path_list = [] # pass to Pytorch Dataset class\n",
    "mask_train_path_list = [] # pass to Pytorch Dataset class\n",
    "\n",
    "image_test_ids = [] # pass to Pytorch Dataset class\n",
    "image_test_path_list = [] # pass to Pytorch Dataset class\n",
    "mask_test_path_list = [] # pass to Pytorch Dataset class\n",
    "\n",
    "tree_train_img = os.walk(os.path.join(cwd, r'dataset\\trainA')) \n",
    "\n",
    "for root, dirs, files in tree_train_img:\n",
    "            for filename in files:\n",
    "                img_id = filename.replace('.jpg','')\n",
    "                image_path = os.path.join(root, filename)\n",
    "                image_train_ids.append(img_id)\n",
    "                image_train_path_list.append(os.path.join(root, filename))\n",
    "\n",
    "\n",
    "tree_test_img = os.walk(os.path.join(cwd, r'dataset\\testA')) \n",
    "for root, dirs, files in tree_test_img:\n",
    "            for filename in files:\n",
    "                img_id = filename.replace('.jpg','')\n",
    "                image_path = os.path.join(root, filename)\n",
    "                image_test_ids.append(img_id)\n",
    "                image_test_path_list.append(os.path.join(root, filename))\n",
    "\n",
    "\n",
    "tree_train_npy = os.walk(os.path.join(npy_path, r'npy_trainB'))\n",
    "for root, dirs, files in tree_train_npy:\n",
    "            for filename in files:\n",
    "                mask_train_path_list.append(os.path.join(root, filename))\n",
    "\n",
    "tree_test_npy = os.walk(os.path.join(npy_path, r'npy_testB'))\n",
    "for root, dirs, files in tree_test_npy:\n",
    "            for filename in files:\n",
    "                mask_test_path_list.append(os.path.join(root, filename))\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for pytorch dataloader\"\"\"\n",
    "    def __init__(self, ids_list, image_path_list, mask_path_list, transform_image, transform_label):\n",
    "        self.image_ids = ids_list\n",
    "        self.image_path_list = image_path_list\n",
    "        self.mask_path_list = mask_path_list\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_path_list[idx]\n",
    "        img_label_path = self.mask_path_list[idx]\n",
    "        image = Image.open(self.image_path_list[idx])\n",
    "        image= ImageOps.grayscale(image)\n",
    "        image_label = np.load(self.mask_path_list[idx])\n",
    "        text_label = self.image_ids[idx]\n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label:\n",
    "            image_label = self.transform_label(image_label)\n",
    "        return image, image_label, text_label\n",
    "\n",
    "\n",
    "training_dataset = CustomImageDataset(ids_list=image_train_ids,\n",
    "                                        image_path_list=image_train_path_list,\n",
    "                                        mask_path_list=mask_train_path_list,\n",
    "                                   transform_image=transform_image,\n",
    "                                   transform_label=transform_label,\n",
    "                                   )\n",
    "\n",
    "validation_dataset = CustomImageDataset(ids_list=image_test_ids,\n",
    "                                        image_path_list=image_test_path_list,\n",
    "                                        mask_path_list=mask_test_path_list,\n",
    "                                   transform_image=transform_image,\n",
    "                                   transform_label=transform_label,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader instances #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Model implementation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)         \n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)         \n",
    "        self.relu = nn.ReLU()  \n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)    \n",
    "        return x\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)    \n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__() \n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(1, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)        \n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)         \n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)        \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, num_channels, kernel_size=1, padding=0)     \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)         \n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)         \n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)         \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and run model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0014214315451681614\n",
      "  batch 2 loss: 0.0012292088940739632\n",
      "  batch 3 loss: 0.0009771047625690699\n",
      "  batch 4 loss: 0.0007376284338533878\n",
      "  batch 5 loss: 0.0005988504271954298\n",
      "  batch 6 loss: 0.0004370987298898399\n",
      "  batch 7 loss: 0.00041860202327370644\n",
      "  batch 8 loss: 0.00047175196232274175\n",
      "  batch 9 loss: 0.0008948147296905518\n",
      "  batch 10 loss: 0.000676861556712538\n",
      "  batch 11 loss: 0.001178947277367115\n",
      "  batch 12 loss: 0.0010429138783365488\n",
      "  batch 13 loss: 0.0008196195121854544\n",
      "  batch 14 loss: 0.0013547197449952364\n",
      "  batch 15 loss: 0.001025184988975525\n",
      "  batch 16 loss: 0.0007728490163572133\n",
      "  batch 17 loss: 0.0009595765150152147\n",
      "  batch 18 loss: 0.0007523289532400668\n",
      "  batch 19 loss: 0.0013438686728477478\n",
      "  batch 20 loss: 0.0012332148617133498\n",
      "  batch 21 loss: 0.0011160783469676971\n",
      "  batch 22 loss: 0.0010784427868202329\n",
      "  batch 23 loss: 0.0011122656287625432\n",
      "  batch 24 loss: 0.0010697003453969955\n",
      "  batch 25 loss: 0.0011476895306259394\n",
      "  batch 26 loss: 0.0008980011334642768\n",
      "  batch 27 loss: 0.0008638303261250257\n",
      "  batch 28 loss: 0.0008634928963147104\n",
      "  batch 29 loss: 0.0008734982693567872\n",
      "  batch 30 loss: 0.0007738699205219746\n",
      "  batch 31 loss: 0.0007724732859060168\n",
      "  batch 32 loss: 0.0008334186859428883\n",
      "  batch 33 loss: 0.0006588565884158015\n",
      "  batch 34 loss: 0.0006719244411215186\n",
      "  batch 35 loss: 0.0006160831544548273\n",
      "  batch 36 loss: 0.000639387231785804\n",
      "  batch 37 loss: 0.0006295817438513041\n",
      "  batch 38 loss: 0.000493026280310005\n",
      "  batch 39 loss: 0.0006789664621464908\n",
      "  batch 40 loss: 0.0011199385626241565\n",
      "  batch 41 loss: 0.00045686395606026053\n",
      "  batch 42 loss: 0.0004330970114096999\n",
      "  batch 43 loss: 0.0007783834007568657\n",
      "  batch 44 loss: 0.000568615272641182\n",
      "  batch 45 loss: 0.0006360486731864512\n",
      "  batch 46 loss: 0.0007637835224159062\n",
      "  batch 47 loss: 0.0007663285359740257\n",
      "  batch 48 loss: 0.0006167350802570581\n",
      "  batch 49 loss: 0.0006233844324015081\n",
      "  batch 50 loss: 0.0006442988524213433\n",
      "  batch 51 loss: 0.0008235542336478829\n",
      "  batch 52 loss: 0.0006842793663963675\n",
      "  batch 53 loss: 0.0006906091002747416\n",
      "  batch 54 loss: 0.000759304326493293\n",
      "  batch 55 loss: 0.0007331951637752354\n",
      "  batch 56 loss: 0.0005924394354224205\n",
      "  batch 57 loss: 0.0006160710472613573\n",
      "  batch 58 loss: 0.00084161531412974\n",
      "  batch 59 loss: 0.0009078638977371156\n",
      "  batch 60 loss: 0.000756445515435189\n",
      "  batch 61 loss: 0.000789072597399354\n",
      "  batch 62 loss: 0.0006043097819201648\n",
      "  batch 63 loss: 0.0006226986879482865\n",
      "  batch 64 loss: 0.0006976445438340306\n",
      "  batch 65 loss: 0.0007976215565577149\n",
      "  batch 66 loss: 0.0005099589470773935\n",
      "  batch 67 loss: 0.0005473291967064142\n",
      "  batch 68 loss: 0.0005539036355912685\n",
      "  batch 69 loss: 0.0006333924829959869\n",
      "  batch 70 loss: 0.0006790590705350041\n",
      "  batch 71 loss: 0.000528429402038455\n",
      "  batch 72 loss: 0.0004868142132181674\n",
      "  batch 73 loss: 0.0004718357522506267\n",
      "  batch 74 loss: 0.0005369078135117888\n",
      "  batch 75 loss: 0.000711372122168541\n",
      "  batch 76 loss: 0.0006917592836543918\n",
      "  batch 77 loss: 0.0006556161097250879\n",
      "  batch 78 loss: 0.0006319000385701656\n",
      "  batch 79 loss: 0.0006322939298115671\n",
      "  batch 80 loss: 0.0006745507707819343\n",
      "  batch 81 loss: 0.0006333552300930023\n",
      "  batch 82 loss: 0.0006593992584384978\n",
      "  batch 83 loss: 0.0006036366685293615\n",
      "  batch 84 loss: 0.0006234738393686712\n",
      "  batch 85 loss: 0.0006085406057536602\n",
      "  batch 86 loss: 0.0006943251937627792\n",
      "  batch 87 loss: 0.000723121571354568\n",
      "  batch 88 loss: 0.0008497228845953941\n",
      "  batch 89 loss: 0.0006235151086002588\n",
      "  batch 90 loss: 0.0007395180291496217\n",
      "  batch 91 loss: 0.0006541031179949641\n",
      "  batch 92 loss: 0.0006327941082417965\n",
      "  batch 93 loss: 0.0005592835368588567\n",
      "  batch 94 loss: 0.0006505667115561664\n",
      "  batch 95 loss: 0.0007231486961245537\n",
      "LOSS train 0.0007231486961245537 valid 0.001110715325921774\n",
      "LOSS train 0.0007231486961245537 valid 0.0012123666238039732\n",
      "LOSS train 0.0007231486961245537 valid 0.001146045746281743\n",
      "LOSS train 0.0007231486961245537 valid 0.0011246590875089169\n",
      "LOSS train 0.0007231486961245537 valid 0.001114196260459721\n",
      "LOSS train 0.0007231486961245537 valid 0.001129546551965177\n",
      "LOSS train 0.0007231486961245537 valid 0.0011926813749596477\n",
      "LOSS train 0.0007231486961245537 valid 0.0012126367073506117\n",
      "LOSS train 0.0007231486961245537 valid 0.001196133205667138\n",
      "LOSS train 0.0007231486961245537 valid 0.001209270441904664\n",
      "LOSS train 0.0007231486961245537 valid 0.0012390513438731432\n",
      "LOSS train 0.0007231486961245537 valid 0.0012449101777747273\n",
      "LOSS train 0.0007231486961245537 valid 0.0012260727817192674\n",
      "LOSS train 0.0007231486961245537 valid 0.0012469071662053466\n",
      "LOSS train 0.0007231486961245537 valid 0.0012774465139955282\n",
      "LOSS train 0.0007231486961245537 valid 0.00130178639665246\n",
      "LOSS train 0.0007231486961245537 valid 0.001301244948990643\n",
      "LOSS train 0.0007231486961245537 valid 0.0013106834376230836\n",
      "LOSS train 0.0007231486961245537 valid 0.001322978874668479\n",
      "LOSS train 0.0007231486961245537 valid 0.0013301428407430649\n",
      "LOSS train 0.0007231486961245537 valid 0.0013257787795737386\n",
      "LOSS train 0.0007231486961245537 valid 0.001329854130744934\n",
      "LOSS train 0.0007231486961245537 valid 0.001336802844889462\n",
      "LOSS train 0.0007231486961245537 valid 0.0013360099401324987\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0005379794165492058\n",
      "  batch 2 loss: 0.0006049260264262557\n",
      "  batch 3 loss: 0.000760588503908366\n",
      "  batch 4 loss: 0.0006287355208769441\n",
      "  batch 5 loss: 0.00047767037176527083\n",
      "  batch 6 loss: 0.0004419110482558608\n",
      "  batch 7 loss: 0.0004242865543346852\n",
      "  batch 8 loss: 0.00044026150135323405\n",
      "  batch 9 loss: 0.0006511339452117682\n",
      "  batch 10 loss: 0.0006152749992907047\n",
      "  batch 11 loss: 0.0009033660171553493\n",
      "  batch 12 loss: 0.0008521267445757985\n",
      "  batch 13 loss: 0.000722561264410615\n",
      "  batch 14 loss: 0.001362716080620885\n",
      "  batch 15 loss: 0.0010310093639418483\n",
      "  batch 16 loss: 0.0007224414148367941\n",
      "  batch 17 loss: 0.0008548200712539256\n",
      "  batch 18 loss: 0.0007781253661960363\n",
      "  batch 19 loss: 0.0011891541071236134\n",
      "  batch 20 loss: 0.0012715836055576801\n",
      "  batch 21 loss: 0.0011352455476298928\n",
      "  batch 22 loss: 0.001004940364509821\n",
      "  batch 23 loss: 0.0010727380868047476\n",
      "  batch 24 loss: 0.00099751609377563\n",
      "  batch 25 loss: 0.0011203114408999681\n",
      "  batch 26 loss: 0.0008390040602535009\n",
      "  batch 27 loss: 0.0008781841606833041\n",
      "  batch 28 loss: 0.0008499089162796736\n",
      "  batch 29 loss: 0.0008003312977962196\n",
      "  batch 30 loss: 0.0006617160979658365\n",
      "  batch 31 loss: 0.0006147391395643353\n",
      "  batch 32 loss: 0.0007315397961065173\n",
      "  batch 33 loss: 0.0005655374843627214\n",
      "  batch 34 loss: 0.0006300606764853001\n",
      "  batch 35 loss: 0.0006104688509367406\n",
      "  batch 36 loss: 0.0006767299491912127\n",
      "  batch 37 loss: 0.000602266110945493\n",
      "  batch 38 loss: 0.000474798318464309\n",
      "  batch 39 loss: 0.000686226412653923\n",
      "  batch 40 loss: 0.0010954251047223806\n",
      "  batch 41 loss: 0.0004527496639639139\n",
      "  batch 42 loss: 0.0004228266188874841\n",
      "  batch 43 loss: 0.0006593074067495763\n",
      "  batch 44 loss: 0.0005335898604243994\n",
      "  batch 45 loss: 0.0005990599747747183\n",
      "  batch 46 loss: 0.0007950299186632037\n",
      "  batch 47 loss: 0.0007312080124393106\n",
      "  batch 48 loss: 0.0005638282163999975\n",
      "  batch 49 loss: 0.0005879494710825384\n",
      "  batch 50 loss: 0.0006414642557501793\n",
      "  batch 51 loss: 0.000837736064568162\n",
      "  batch 52 loss: 0.0007087104022502899\n",
      "  batch 53 loss: 0.0006798264803364873\n",
      "  batch 54 loss: 0.0007053847657516599\n",
      "  batch 55 loss: 0.0006734107155352831\n",
      "  batch 56 loss: 0.000563767331186682\n",
      "  batch 57 loss: 0.0006571086123585701\n",
      "  batch 58 loss: 0.0008996680844575167\n",
      "  batch 59 loss: 0.0009260629885829985\n",
      "  batch 60 loss: 0.0007704118033871055\n",
      "  batch 61 loss: 0.0007985365809872746\n",
      "  batch 62 loss: 0.0006262470269575715\n",
      "  batch 63 loss: 0.0006432487862184644\n",
      "  batch 64 loss: 0.0007094602915458381\n",
      "  batch 65 loss: 0.0007641682168468833\n",
      "  batch 66 loss: 0.000553650432266295\n",
      "  batch 67 loss: 0.0005580483702942729\n",
      "  batch 68 loss: 0.0005605641636066139\n",
      "  batch 69 loss: 0.0005838057259097695\n",
      "  batch 70 loss: 0.0006653140299022198\n",
      "  batch 71 loss: 0.0005047597223892808\n",
      "  batch 72 loss: 0.00045950349885970354\n",
      "  batch 73 loss: 0.00044558639638125896\n",
      "  batch 74 loss: 0.000511413614731282\n",
      "  batch 75 loss: 0.0007269781781360507\n",
      "  batch 76 loss: 0.0007135557243600488\n",
      "  batch 77 loss: 0.0006605023518204689\n",
      "  batch 78 loss: 0.0006336843362078071\n",
      "  batch 79 loss: 0.000613058393355459\n",
      "  batch 80 loss: 0.000661892700009048\n",
      "  batch 81 loss: 0.0006199978524819016\n",
      "  batch 82 loss: 0.0006485443445853889\n",
      "  batch 83 loss: 0.0006068286602385342\n",
      "  batch 84 loss: 0.0006226568948477507\n",
      "  batch 85 loss: 0.0006186167593114078\n",
      "  batch 86 loss: 0.0007140252273529768\n",
      "  batch 87 loss: 0.0007442555506713688\n",
      "  batch 88 loss: 0.0008348552510142326\n",
      "  batch 89 loss: 0.000620315782725811\n",
      "  batch 90 loss: 0.0007579707307741046\n",
      "  batch 91 loss: 0.0006875327089801431\n",
      "  batch 92 loss: 0.0006244854303076863\n",
      "  batch 93 loss: 0.0005494388751685619\n",
      "  batch 94 loss: 0.000666740583255887\n",
      "  batch 95 loss: 0.0007442454807460308\n",
      "LOSS train 0.0007442454807460308 valid 0.001134388381615281\n",
      "LOSS train 0.0007442454807460308 valid 0.0012689571594819427\n",
      "LOSS train 0.0007442454807460308 valid 0.0011700417380779982\n",
      "LOSS train 0.0007442454807460308 valid 0.0011419432703405619\n",
      "LOSS train 0.0007442454807460308 valid 0.001136920996941626\n",
      "LOSS train 0.0007442454807460308 valid 0.0011563239386305213\n",
      "LOSS train 0.0007442454807460308 valid 0.0012257552007213235\n",
      "LOSS train 0.0007442454807460308 valid 0.0012501322198659182\n",
      "LOSS train 0.0007442454807460308 valid 0.0012314565246924758\n",
      "LOSS train 0.0007442454807460308 valid 0.0012506073107942939\n",
      "LOSS train 0.0007442454807460308 valid 0.0012929815566167235\n",
      "LOSS train 0.0007442454807460308 valid 0.0012976082507520914\n",
      "LOSS train 0.0007442454807460308 valid 0.0012751115718856454\n",
      "LOSS train 0.0007442454807460308 valid 0.0012984664645045996\n",
      "LOSS train 0.0007442454807460308 valid 0.0013346197083592415\n",
      "LOSS train 0.0007442454807460308 valid 0.0013611289905384183\n",
      "LOSS train 0.0007442454807460308 valid 0.0013633770868182182\n",
      "LOSS train 0.0007442454807460308 valid 0.0013747889315709472\n",
      "LOSS train 0.0007442454807460308 valid 0.0013870162656530738\n",
      "LOSS train 0.0007442454807460308 valid 0.00139118661172688\n",
      "LOSS train 0.0007442454807460308 valid 0.0013835378922522068\n",
      "LOSS train 0.0007442454807460308 valid 0.001386974356137216\n",
      "LOSS train 0.0007442454807460308 valid 0.001393353333696723\n",
      "LOSS train 0.0007442454807460308 valid 0.0013917040778324008\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0005334907909855247\n",
      "  batch 2 loss: 0.0005992440273985267\n",
      "  batch 3 loss: 0.0007524987449869514\n",
      "  batch 4 loss: 0.0006160464254207909\n",
      "  batch 5 loss: 0.0004689009510912001\n",
      "  batch 6 loss: 0.00042033003410324454\n",
      "  batch 7 loss: 0.00039798126090317965\n",
      "  batch 8 loss: 0.00044002546928822994\n",
      "  batch 9 loss: 0.0006544574862346053\n",
      "  batch 10 loss: 0.0006104629719629884\n",
      "  batch 11 loss: 0.0009099571034312248\n",
      "  batch 12 loss: 0.0008508276659995317\n",
      "  batch 13 loss: 0.0006992575945332646\n",
      "  batch 14 loss: 0.0012588195968419313\n",
      "  batch 15 loss: 0.0010169717716053128\n",
      "  batch 16 loss: 0.0007875444134697318\n",
      "  batch 17 loss: 0.0008061649859882891\n",
      "  batch 18 loss: 0.0007903396035544574\n",
      "  batch 19 loss: 0.0010859905742108822\n",
      "  batch 20 loss: 0.001137029379606247\n",
      "  batch 21 loss: 0.001057446002960205\n",
      "  batch 22 loss: 0.000968947890214622\n",
      "  batch 23 loss: 0.001053379848599434\n",
      "  batch 24 loss: 0.0009468710049986839\n",
      "  batch 25 loss: 0.0010494883172214031\n",
      "  batch 26 loss: 0.0008386437548324466\n",
      "  batch 27 loss: 0.0008180119912140071\n",
      "  batch 28 loss: 0.0008476038929075003\n",
      "  batch 29 loss: 0.0007959539070725441\n",
      "  batch 30 loss: 0.000674102921038866\n",
      "  batch 31 loss: 0.0006311796605587006\n",
      "  batch 32 loss: 0.0007365326746366918\n",
      "  batch 33 loss: 0.0005795645993202925\n",
      "  batch 34 loss: 0.0006013101083226502\n",
      "  batch 35 loss: 0.0005646910285577178\n",
      "  batch 36 loss: 0.0005833406466990709\n",
      "  batch 37 loss: 0.0006469474174082279\n",
      "  batch 38 loss: 0.000492764578666538\n",
      "  batch 39 loss: 0.0007150777382776141\n",
      "  batch 40 loss: 0.0010411038529127836\n",
      "  batch 41 loss: 0.0004471616121008992\n",
      "  batch 42 loss: 0.0004423761274665594\n",
      "  batch 43 loss: 0.0006972927367314696\n",
      "  batch 44 loss: 0.0005479584215208888\n",
      "  batch 45 loss: 0.0006067282520234585\n",
      "  batch 46 loss: 0.0007082604570314288\n",
      "  batch 47 loss: 0.0006542386836372316\n",
      "  batch 48 loss: 0.0005290942499414086\n",
      "  batch 49 loss: 0.0005763216177001595\n",
      "  batch 50 loss: 0.0006317966617643833\n",
      "  batch 51 loss: 0.0007317443378269672\n",
      "  batch 52 loss: 0.0006220454815775156\n",
      "  batch 53 loss: 0.000613986689131707\n",
      "  batch 54 loss: 0.0007113849278539419\n",
      "  batch 55 loss: 0.0006495374254882336\n",
      "  batch 56 loss: 0.0004977186908945441\n",
      "  batch 57 loss: 0.0005470623727887869\n",
      "  batch 58 loss: 0.0008638801518827677\n",
      "  batch 59 loss: 0.0009645400568842888\n",
      "  batch 60 loss: 0.0007156279752962291\n",
      "  batch 61 loss: 0.0007398570305667818\n",
      "  batch 62 loss: 0.0005516719538718462\n",
      "  batch 63 loss: 0.0005545244785025716\n",
      "  batch 64 loss: 0.0006225457764230669\n",
      "  batch 65 loss: 0.0007429664256051183\n",
      "  batch 66 loss: 0.0005002223188057542\n",
      "  batch 67 loss: 0.0005169727373868227\n",
      "  batch 68 loss: 0.0005194061086513102\n",
      "  batch 69 loss: 0.0005858482327312231\n",
      "  batch 70 loss: 0.0006539293099194765\n",
      "  batch 71 loss: 0.00048394029727205634\n",
      "  batch 72 loss: 0.00046689200098626316\n",
      "  batch 73 loss: 0.0004536204505711794\n",
      "  batch 74 loss: 0.0005074140499345958\n",
      "  batch 75 loss: 0.0006735292263329029\n",
      "  batch 76 loss: 0.000660877616610378\n",
      "  batch 77 loss: 0.0006378534017130733\n",
      "  batch 78 loss: 0.0006212295265868306\n",
      "  batch 79 loss: 0.0006074659177102149\n",
      "  batch 80 loss: 0.0006477001588791609\n",
      "  batch 81 loss: 0.0005996064282953739\n",
      "  batch 82 loss: 0.0006365241715684533\n",
      "  batch 83 loss: 0.000580654654186219\n",
      "  batch 84 loss: 0.0005855016643181443\n",
      "  batch 85 loss: 0.0005805058171972632\n",
      "  batch 86 loss: 0.0007041487260721624\n",
      "  batch 87 loss: 0.0007362486212514341\n",
      "  batch 88 loss: 0.0008180063450708985\n",
      "  batch 89 loss: 0.0006311520119197667\n",
      "  batch 90 loss: 0.0006974596763029695\n",
      "  batch 91 loss: 0.0006777386297471821\n",
      "  batch 92 loss: 0.0006096700672060251\n",
      "  batch 93 loss: 0.0005714403232559562\n",
      "  batch 94 loss: 0.0006465588812716305\n",
      "  batch 95 loss: 0.000695802504196763\n",
      "LOSS train 0.000695802504196763 valid 0.0009342092089354992\n",
      "LOSS train 0.000695802504196763 valid 0.0010948049603030086\n",
      "LOSS train 0.000695802504196763 valid 0.001030594459734857\n",
      "LOSS train 0.000695802504196763 valid 0.0010142144747078419\n",
      "LOSS train 0.000695802504196763 valid 0.0010143350809812546\n",
      "LOSS train 0.000695802504196763 valid 0.0010193506022915244\n",
      "LOSS train 0.000695802504196763 valid 0.0010884180665016174\n",
      "LOSS train 0.000695802504196763 valid 0.0011196898994967341\n",
      "LOSS train 0.000695802504196763 valid 0.0011060793185606599\n",
      "LOSS train 0.000695802504196763 valid 0.001126880873925984\n",
      "LOSS train 0.000695802504196763 valid 0.0011525109875947237\n",
      "LOSS train 0.000695802504196763 valid 0.001155766542069614\n",
      "LOSS train 0.000695802504196763 valid 0.0011376432375982404\n",
      "LOSS train 0.000695802504196763 valid 0.001160187995992601\n",
      "LOSS train 0.000695802504196763 valid 0.0011964181903749704\n",
      "LOSS train 0.000695802504196763 valid 0.0012233530869707465\n",
      "LOSS train 0.000695802504196763 valid 0.001221262733452022\n",
      "LOSS train 0.000695802504196763 valid 0.001232456648722291\n",
      "LOSS train 0.000695802504196763 valid 0.001246749539859593\n",
      "LOSS train 0.000695802504196763 valid 0.001252688467502594\n",
      "LOSS train 0.000695802504196763 valid 0.0012481320882216096\n",
      "LOSS train 0.000695802504196763 valid 0.001254499307833612\n",
      "LOSS train 0.000695802504196763 valid 0.0012626051902770996\n",
      "LOSS train 0.000695802504196763 valid 0.001262902864255011\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 0.0005286135128699243\n",
      "  batch 2 loss: 0.0005920524708926678\n",
      "  batch 3 loss: 0.0007125705596990883\n",
      "  batch 4 loss: 0.0006364990258589387\n",
      "  batch 5 loss: 0.0004710354842245579\n",
      "  batch 6 loss: 0.00044106293353252113\n",
      "  batch 7 loss: 0.0004103428218513727\n",
      "  batch 8 loss: 0.000422945071477443\n",
      "  batch 9 loss: 0.0006151304114609957\n",
      "  batch 10 loss: 0.0005692748818546534\n",
      "  batch 11 loss: 0.000852141180075705\n",
      "  batch 12 loss: 0.0008092690259218216\n",
      "  batch 13 loss: 0.0006839034031145275\n",
      "  batch 14 loss: 0.001269698143005371\n",
      "  batch 15 loss: 0.0009168469696305692\n",
      "  batch 16 loss: 0.0007050955900922418\n",
      "  batch 17 loss: 0.0009597173193469644\n",
      "  batch 18 loss: 0.000717798771802336\n",
      "  batch 19 loss: 0.0010904348455369473\n",
      "  batch 20 loss: 0.0011218443978577852\n",
      "  batch 21 loss: 0.0010464551160112023\n",
      "  batch 22 loss: 0.0009820838458836079\n",
      "  batch 23 loss: 0.0009948352817445993\n",
      "  batch 24 loss: 0.0009216550970450044\n",
      "  batch 25 loss: 0.0010145676787942648\n",
      "  batch 26 loss: 0.0008447809377685189\n",
      "  batch 27 loss: 0.0008570338832214475\n",
      "  batch 28 loss: 0.0009094998822547495\n",
      "  batch 29 loss: 0.0007844678475521505\n",
      "  batch 30 loss: 0.0007239705300889909\n",
      "  batch 31 loss: 0.0006797642563469708\n",
      "  batch 32 loss: 0.0007377049187198281\n",
      "  batch 33 loss: 0.0006584971561096609\n",
      "  batch 34 loss: 0.0006295070052146912\n",
      "  batch 35 loss: 0.000614392280112952\n",
      "  batch 36 loss: 0.0005952088395133615\n",
      "  batch 37 loss: 0.0006048371433280408\n",
      "  batch 38 loss: 0.00048330752179026604\n",
      "  batch 39 loss: 0.0006622087676078081\n",
      "  batch 40 loss: 0.001038496382534504\n",
      "  batch 41 loss: 0.0004196900699753314\n",
      "  batch 42 loss: 0.0003995663137175143\n",
      "  batch 43 loss: 0.0006725126877427101\n",
      "  batch 44 loss: 0.000521694018971175\n",
      "  batch 45 loss: 0.0005666700308211148\n",
      "  batch 46 loss: 0.0007472999859601259\n",
      "  batch 47 loss: 0.0007007516687735915\n",
      "  batch 48 loss: 0.000538801250513643\n",
      "  batch 49 loss: 0.0005799286300316453\n",
      "  batch 50 loss: 0.0006236789049580693\n",
      "  batch 51 loss: 0.0007044168887659907\n",
      "  batch 52 loss: 0.0006507497746497393\n",
      "  batch 53 loss: 0.000636344135273248\n",
      "  batch 54 loss: 0.000766260433010757\n",
      "  batch 55 loss: 0.0006913293618708849\n",
      "  batch 56 loss: 0.0005272508133202791\n",
      "  batch 57 loss: 0.0005278488388285041\n",
      "  batch 58 loss: 0.000864301691763103\n",
      "  batch 59 loss: 0.0010109120048582554\n",
      "  batch 60 loss: 0.0006977571174502373\n",
      "  batch 61 loss: 0.0007607075967825949\n",
      "  batch 62 loss: 0.0005783698870800436\n",
      "  batch 63 loss: 0.0005742727080360055\n",
      "  batch 64 loss: 0.0006666823755949736\n",
      "  batch 65 loss: 0.0007497754413634539\n",
      "  batch 66 loss: 0.0005223738262429833\n",
      "  batch 67 loss: 0.0005304087535478175\n",
      "  batch 68 loss: 0.0005447760340757668\n",
      "  batch 69 loss: 0.0005457529332488775\n",
      "  batch 70 loss: 0.0006586392992176116\n",
      "  batch 71 loss: 0.0005035221111029387\n",
      "  batch 72 loss: 0.00048224785132333636\n",
      "  batch 73 loss: 0.000479582668049261\n",
      "  batch 74 loss: 0.0005120895802974701\n",
      "  batch 75 loss: 0.0006806252640672028\n",
      "  batch 76 loss: 0.0006096056895330548\n",
      "  batch 77 loss: 0.000615950208157301\n",
      "  batch 78 loss: 0.000598146696574986\n",
      "  batch 79 loss: 0.000570368894841522\n",
      "  batch 80 loss: 0.0006247848505154252\n",
      "  batch 81 loss: 0.0005754524026997387\n",
      "  batch 82 loss: 0.0006190597778186202\n",
      "  batch 83 loss: 0.0005752891302108765\n",
      "  batch 84 loss: 0.0005606884369626641\n",
      "  batch 85 loss: 0.0005522967549040914\n",
      "  batch 86 loss: 0.0006633182056248188\n",
      "  batch 87 loss: 0.00069010304287076\n",
      "  batch 88 loss: 0.0007747435593046248\n",
      "  batch 89 loss: 0.0005809123395010829\n",
      "  batch 90 loss: 0.0006797158857807517\n",
      "  batch 91 loss: 0.0006941537139937282\n",
      "  batch 92 loss: 0.0005926643498241901\n",
      "  batch 93 loss: 0.0005364799872040749\n",
      "  batch 94 loss: 0.0006489126826636493\n",
      "  batch 95 loss: 0.0007156017236411572\n",
      "LOSS train 0.0007156017236411572 valid 0.0010490294080227613\n",
      "LOSS train 0.0007156017236411572 valid 0.0012205304810777307\n",
      "LOSS train 0.0007156017236411572 valid 0.0011050463654100895\n",
      "LOSS train 0.0007156017236411572 valid 0.0010858005844056606\n",
      "LOSS train 0.0007156017236411572 valid 0.0010904320515692234\n",
      "LOSS train 0.0007156017236411572 valid 0.0011059490498155355\n",
      "LOSS train 0.0007156017236411572 valid 0.001191275892779231\n",
      "LOSS train 0.0007156017236411572 valid 0.0012207221006974578\n",
      "LOSS train 0.0007156017236411572 valid 0.0011989000486209989\n",
      "LOSS train 0.0007156017236411572 valid 0.0012240918586030602\n",
      "LOSS train 0.0007156017236411572 valid 0.0012652543373405933\n",
      "LOSS train 0.0007156017236411572 valid 0.0012688360875472426\n",
      "LOSS train 0.0007156017236411572 valid 0.0012417662655934691\n",
      "LOSS train 0.0007156017236411572 valid 0.0012672095326706767\n",
      "LOSS train 0.0007156017236411572 valid 0.0013151820749044418\n",
      "LOSS train 0.0007156017236411572 valid 0.0013475108426064253\n",
      "LOSS train 0.0007156017236411572 valid 0.0013488701079040766\n",
      "LOSS train 0.0007156017236411572 valid 0.0013616901123896241\n",
      "LOSS train 0.0007156017236411572 valid 0.00137315911706537\n",
      "LOSS train 0.0007156017236411572 valid 0.0013848430244252086\n",
      "LOSS train 0.0007156017236411572 valid 0.0013843089109286666\n",
      "LOSS train 0.0007156017236411572 valid 0.0013877098681405187\n",
      "LOSS train 0.0007156017236411572 valid 0.001398997032083571\n",
      "LOSS train 0.0007156017236411572 valid 0.0014008160214871168\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.0004997290670871735\n",
      "  batch 2 loss: 0.000593809294514358\n",
      "  batch 3 loss: 0.0007247509784065187\n",
      "  batch 4 loss: 0.0005911313928663731\n",
      "  batch 5 loss: 0.00048063870053738356\n",
      "  batch 6 loss: 0.00045077718095853925\n",
      "  batch 7 loss: 0.0004078138736076653\n",
      "  batch 8 loss: 0.0004192405613139272\n",
      "  batch 9 loss: 0.000597205595113337\n",
      "  batch 10 loss: 0.000552249257452786\n",
      "  batch 11 loss: 0.0008417522767558694\n",
      "  batch 12 loss: 0.000807669130153954\n",
      "  batch 13 loss: 0.000658493023365736\n",
      "  batch 14 loss: 0.0012450695503503084\n",
      "  batch 15 loss: 0.0009723780676722527\n",
      "  batch 16 loss: 0.000689338194206357\n",
      "  batch 17 loss: 0.0007569565786980093\n",
      "  batch 18 loss: 0.000714463647454977\n",
      "  batch 19 loss: 0.0010689884657040238\n",
      "  batch 20 loss: 0.0010763176251202822\n",
      "  batch 21 loss: 0.0010669628391042352\n",
      "  batch 22 loss: 0.0008762381039559841\n",
      "  batch 23 loss: 0.0009571308037266135\n",
      "  batch 24 loss: 0.0008599439170211554\n",
      "  batch 25 loss: 0.0009398798574693501\n",
      "  batch 26 loss: 0.0008127644541673362\n",
      "  batch 27 loss: 0.0007600190001539886\n",
      "  batch 28 loss: 0.0008279791800305247\n",
      "  batch 29 loss: 0.0007290202775038779\n",
      "  batch 30 loss: 0.0006699716905131936\n",
      "  batch 31 loss: 0.0006248702411539853\n",
      "  batch 32 loss: 0.0006901668384671211\n",
      "  batch 33 loss: 0.0005959179252386093\n",
      "  batch 34 loss: 0.0005707322852686048\n",
      "  batch 35 loss: 0.0005300531629472971\n",
      "  batch 36 loss: 0.0005532383220270276\n",
      "  batch 37 loss: 0.0005189037183299661\n",
      "  batch 38 loss: 0.0004329415096435696\n",
      "  batch 39 loss: 0.0005754104349762201\n",
      "  batch 40 loss: 0.0010769490618258715\n",
      "  batch 41 loss: 0.000393746595364064\n",
      "  batch 42 loss: 0.00033930863719433546\n",
      "  batch 43 loss: 0.0005856528878211975\n",
      "  batch 44 loss: 0.0004578432999551296\n",
      "  batch 45 loss: 0.000535482307896018\n",
      "  batch 46 loss: 0.0007138727232813835\n",
      "  batch 47 loss: 0.0006331618642434478\n",
      "  batch 48 loss: 0.0004979979712516069\n",
      "  batch 49 loss: 0.0005300087505020201\n",
      "  batch 50 loss: 0.0005541108548641205\n",
      "  batch 51 loss: 0.0006507844082079828\n",
      "  batch 52 loss: 0.0005722979549318552\n",
      "  batch 53 loss: 0.0005687949014827609\n",
      "  batch 54 loss: 0.0006933751865290105\n",
      "  batch 55 loss: 0.0006346026202663779\n",
      "  batch 56 loss: 0.0004951049922965467\n",
      "  batch 57 loss: 0.00047408154932782054\n",
      "  batch 58 loss: 0.0007989594014361501\n",
      "  batch 59 loss: 0.0009929490042850375\n",
      "  batch 60 loss: 0.000648935791105032\n",
      "  batch 61 loss: 0.0006755237700417638\n",
      "  batch 62 loss: 0.0004931587027385831\n",
      "  batch 63 loss: 0.0004718807467725128\n",
      "  batch 64 loss: 0.0005384805845096707\n",
      "  batch 65 loss: 0.0006499025039374828\n",
      "  batch 66 loss: 0.00046494381967931986\n",
      "  batch 67 loss: 0.00044632278149947524\n",
      "  batch 68 loss: 0.0004895410384051502\n",
      "  batch 69 loss: 0.000496195862069726\n",
      "  batch 70 loss: 0.000586599693633616\n",
      "  batch 71 loss: 0.00043223798274993896\n",
      "  batch 72 loss: 0.00040849740616977215\n",
      "  batch 73 loss: 0.00039373492472805083\n",
      "  batch 74 loss: 0.00047190592158585787\n",
      "  batch 75 loss: 0.0006387861212715507\n",
      "  batch 76 loss: 0.0005693902494385839\n",
      "  batch 77 loss: 0.0005474709905683994\n",
      "  batch 78 loss: 0.0005250879330560565\n",
      "  batch 79 loss: 0.0004628642927855253\n",
      "  batch 80 loss: 0.0005411622114479542\n",
      "  batch 81 loss: 0.0004953971365466714\n",
      "  batch 82 loss: 0.0005351697327569127\n",
      "  batch 83 loss: 0.0004957934143021703\n",
      "  batch 84 loss: 0.0004722324083559215\n",
      "  batch 85 loss: 0.0004924809327349067\n",
      "  batch 86 loss: 0.0005715942825190723\n",
      "  batch 87 loss: 0.0006125062936916947\n",
      "  batch 88 loss: 0.0007860468467697501\n",
      "  batch 89 loss: 0.0005376872140914202\n",
      "  batch 90 loss: 0.0006162326317280531\n",
      "  batch 91 loss: 0.0006640724604949355\n",
      "  batch 92 loss: 0.0005140446592122316\n",
      "  batch 93 loss: 0.0004817449953407049\n",
      "  batch 94 loss: 0.0005497571546584368\n",
      "  batch 95 loss: 0.0006803511641919613\n",
      "LOSS train 0.0006803511641919613 valid 0.001006033387966454\n",
      "LOSS train 0.0006803511641919613 valid 0.001151924254372716\n",
      "LOSS train 0.0006803511641919613 valid 0.0010226541198790073\n",
      "LOSS train 0.0006803511641919613 valid 0.0009942384203895926\n",
      "LOSS train 0.0006803511641919613 valid 0.00102167425211519\n",
      "LOSS train 0.0006803511641919613 valid 0.0010549236321821809\n",
      "LOSS train 0.0006803511641919613 valid 0.0011423028772696853\n",
      "LOSS train 0.0006803511641919613 valid 0.0011732750572264194\n",
      "LOSS train 0.0006803511641919613 valid 0.0011556589743122458\n",
      "LOSS train 0.0006803511641919613 valid 0.001180001418106258\n",
      "LOSS train 0.0006803511641919613 valid 0.001218179240822792\n",
      "LOSS train 0.0006803511641919613 valid 0.0012250705622136593\n",
      "LOSS train 0.0006803511641919613 valid 0.001199869206175208\n",
      "LOSS train 0.0006803511641919613 valid 0.0012276556808501482\n",
      "LOSS train 0.0006803511641919613 valid 0.0012968567898496985\n",
      "LOSS train 0.0006803511641919613 valid 0.001337571651674807\n",
      "LOSS train 0.0006803511641919613 valid 0.001345412340015173\n",
      "LOSS train 0.0006803511641919613 valid 0.001361763570457697\n",
      "LOSS train 0.0006803511641919613 valid 0.0013770299265161157\n",
      "LOSS train 0.0006803511641919613 valid 0.0013722473522648215\n",
      "LOSS train 0.0006803511641919613 valid 0.0013619548408314586\n",
      "LOSS train 0.0006803511641919613 valid 0.0013673709472641349\n",
      "LOSS train 0.0006803511641919613 valid 0.001375803374685347\n",
      "LOSS train 0.0006803511641919613 valid 0.0013694091467186809\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0005202266620472074\n",
      "  batch 2 loss: 0.0006161801866255701\n",
      "  batch 3 loss: 0.0007095519686117768\n",
      "  batch 4 loss: 0.0005844520637765527\n",
      "  batch 5 loss: 0.00043874350376427174\n",
      "  batch 6 loss: 0.0003516153956297785\n",
      "  batch 7 loss: 0.00037336270906962454\n",
      "  batch 8 loss: 0.00039951945655047894\n",
      "  batch 9 loss: 0.0006055684061720967\n",
      "  batch 10 loss: 0.0005353372544050217\n",
      "  batch 11 loss: 0.0007990107405930758\n",
      "  batch 12 loss: 0.0007502046646550298\n",
      "  batch 13 loss: 0.00065664725843817\n",
      "  batch 14 loss: 0.00114923226647079\n",
      "  batch 15 loss: 0.0007969391299411654\n",
      "  batch 16 loss: 0.0006291502504609525\n",
      "  batch 17 loss: 0.0008068743627518415\n",
      "  batch 18 loss: 0.0006585189257748425\n",
      "  batch 19 loss: 0.0010493306908756495\n",
      "  batch 20 loss: 0.0010772927198559046\n",
      "  batch 21 loss: 0.001036434550769627\n",
      "  batch 22 loss: 0.0008438945515081286\n",
      "  batch 23 loss: 0.0009624267695471644\n",
      "  batch 24 loss: 0.0007854241994209588\n",
      "  batch 25 loss: 0.0008981212740764022\n",
      "  batch 26 loss: 0.0008049997268244624\n",
      "  batch 27 loss: 0.0007386341458186507\n",
      "  batch 28 loss: 0.0009220464853569865\n",
      "  batch 29 loss: 0.0007017855532467365\n",
      "  batch 30 loss: 0.0006887075724080205\n",
      "  batch 31 loss: 0.0006713845068588853\n",
      "  batch 32 loss: 0.0006741905817762017\n",
      "  batch 33 loss: 0.0005878398660570383\n",
      "  batch 34 loss: 0.0005868077860213816\n",
      "  batch 35 loss: 0.00053452979773283\n",
      "  batch 36 loss: 0.0005614424007944763\n",
      "  batch 37 loss: 0.0005153700476512313\n",
      "  batch 38 loss: 0.000411597837228328\n",
      "  batch 39 loss: 0.0005244670901447535\n",
      "  batch 40 loss: 0.0008729981491342187\n",
      "  batch 41 loss: 0.00037251412868499756\n",
      "  batch 42 loss: 0.00034722386044450104\n",
      "  batch 43 loss: 0.0005487840389832854\n",
      "  batch 44 loss: 0.0004286810290068388\n",
      "  batch 45 loss: 0.00047975851339288056\n",
      "  batch 46 loss: 0.0006730814930051565\n",
      "  batch 47 loss: 0.0006267002318054438\n",
      "  batch 48 loss: 0.00045169569784775376\n",
      "  batch 49 loss: 0.0004976334166713059\n",
      "  batch 50 loss: 0.0005352802108973265\n",
      "  batch 51 loss: 0.000597710779402405\n",
      "  batch 52 loss: 0.0005177825223654509\n",
      "  batch 53 loss: 0.0004923092201352119\n",
      "  batch 54 loss: 0.0005863510887138546\n",
      "  batch 55 loss: 0.00060647027567029\n",
      "  batch 56 loss: 0.00048165899352170527\n",
      "  batch 57 loss: 0.0004256827523931861\n",
      "  batch 58 loss: 0.0007726437179371715\n",
      "  batch 59 loss: 0.0008888610755093396\n",
      "  batch 60 loss: 0.0006594000151380897\n",
      "  batch 61 loss: 0.0005782675580121577\n",
      "  batch 62 loss: 0.0004694010131061077\n",
      "  batch 63 loss: 0.0004227185854688287\n",
      "  batch 64 loss: 0.00046689267037436366\n",
      "  batch 65 loss: 0.0005508158355951309\n",
      "  batch 66 loss: 0.0004197179223410785\n",
      "  batch 67 loss: 0.0004107614513486624\n",
      "  batch 68 loss: 0.0004491744330152869\n",
      "  batch 69 loss: 0.00044524658005684614\n",
      "  batch 70 loss: 0.0005257531302049756\n",
      "  batch 71 loss: 0.00038546352880075574\n",
      "  batch 72 loss: 0.00036536058178171515\n",
      "  batch 73 loss: 0.0003725682327058166\n",
      "  batch 74 loss: 0.00041022279765456915\n",
      "  batch 75 loss: 0.0005679595633409917\n",
      "  batch 76 loss: 0.00047791708493605256\n",
      "  batch 77 loss: 0.0004343792097643018\n",
      "  batch 78 loss: 0.00047653703950345516\n",
      "  batch 79 loss: 0.0004001995548605919\n",
      "  batch 80 loss: 0.000500930065754801\n",
      "  batch 81 loss: 0.0004233173676766455\n",
      "  batch 82 loss: 0.00046759427641518414\n",
      "  batch 83 loss: 0.0004539557558018714\n",
      "  batch 84 loss: 0.0004052964213769883\n",
      "  batch 85 loss: 0.00045078794937580824\n",
      "  batch 86 loss: 0.0004952653544023633\n",
      "  batch 87 loss: 0.0005436167703010142\n",
      "  batch 88 loss: 0.0007311993977054954\n",
      "  batch 89 loss: 0.0005163830355741084\n",
      "  batch 90 loss: 0.0006161949131637812\n",
      "  batch 91 loss: 0.0005886135040782392\n",
      "  batch 92 loss: 0.0004994426853954792\n",
      "  batch 93 loss: 0.0004693963273894042\n",
      "  batch 94 loss: 0.0005104315932840109\n",
      "  batch 95 loss: 0.0006552867707796395\n",
      "LOSS train 0.0006552867707796395 valid 0.0008768434636294842\n",
      "LOSS train 0.0006552867707796395 valid 0.0010413448326289654\n",
      "LOSS train 0.0006552867707796395 valid 0.0009379800176247954\n",
      "LOSS train 0.0006552867707796395 valid 0.0009294224437326193\n",
      "LOSS train 0.0006552867707796395 valid 0.000932051450945437\n",
      "LOSS train 0.0006552867707796395 valid 0.0009372028289362788\n",
      "LOSS train 0.0006552867707796395 valid 0.001001492841169238\n",
      "LOSS train 0.0006552867707796395 valid 0.0010307751363143325\n",
      "LOSS train 0.0006552867707796395 valid 0.0010135627817362547\n",
      "LOSS train 0.0006552867707796395 valid 0.0010301065631210804\n",
      "LOSS train 0.0006552867707796395 valid 0.001057630404829979\n",
      "LOSS train 0.0006552867707796395 valid 0.0010567998979240656\n",
      "LOSS train 0.0006552867707796395 valid 0.0010412452975288033\n",
      "LOSS train 0.0006552867707796395 valid 0.001059223315678537\n",
      "LOSS train 0.0006552867707796395 valid 0.0011075164657086134\n",
      "LOSS train 0.0006552867707796395 valid 0.0011367648839950562\n",
      "LOSS train 0.0006552867707796395 valid 0.0011373029556125402\n",
      "LOSS train 0.0006552867707796395 valid 0.0011472641490399837\n",
      "LOSS train 0.0006552867707796395 valid 0.0011596103431656957\n",
      "LOSS train 0.0006552867707796395 valid 0.0011642955942079425\n",
      "LOSS train 0.0006552867707796395 valid 0.001157631166279316\n",
      "LOSS train 0.0006552867707796395 valid 0.0011672956170514226\n",
      "LOSS train 0.0006552867707796395 valid 0.0011746023083105683\n",
      "LOSS train 0.0006552867707796395 valid 0.0011759100016206503\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0004809384699910879\n",
      "  batch 2 loss: 0.0005455184145830572\n",
      "  batch 3 loss: 0.0007028409163467586\n",
      "  batch 4 loss: 0.0005423931870609522\n",
      "  batch 5 loss: 0.00043018453288823366\n",
      "  batch 6 loss: 0.00037399749271571636\n",
      "  batch 7 loss: 0.0003460656735114753\n",
      "  batch 8 loss: 0.00038663981831632555\n",
      "  batch 9 loss: 0.0005316780298016965\n",
      "  batch 10 loss: 0.00044843737850897014\n",
      "  batch 11 loss: 0.000741094583645463\n",
      "  batch 12 loss: 0.0007291255169548094\n",
      "  batch 13 loss: 0.0005907989689148962\n",
      "  batch 14 loss: 0.0011085073929280043\n",
      "  batch 15 loss: 0.0008041628170758486\n",
      "  batch 16 loss: 0.0006335676880553365\n",
      "  batch 17 loss: 0.0008548077312298119\n",
      "  batch 18 loss: 0.0006644708337262273\n",
      "  batch 19 loss: 0.0010677734389901161\n",
      "  batch 20 loss: 0.0010103490203619003\n",
      "  batch 21 loss: 0.0010153604671359062\n",
      "  batch 22 loss: 0.0007788771763443947\n",
      "  batch 23 loss: 0.0009144748910330236\n",
      "  batch 24 loss: 0.0007865360239520669\n",
      "  batch 25 loss: 0.0008606507908552885\n",
      "  batch 26 loss: 0.0007613848429173231\n",
      "  batch 27 loss: 0.0006607410032302141\n",
      "  batch 28 loss: 0.0008333366131410003\n",
      "  batch 29 loss: 0.0005822359235025942\n",
      "  batch 30 loss: 0.0006315833888947964\n",
      "  batch 31 loss: 0.0005987213808111846\n",
      "  batch 32 loss: 0.0006271771853789687\n",
      "  batch 33 loss: 0.000587673275731504\n",
      "  batch 34 loss: 0.0005604641046375036\n",
      "  batch 35 loss: 0.0004890650743618608\n",
      "  batch 36 loss: 0.0005157970590516925\n",
      "  batch 37 loss: 0.000481581490021199\n",
      "  batch 38 loss: 0.00040693581104278564\n",
      "  batch 39 loss: 0.0004869388067163527\n",
      "  batch 40 loss: 0.0008283324423246086\n",
      "  batch 41 loss: 0.00038682317244820297\n",
      "  batch 42 loss: 0.0003374628722667694\n",
      "  batch 43 loss: 0.0004830457619391382\n",
      "  batch 44 loss: 0.0003837738186120987\n",
      "  batch 45 loss: 0.00046883037430234253\n",
      "  batch 46 loss: 0.0006324256537482142\n",
      "  batch 47 loss: 0.0005121560534462333\n",
      "  batch 48 loss: 0.00033867877209559083\n",
      "  batch 49 loss: 0.0004014940932393074\n",
      "  batch 50 loss: 0.00045584156760014594\n",
      "  batch 51 loss: 0.0005106389871798456\n",
      "  batch 52 loss: 0.0004185462021268904\n",
      "  batch 53 loss: 0.00041838287143036723\n",
      "  batch 54 loss: 0.000524448580108583\n",
      "  batch 55 loss: 0.0005166772753000259\n",
      "  batch 56 loss: 0.0004415703588165343\n",
      "  batch 57 loss: 0.00036698830081149936\n",
      "  batch 58 loss: 0.0007243694271892309\n",
      "  batch 59 loss: 0.0009560369071550667\n",
      "  batch 60 loss: 0.0006048322538845241\n",
      "  batch 61 loss: 0.0005093322251923382\n",
      "  batch 62 loss: 0.0004318433639127761\n",
      "  batch 63 loss: 0.00035155925434082747\n",
      "  batch 64 loss: 0.0003864614409394562\n",
      "  batch 65 loss: 0.000447714701294899\n",
      "  batch 66 loss: 0.00036454026121646166\n",
      "  batch 67 loss: 0.0003548164386302233\n",
      "  batch 68 loss: 0.0004349132941570133\n",
      "  batch 69 loss: 0.00043895101407542825\n",
      "  batch 70 loss: 0.0004910165444016457\n",
      "  batch 71 loss: 0.0003715101338457316\n",
      "  batch 72 loss: 0.0003411741927266121\n",
      "  batch 73 loss: 0.0003484795452095568\n",
      "  batch 74 loss: 0.00038369145477190614\n",
      "  batch 75 loss: 0.0005539003759622574\n",
      "  batch 76 loss: 0.00048376337508670986\n",
      "  batch 77 loss: 0.0004233777872286737\n",
      "  batch 78 loss: 0.00042305843089707196\n",
      "  batch 79 loss: 0.00036797061329707503\n",
      "  batch 80 loss: 0.00043409294448792934\n",
      "  batch 81 loss: 0.000397772149881348\n",
      "  batch 82 loss: 0.00042819854570552707\n",
      "  batch 83 loss: 0.0004316846316214651\n",
      "  batch 84 loss: 0.000354534771759063\n",
      "  batch 85 loss: 0.0004232330829836428\n",
      "  batch 86 loss: 0.00048588658683001995\n",
      "  batch 87 loss: 0.0004998856456950307\n",
      "  batch 88 loss: 0.0006738576921634376\n",
      "  batch 89 loss: 0.0005269424291327596\n",
      "  batch 90 loss: 0.0005987680633552372\n",
      "  batch 91 loss: 0.0005599670112133026\n",
      "  batch 92 loss: 0.00045735714957118034\n",
      "  batch 93 loss: 0.0004382742918096483\n",
      "  batch 94 loss: 0.0004931826260872185\n",
      "  batch 95 loss: 0.000649590918328613\n",
      "LOSS train 0.000649590918328613 valid 0.0008057325030677021\n",
      "LOSS train 0.000649590918328613 valid 0.0009425358148291707\n",
      "LOSS train 0.000649590918328613 valid 0.000860258936882019\n",
      "LOSS train 0.000649590918328613 valid 0.0008518275571987033\n",
      "LOSS train 0.000649590918328613 valid 0.0008461456745862961\n",
      "LOSS train 0.000649590918328613 valid 0.0008394927135668695\n",
      "LOSS train 0.000649590918328613 valid 0.0008999581332318485\n",
      "LOSS train 0.000649590918328613 valid 0.0009190620621666312\n",
      "LOSS train 0.000649590918328613 valid 0.0008966131135821342\n",
      "LOSS train 0.000649590918328613 valid 0.0009123404743149877\n",
      "LOSS train 0.000649590918328613 valid 0.0009418677072972059\n",
      "LOSS train 0.000649590918328613 valid 0.0009456714033149183\n",
      "LOSS train 0.000649590918328613 valid 0.0009312891634181142\n",
      "LOSS train 0.000649590918328613 valid 0.0009489376097917557\n",
      "LOSS train 0.000649590918328613 valid 0.0009918169816955924\n",
      "LOSS train 0.000649590918328613 valid 0.0010168596636503935\n",
      "LOSS train 0.000649590918328613 valid 0.0010175619972869754\n",
      "LOSS train 0.000649590918328613 valid 0.0010255021043121815\n",
      "LOSS train 0.000649590918328613 valid 0.0010337956482544541\n",
      "LOSS train 0.000649590918328613 valid 0.0010451229754835367\n",
      "LOSS train 0.000649590918328613 valid 0.0010411515831947327\n",
      "LOSS train 0.000649590918328613 valid 0.001051089959219098\n",
      "LOSS train 0.000649590918328613 valid 0.001058186637237668\n",
      "LOSS train 0.000649590918328613 valid 0.0010615348583087325\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0004307507479097694\n",
      "  batch 2 loss: 0.0004814170824829489\n",
      "  batch 3 loss: 0.0006399144767783582\n",
      "  batch 4 loss: 0.000488888006657362\n",
      "  batch 5 loss: 0.00037879461888223886\n",
      "  batch 6 loss: 0.00035208193003199995\n",
      "  batch 7 loss: 0.0003255684277974069\n",
      "  batch 8 loss: 0.00037086702650412917\n",
      "  batch 9 loss: 0.0004525348194874823\n",
      "  batch 10 loss: 0.0004069558926858008\n",
      "  batch 11 loss: 0.0006954062264412642\n",
      "  batch 12 loss: 0.0006784640718251467\n",
      "  batch 13 loss: 0.0005360882496461272\n",
      "  batch 14 loss: 0.0011166944168508053\n",
      "  batch 15 loss: 0.0007342952885665\n",
      "  batch 16 loss: 0.0005123309674672782\n",
      "  batch 17 loss: 0.0007029178086668253\n",
      "  batch 18 loss: 0.0005697566084563732\n",
      "  batch 19 loss: 0.0010370679665356874\n",
      "  batch 20 loss: 0.000959979195613414\n",
      "  batch 21 loss: 0.0010381960310041904\n",
      "  batch 22 loss: 0.0007712359074503183\n",
      "  batch 23 loss: 0.0009001368889585137\n",
      "  batch 24 loss: 0.0007488740375265479\n",
      "  batch 25 loss: 0.0008403606479987502\n",
      "  batch 26 loss: 0.0006965757929719985\n",
      "  batch 27 loss: 0.0006075837300159037\n",
      "  batch 28 loss: 0.0007500777719542384\n",
      "  batch 29 loss: 0.0005263154162093997\n",
      "  batch 30 loss: 0.0006020765868015587\n",
      "  batch 31 loss: 0.00056248321197927\n",
      "  batch 32 loss: 0.0005716746672987938\n",
      "  batch 33 loss: 0.0005856103962287307\n",
      "  batch 34 loss: 0.0005370028666220605\n",
      "  batch 35 loss: 0.0004823302442673594\n",
      "  batch 36 loss: 0.0004631365300156176\n",
      "  batch 37 loss: 0.0004246499156579375\n",
      "  batch 38 loss: 0.00037973374128341675\n",
      "  batch 39 loss: 0.0004562795511446893\n",
      "  batch 40 loss: 0.0006930993986316025\n",
      "  batch 41 loss: 0.0003772270865738392\n",
      "  batch 42 loss: 0.0003524953208398074\n",
      "  batch 43 loss: 0.000459098257124424\n",
      "  batch 44 loss: 0.0003628877457231283\n",
      "  batch 45 loss: 0.00039834954077377915\n",
      "  batch 46 loss: 0.0005261893384158611\n",
      "  batch 47 loss: 0.0004939663922414184\n",
      "  batch 48 loss: 0.00033784008701331913\n",
      "  batch 49 loss: 0.0003748902818188071\n",
      "  batch 50 loss: 0.0004102588281966746\n",
      "  batch 51 loss: 0.000478398404084146\n",
      "  batch 52 loss: 0.00038115738425403833\n",
      "  batch 53 loss: 0.00038870834396220744\n",
      "  batch 54 loss: 0.0004916440229862928\n",
      "  batch 55 loss: 0.0004690002533607185\n",
      "  batch 56 loss: 0.000418599956901744\n",
      "  batch 57 loss: 0.000341415754519403\n",
      "  batch 58 loss: 0.0006728011649101973\n",
      "  batch 59 loss: 0.0009028622880578041\n",
      "  batch 60 loss: 0.0005414905026555061\n",
      "  batch 61 loss: 0.00042058975668624043\n",
      "  batch 62 loss: 0.0004934550961479545\n",
      "  batch 63 loss: 0.00032602553255856037\n",
      "  batch 64 loss: 0.00033464771695435047\n",
      "  batch 65 loss: 0.00038318990846164525\n",
      "  batch 66 loss: 0.00035411235876381397\n",
      "  batch 67 loss: 0.00033772303140722215\n",
      "  batch 68 loss: 0.0004059219791088253\n",
      "  batch 69 loss: 0.0003935430431738496\n",
      "  batch 70 loss: 0.0004463706281967461\n",
      "  batch 71 loss: 0.00035485904663801193\n",
      "  batch 72 loss: 0.00031586684053763747\n",
      "  batch 73 loss: 0.00031112050055526197\n",
      "  batch 74 loss: 0.00036263526999391615\n",
      "  batch 75 loss: 0.0005477769882418215\n",
      "  batch 76 loss: 0.0004465421079657972\n",
      "  batch 77 loss: 0.0004021008498966694\n",
      "  batch 78 loss: 0.0004040571511723101\n",
      "  batch 79 loss: 0.00037309262552298605\n",
      "  batch 80 loss: 0.00038977497024461627\n",
      "  batch 81 loss: 0.0003812143986579031\n",
      "  batch 82 loss: 0.00040680813253857195\n",
      "  batch 83 loss: 0.0004055921745020896\n",
      "  batch 84 loss: 0.00033988035283982754\n",
      "  batch 85 loss: 0.00040261095273308456\n",
      "  batch 86 loss: 0.0004688084009103477\n",
      "  batch 87 loss: 0.00047135778004303575\n",
      "  batch 88 loss: 0.0006741748657077551\n",
      "  batch 89 loss: 0.0005199651932343841\n",
      "  batch 90 loss: 0.0006355812074616551\n",
      "  batch 91 loss: 0.0005793001619167626\n",
      "  batch 92 loss: 0.0004910266725346446\n",
      "  batch 93 loss: 0.00044570694444701076\n",
      "  batch 94 loss: 0.0004440384800545871\n",
      "  batch 95 loss: 0.0006776948575861752\n",
      "LOSS train 0.0006776948575861752 valid 0.0007935804314911366\n",
      "LOSS train 0.0006776948575861752 valid 0.0009240460931323469\n",
      "LOSS train 0.0006776948575861752 valid 0.00083891925169155\n",
      "LOSS train 0.0006776948575861752 valid 0.0008234960259869695\n",
      "LOSS train 0.0006776948575861752 valid 0.000813162128906697\n",
      "LOSS train 0.0006776948575861752 valid 0.0007942543597891927\n",
      "LOSS train 0.0006776948575861752 valid 0.000851619231980294\n",
      "LOSS train 0.0006776948575861752 valid 0.0008726377272978425\n",
      "LOSS train 0.0006776948575861752 valid 0.0008435725467279553\n",
      "LOSS train 0.0006776948575861752 valid 0.0008587430347688496\n",
      "LOSS train 0.0006776948575861752 valid 0.0008927194285206497\n",
      "LOSS train 0.0006776948575861752 valid 0.0008982302388176322\n",
      "LOSS train 0.0006776948575861752 valid 0.0008857670472934842\n",
      "LOSS train 0.0006776948575861752 valid 0.0009045912884175777\n",
      "LOSS train 0.0006776948575861752 valid 0.0009566079825162888\n",
      "LOSS train 0.0006776948575861752 valid 0.0009856615215539932\n",
      "LOSS train 0.0006776948575861752 valid 0.0009846232132986188\n",
      "LOSS train 0.0006776948575861752 valid 0.0009948379592970014\n",
      "LOSS train 0.0006776948575861752 valid 0.0010020906338468194\n",
      "LOSS train 0.0006776948575861752 valid 0.0010129179572686553\n",
      "LOSS train 0.0006776948575861752 valid 0.0010088090784847736\n",
      "LOSS train 0.0006776948575861752 valid 0.0010209755273535848\n",
      "LOSS train 0.0006776948575861752 valid 0.00102808082010597\n",
      "LOSS train 0.0006776948575861752 valid 0.001028398284688592\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.0004732192028313875\n",
      "  batch 2 loss: 0.0004681329883169383\n",
      "  batch 3 loss: 0.0006445382605306804\n",
      "  batch 4 loss: 0.0004797338624484837\n",
      "  batch 5 loss: 0.00036655867006629705\n",
      "  batch 6 loss: 0.0003178147308062762\n",
      "  batch 7 loss: 0.0003092201368417591\n",
      "  batch 8 loss: 0.0003455274272710085\n",
      "  batch 9 loss: 0.00048452182090841234\n",
      "  batch 10 loss: 0.00042900966946035624\n",
      "  batch 11 loss: 0.0007516165496781468\n",
      "  batch 12 loss: 0.0006494092522189021\n",
      "  batch 13 loss: 0.0005678762099705637\n",
      "  batch 14 loss: 0.0011501000262796879\n",
      "  batch 15 loss: 0.0007322694873437285\n",
      "  batch 16 loss: 0.0005757578182965517\n",
      "  batch 17 loss: 0.0006672557210549712\n",
      "  batch 18 loss: 0.0005594834219664335\n",
      "  batch 19 loss: 0.00111991330049932\n",
      "  batch 20 loss: 0.0009325544815510511\n",
      "  batch 21 loss: 0.0009633805602788925\n",
      "  batch 22 loss: 0.0008153807139024138\n",
      "  batch 23 loss: 0.000901366351172328\n",
      "  batch 24 loss: 0.0007337768911384046\n",
      "  batch 25 loss: 0.0008653903496451676\n",
      "  batch 26 loss: 0.0006750790635123849\n",
      "  batch 27 loss: 0.0005548587068915367\n",
      "  batch 28 loss: 0.0006661899969913065\n",
      "  batch 29 loss: 0.0004917113692499697\n",
      "  batch 30 loss: 0.0005566167528741062\n",
      "  batch 31 loss: 0.000526890333276242\n",
      "  batch 32 loss: 0.0005505230510607362\n",
      "  batch 33 loss: 0.0006115344585850835\n",
      "  batch 34 loss: 0.0005431133322417736\n",
      "  batch 35 loss: 0.00046812454820610583\n",
      "  batch 36 loss: 0.00047369982348755\n",
      "  batch 37 loss: 0.0004191517364233732\n",
      "  batch 38 loss: 0.00040232809260487556\n",
      "  batch 39 loss: 0.00043994098086841404\n",
      "  batch 40 loss: 0.000578417326323688\n",
      "  batch 41 loss: 0.00040560547495260835\n",
      "  batch 42 loss: 0.00041343027260154486\n",
      "  batch 43 loss: 0.00042835576459765434\n",
      "  batch 44 loss: 0.0003442792221903801\n",
      "  batch 45 loss: 0.00038632648647762835\n",
      "  batch 46 loss: 0.0005038175149820745\n",
      "  batch 47 loss: 0.00044533636537380517\n",
      "  batch 48 loss: 0.0003245954285375774\n",
      "  batch 49 loss: 0.00036876488593406975\n",
      "  batch 50 loss: 0.0004020904889330268\n",
      "  batch 51 loss: 0.0004712547524832189\n",
      "  batch 52 loss: 0.00038121320540085435\n",
      "  batch 53 loss: 0.0003946713113691658\n",
      "  batch 54 loss: 0.0004745140904560685\n",
      "  batch 55 loss: 0.00043757871026173234\n",
      "  batch 56 loss: 0.0003850444336421788\n",
      "  batch 57 loss: 0.0003406974719837308\n",
      "  batch 58 loss: 0.0006386921741068363\n",
      "  batch 59 loss: 0.0008579643908888102\n",
      "  batch 60 loss: 0.0005542431026697159\n",
      "  batch 61 loss: 0.00044611722114495933\n",
      "  batch 62 loss: 0.00042779784416779876\n",
      "  batch 63 loss: 0.00030074105598032475\n",
      "  batch 64 loss: 0.0003193521988578141\n",
      "  batch 65 loss: 0.0003686050185933709\n",
      "  batch 66 loss: 0.0003323388518765569\n",
      "  batch 67 loss: 0.00030608524684794247\n",
      "  batch 68 loss: 0.0004005018272437155\n",
      "  batch 69 loss: 0.0003744281711988151\n",
      "  batch 70 loss: 0.00042620382737368345\n",
      "  batch 71 loss: 0.00033656402956694365\n",
      "  batch 72 loss: 0.00032375281443819404\n",
      "  batch 73 loss: 0.00033019378315657377\n",
      "  batch 74 loss: 0.0003309652965981513\n",
      "  batch 75 loss: 0.0004763220786117017\n",
      "  batch 76 loss: 0.0003918727452401072\n",
      "  batch 77 loss: 0.0003656152985058725\n",
      "  batch 78 loss: 0.0003716997744049877\n",
      "  batch 79 loss: 0.0003634760796558112\n",
      "  batch 80 loss: 0.0003959686728194356\n",
      "  batch 81 loss: 0.0003766645095311105\n",
      "  batch 82 loss: 0.0003972453996539116\n",
      "  batch 83 loss: 0.00039923202712088823\n",
      "  batch 84 loss: 0.000337768520694226\n",
      "  batch 85 loss: 0.0003707599244080484\n",
      "  batch 86 loss: 0.0004202515701763332\n",
      "  batch 87 loss: 0.00043841288425028324\n",
      "  batch 88 loss: 0.000628740293905139\n",
      "  batch 89 loss: 0.00047011705464683473\n",
      "  batch 90 loss: 0.0005611931555904448\n",
      "  batch 91 loss: 0.0005816045450046659\n",
      "  batch 92 loss: 0.00046236516209319234\n",
      "  batch 93 loss: 0.0004738135903608054\n",
      "  batch 94 loss: 0.00043039541924372315\n",
      "  batch 95 loss: 0.000659799377899617\n",
      "LOSS train 0.000659799377899617 valid 0.0007711483631283045\n",
      "LOSS train 0.000659799377899617 valid 0.0008668525842949748\n",
      "LOSS train 0.000659799377899617 valid 0.0008075113291852176\n",
      "LOSS train 0.000659799377899617 valid 0.0008024392882362008\n",
      "LOSS train 0.000659799377899617 valid 0.0007904578815214336\n",
      "LOSS train 0.000659799377899617 valid 0.0007670372724533081\n",
      "LOSS train 0.000659799377899617 valid 0.0008119193371385336\n",
      "LOSS train 0.000659799377899617 valid 0.0008261393522843719\n",
      "LOSS train 0.000659799377899617 valid 0.0008012329344637692\n",
      "LOSS train 0.000659799377899617 valid 0.0008110107737593353\n",
      "LOSS train 0.000659799377899617 valid 0.0008375612087547779\n",
      "LOSS train 0.000659799377899617 valid 0.0008415802731178701\n",
      "LOSS train 0.000659799377899617 valid 0.0008328204276040196\n",
      "LOSS train 0.000659799377899617 valid 0.0008469003369100392\n",
      "LOSS train 0.000659799377899617 valid 0.000889916846062988\n",
      "LOSS train 0.000659799377899617 valid 0.0009158214088529348\n",
      "LOSS train 0.000659799377899617 valid 0.0009175955201499164\n",
      "LOSS train 0.000659799377899617 valid 0.0009254741598851979\n",
      "LOSS train 0.000659799377899617 valid 0.0009343699784949422\n",
      "LOSS train 0.000659799377899617 valid 0.000941856240388006\n",
      "LOSS train 0.000659799377899617 valid 0.0009394375374540687\n",
      "LOSS train 0.000659799377899617 valid 0.0009527969523333013\n",
      "LOSS train 0.000659799377899617 valid 0.0009594206931069493\n",
      "LOSS train 0.000659799377899617 valid 0.0009588885586708784\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.0004908070550300181\n",
      "  batch 2 loss: 0.0004824553616344929\n",
      "  batch 3 loss: 0.0006011293735355139\n",
      "  batch 4 loss: 0.0005028587765991688\n",
      "  batch 5 loss: 0.0003820571582764387\n",
      "  batch 6 loss: 0.0003383094444870949\n",
      "  batch 7 loss: 0.00032575742807239294\n",
      "  batch 8 loss: 0.0003462028980720788\n",
      "  batch 9 loss: 0.0004801289760507643\n",
      "  batch 10 loss: 0.000423859542934224\n",
      "  batch 11 loss: 0.0007241935236379504\n",
      "  batch 12 loss: 0.0006484852638095617\n",
      "  batch 13 loss: 0.0005564466118812561\n",
      "  batch 14 loss: 0.0010983895044773817\n",
      "  batch 15 loss: 0.0007376563735306263\n",
      "  batch 16 loss: 0.0005481630214489996\n",
      "  batch 17 loss: 0.0006635334575548768\n",
      "  batch 18 loss: 0.0005510621704161167\n",
      "  batch 19 loss: 0.0009982382180169225\n",
      "  batch 20 loss: 0.0008675764547660947\n",
      "  batch 21 loss: 0.0009468260686844587\n",
      "  batch 22 loss: 0.0007853198912926018\n",
      "  batch 23 loss: 0.0008260044269263744\n",
      "  batch 24 loss: 0.0007273501250892878\n",
      "  batch 25 loss: 0.0008589852368459105\n",
      "  batch 26 loss: 0.0006603526417165995\n",
      "  batch 27 loss: 0.0004882593057118356\n",
      "  batch 28 loss: 0.000627241563051939\n",
      "  batch 29 loss: 0.00042909482726827264\n",
      "  batch 30 loss: 0.0004871624696534127\n",
      "  batch 31 loss: 0.0004687919863499701\n",
      "  batch 32 loss: 0.0005100832786411047\n",
      "  batch 33 loss: 0.0005850073066540062\n",
      "  batch 34 loss: 0.0005179381114430726\n",
      "  batch 35 loss: 0.0004471788415685296\n",
      "  batch 36 loss: 0.0004567485593724996\n",
      "  batch 37 loss: 0.0004029903211630881\n",
      "  batch 38 loss: 0.00038998096715658903\n",
      "  batch 39 loss: 0.00043068791273981333\n",
      "  batch 40 loss: 0.000552077020984143\n",
      "  batch 41 loss: 0.0004063459055032581\n",
      "  batch 42 loss: 0.00041430527926422656\n",
      "  batch 43 loss: 0.0004119397490285337\n",
      "  batch 44 loss: 0.0003340903203934431\n",
      "  batch 45 loss: 0.0003650081343948841\n",
      "  batch 46 loss: 0.0004934124881401658\n",
      "  batch 47 loss: 0.0004208014579489827\n",
      "  batch 48 loss: 0.00031715381192043424\n",
      "  batch 49 loss: 0.00036175097920931876\n",
      "  batch 50 loss: 0.0003897897549904883\n",
      "  batch 51 loss: 0.00044933255412615836\n",
      "  batch 52 loss: 0.0003602153155952692\n",
      "  batch 53 loss: 0.00038475252222269773\n",
      "  batch 54 loss: 0.00045707152457907796\n",
      "  batch 55 loss: 0.00044279906433075666\n",
      "  batch 56 loss: 0.00038245361065492034\n",
      "  batch 57 loss: 0.00031723867869004607\n",
      "  batch 58 loss: 0.0005881013348698616\n",
      "  batch 59 loss: 0.0008222766919061542\n",
      "  batch 60 loss: 0.0005258191959001124\n",
      "  batch 61 loss: 0.00040162127697840333\n",
      "  batch 62 loss: 0.00042203854536637664\n",
      "  batch 63 loss: 0.00028260130784474313\n",
      "  batch 64 loss: 0.00028397171990945935\n",
      "  batch 65 loss: 0.0003256245981901884\n",
      "  batch 66 loss: 0.00033195799915120006\n",
      "  batch 67 loss: 0.0003086356446146965\n",
      "  batch 68 loss: 0.0004388554662000388\n",
      "  batch 69 loss: 0.00038879545172676444\n",
      "  batch 70 loss: 0.00042312557343393564\n",
      "  batch 71 loss: 0.00032005744287744164\n",
      "  batch 72 loss: 0.00030510095530189574\n",
      "  batch 73 loss: 0.00031285028671845794\n",
      "  batch 74 loss: 0.0003266324056312442\n",
      "  batch 75 loss: 0.00046639906940981746\n",
      "  batch 76 loss: 0.0003829181077890098\n",
      "  batch 77 loss: 0.0003587013343349099\n",
      "  batch 78 loss: 0.0003646313853096217\n",
      "  batch 79 loss: 0.00036904343869537115\n",
      "  batch 80 loss: 0.00040261191315948963\n",
      "  batch 81 loss: 0.00037833594251424074\n",
      "  batch 82 loss: 0.00039081828435882926\n",
      "  batch 83 loss: 0.0004027968388982117\n",
      "  batch 84 loss: 0.000342086743330583\n",
      "  batch 85 loss: 0.00037156010512262583\n",
      "  batch 86 loss: 0.000403158221160993\n",
      "  batch 87 loss: 0.0004240314883645624\n",
      "  batch 88 loss: 0.0006550673278979957\n",
      "  batch 89 loss: 0.0004578704829327762\n",
      "  batch 90 loss: 0.0005717130843549967\n",
      "  batch 91 loss: 0.0005361965158954263\n",
      "  batch 92 loss: 0.00045621494064107537\n",
      "  batch 93 loss: 0.0004462959768716246\n",
      "  batch 94 loss: 0.0004228310426697135\n",
      "  batch 95 loss: 0.0006381658022291958\n",
      "LOSS train 0.0006381658022291958 valid 0.0007517527556046844\n",
      "LOSS train 0.0006381658022291958 valid 0.0008567467448301613\n",
      "LOSS train 0.0006381658022291958 valid 0.0008204252808354795\n",
      "LOSS train 0.0006381658022291958 valid 0.0008154064998961985\n",
      "LOSS train 0.0006381658022291958 valid 0.0007972088060341775\n",
      "LOSS train 0.0006381658022291958 valid 0.0007659144466742873\n",
      "LOSS train 0.0006381658022291958 valid 0.0008067986345849931\n",
      "LOSS train 0.0006381658022291958 valid 0.000821219349745661\n",
      "LOSS train 0.0006381658022291958 valid 0.0007974949548952281\n",
      "LOSS train 0.0006381658022291958 valid 0.0008070385083556175\n",
      "LOSS train 0.0006381658022291958 valid 0.0008294663275592029\n",
      "LOSS train 0.0006381658022291958 valid 0.0008341664797626436\n",
      "LOSS train 0.0006381658022291958 valid 0.0008325224043801427\n",
      "LOSS train 0.0006381658022291958 valid 0.0008464454440400004\n",
      "LOSS train 0.0006381658022291958 valid 0.0008835174376145005\n",
      "LOSS train 0.0006381658022291958 valid 0.0009066513157449663\n",
      "LOSS train 0.0006381658022291958 valid 0.0009038177668116987\n",
      "LOSS train 0.0006381658022291958 valid 0.0009088704828172922\n",
      "LOSS train 0.0006381658022291958 valid 0.0009149292600341141\n",
      "LOSS train 0.0006381658022291958 valid 0.0009222115622833371\n",
      "LOSS train 0.0006381658022291958 valid 0.0009182898211292922\n",
      "LOSS train 0.0006381658022291958 valid 0.0009302570251747966\n",
      "LOSS train 0.0006381658022291958 valid 0.000936575757805258\n",
      "LOSS train 0.0006381658022291958 valid 0.0009334889473393559\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.00047158743836916983\n",
      "  batch 2 loss: 0.0004477671755012125\n",
      "  batch 3 loss: 0.0005861434619873762\n",
      "  batch 4 loss: 0.0004894911544397473\n",
      "  batch 5 loss: 0.00041032841545529664\n",
      "  batch 6 loss: 0.0003825073654297739\n",
      "  batch 7 loss: 0.00036193340201862156\n",
      "  batch 8 loss: 0.00037375700776465237\n",
      "  batch 9 loss: 0.00039482791908085346\n",
      "  batch 10 loss: 0.0003503878542687744\n",
      "  batch 11 loss: 0.0006444754544645548\n",
      "  batch 12 loss: 0.0005987933836877346\n",
      "  batch 13 loss: 0.0005240767495706677\n",
      "  batch 14 loss: 0.0010652951896190643\n",
      "  batch 15 loss: 0.0007070559076964855\n",
      "  batch 16 loss: 0.0005116274696774781\n",
      "  batch 17 loss: 0.0006434572860598564\n",
      "  batch 18 loss: 0.0005291123525239527\n",
      "  batch 19 loss: 0.001022027456201613\n",
      "  batch 20 loss: 0.000893457152415067\n",
      "  batch 21 loss: 0.0009456708794459701\n",
      "  batch 22 loss: 0.0008055097423493862\n",
      "  batch 23 loss: 0.000853900273796171\n",
      "  batch 24 loss: 0.0007417190354317427\n",
      "  batch 25 loss: 0.0008568315533921123\n",
      "  batch 26 loss: 0.0006256797350943089\n",
      "  batch 27 loss: 0.0004997630603611469\n",
      "  batch 28 loss: 0.0006164747755974531\n",
      "  batch 29 loss: 0.00044085903209634125\n",
      "  batch 30 loss: 0.00048758566845208406\n",
      "  batch 31 loss: 0.0004641372652258724\n",
      "  batch 32 loss: 0.000518881541211158\n",
      "  batch 33 loss: 0.0005727146053686738\n",
      "  batch 34 loss: 0.0005053849308751523\n",
      "  batch 35 loss: 0.00043615075992420316\n",
      "  batch 36 loss: 0.00045698555186390877\n",
      "  batch 37 loss: 0.00039423181442543864\n",
      "  batch 38 loss: 0.000394304224755615\n",
      "  batch 39 loss: 0.0004319400177337229\n",
      "  batch 40 loss: 0.0005417835200205445\n",
      "  batch 41 loss: 0.00041888997657224536\n",
      "  batch 42 loss: 0.0004432686255313456\n",
      "  batch 43 loss: 0.00041716816485859454\n",
      "  batch 44 loss: 0.0003335281799081713\n",
      "  batch 45 loss: 0.00037238921504467726\n",
      "  batch 46 loss: 0.00048328799312002957\n",
      "  batch 47 loss: 0.0004022807697765529\n",
      "  batch 48 loss: 0.0003084098862018436\n",
      "  batch 49 loss: 0.0003527152875903994\n",
      "  batch 50 loss: 0.00037386873736977577\n",
      "  batch 51 loss: 0.00043781011481769383\n",
      "  batch 52 loss: 0.00034847226925194263\n",
      "  batch 53 loss: 0.00036519940476864576\n",
      "  batch 54 loss: 0.0004447036480996758\n",
      "  batch 55 loss: 0.00042127445340156555\n",
      "  batch 56 loss: 0.0003714527701959014\n",
      "  batch 57 loss: 0.0003146985836792737\n",
      "  batch 58 loss: 0.0005980780115351081\n",
      "  batch 59 loss: 0.0008268611272796988\n",
      "  batch 60 loss: 0.0005285197403281927\n",
      "  batch 61 loss: 0.0004249151679687202\n",
      "  batch 62 loss: 0.00040525023359805346\n",
      "  batch 63 loss: 0.00027558807050809264\n",
      "  batch 64 loss: 0.00030755181796848774\n",
      "  batch 65 loss: 0.00033956332481466234\n",
      "  batch 66 loss: 0.00031512140412814915\n",
      "  batch 67 loss: 0.0002961750724352896\n",
      "  batch 68 loss: 0.0003847229527309537\n",
      "  batch 69 loss: 0.0003628441190812737\n",
      "  batch 70 loss: 0.00041901867371052504\n",
      "  batch 71 loss: 0.00031863676849752665\n",
      "  batch 72 loss: 0.00030113907996565104\n",
      "  batch 73 loss: 0.00030562974279746413\n",
      "  batch 74 loss: 0.00032760732574388385\n",
      "  batch 75 loss: 0.00046329916222020984\n",
      "  batch 76 loss: 0.00038732250686734915\n",
      "  batch 77 loss: 0.0003450180229265243\n",
      "  batch 78 loss: 0.0003518818994052708\n",
      "  batch 79 loss: 0.00033990846714004874\n",
      "  batch 80 loss: 0.00038419722113758326\n",
      "  batch 81 loss: 0.00035789606045000255\n",
      "  batch 82 loss: 0.0003806488821282983\n",
      "  batch 83 loss: 0.0003917876456398517\n",
      "  batch 84 loss: 0.00032155629014596343\n",
      "  batch 85 loss: 0.0003600450581870973\n",
      "  batch 86 loss: 0.00040724052814766765\n",
      "  batch 87 loss: 0.0004281365545466542\n",
      "  batch 88 loss: 0.0006378401885740459\n",
      "  batch 89 loss: 0.0004592474433593452\n",
      "  batch 90 loss: 0.0005443020490929484\n",
      "  batch 91 loss: 0.0005466118454933167\n",
      "  batch 92 loss: 0.0004302718152757734\n",
      "  batch 93 loss: 0.0004235172236803919\n",
      "  batch 94 loss: 0.00040853413520380855\n",
      "  batch 95 loss: 0.0006440645083785057\n",
      "LOSS train 0.0006440645083785057 valid 0.0007345504127442837\n",
      "LOSS train 0.0006440645083785057 valid 0.0008588578784838319\n",
      "LOSS train 0.0006440645083785057 valid 0.000803017639555037\n",
      "LOSS train 0.0006440645083785057 valid 0.0007929709972813725\n",
      "LOSS train 0.0006440645083785057 valid 0.0007807613001205027\n",
      "LOSS train 0.0006440645083785057 valid 0.000751956133171916\n",
      "LOSS train 0.0006440645083785057 valid 0.000794810417573899\n",
      "LOSS train 0.0006440645083785057 valid 0.0008118599071167409\n",
      "LOSS train 0.0006440645083785057 valid 0.0007860984769649804\n",
      "LOSS train 0.0006440645083785057 valid 0.0007986598066054285\n",
      "LOSS train 0.0006440645083785057 valid 0.0008285306394100189\n",
      "LOSS train 0.0006440645083785057 valid 0.0008327660034410655\n",
      "LOSS train 0.0006440645083785057 valid 0.0008253199048340321\n",
      "LOSS train 0.0006440645083785057 valid 0.0008396105840802193\n",
      "LOSS train 0.0006440645083785057 valid 0.0008827975252643228\n",
      "LOSS train 0.0006440645083785057 valid 0.0009091275278478861\n",
      "LOSS train 0.0006440645083785057 valid 0.0009090546518564224\n",
      "LOSS train 0.0006440645083785057 valid 0.0009143842034973204\n",
      "LOSS train 0.0006440645083785057 valid 0.0009190781274810433\n",
      "LOSS train 0.0006440645083785057 valid 0.0009286042186431587\n",
      "LOSS train 0.0006440645083785057 valid 0.0009250986040569842\n",
      "LOSS train 0.0006440645083785057 valid 0.0009372152271680534\n",
      "LOSS train 0.0006440645083785057 valid 0.0009441662696190178\n",
      "LOSS train 0.0006440645083785057 valid 0.0009451726218685508\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.00047271628864109516\n",
      "  batch 2 loss: 0.0004485810350161046\n",
      "  batch 3 loss: 0.0005909840110689402\n",
      "  batch 4 loss: 0.0004823220078833401\n",
      "  batch 5 loss: 0.00041119137313216925\n",
      "  batch 6 loss: 0.00038701295852661133\n",
      "  batch 7 loss: 0.00035861064679920673\n",
      "  batch 8 loss: 0.00037540640914812684\n",
      "  batch 9 loss: 0.0003786507295444608\n",
      "  batch 10 loss: 0.00033097807317972183\n",
      "  batch 11 loss: 0.0006273039616644382\n",
      "  batch 12 loss: 0.0005992522928863764\n",
      "  batch 13 loss: 0.0005060599651187658\n",
      "  batch 14 loss: 0.0010103004751726985\n",
      "  batch 15 loss: 0.0006812610663473606\n",
      "  batch 16 loss: 0.0005318681360222399\n",
      "  batch 17 loss: 0.0006298411171883345\n",
      "  batch 18 loss: 0.0005303089274093509\n",
      "  batch 19 loss: 0.0009979677852243185\n",
      "  batch 20 loss: 0.0008634236874058843\n",
      "  batch 21 loss: 0.0009102350450120866\n",
      "  batch 22 loss: 0.0007986428681761026\n",
      "  batch 23 loss: 0.0008231769315898418\n",
      "  batch 24 loss: 0.0007412401027977467\n",
      "  batch 25 loss: 0.0009099192102439702\n",
      "  batch 26 loss: 0.0006256138440221548\n",
      "  batch 27 loss: 0.0004728871281258762\n",
      "  batch 28 loss: 0.0005860449746251106\n",
      "  batch 29 loss: 0.0004103254759684205\n",
      "  batch 30 loss: 0.00045393325854092836\n",
      "  batch 31 loss: 0.00043444131733849645\n",
      "  batch 32 loss: 0.00048781183431856334\n",
      "  batch 33 loss: 0.0005349970306269825\n",
      "  batch 34 loss: 0.0004959775251336396\n",
      "  batch 35 loss: 0.00043065601494163275\n",
      "  batch 36 loss: 0.0004432557034306228\n",
      "  batch 37 loss: 0.0003870799846481532\n",
      "  batch 38 loss: 0.000386068772058934\n",
      "  batch 39 loss: 0.00042703235521912575\n",
      "  batch 40 loss: 0.0005238891462795436\n",
      "  batch 41 loss: 0.0004191493790131062\n",
      "  batch 42 loss: 0.00045004385174252093\n",
      "  batch 43 loss: 0.00040505980723537505\n",
      "  batch 44 loss: 0.00032545681460760534\n",
      "  batch 45 loss: 0.0003622765652835369\n",
      "  batch 46 loss: 0.00046677503269165754\n",
      "  batch 47 loss: 0.000394822855014354\n",
      "  batch 48 loss: 0.0003012663219124079\n",
      "  batch 49 loss: 0.0003515172575134784\n",
      "  batch 50 loss: 0.0003652112209238112\n",
      "  batch 51 loss: 0.00042702851351350546\n",
      "  batch 52 loss: 0.00033182965125888586\n",
      "  batch 53 loss: 0.0003516477008815855\n",
      "  batch 54 loss: 0.0004214008804410696\n",
      "  batch 55 loss: 0.0004026749520562589\n",
      "  batch 56 loss: 0.0003559784672688693\n",
      "  batch 57 loss: 0.0003021034935954958\n",
      "  batch 58 loss: 0.0005851642927154899\n",
      "  batch 59 loss: 0.0008040111279115081\n",
      "  batch 60 loss: 0.0005113242659717798\n",
      "  batch 61 loss: 0.0004145913408137858\n",
      "  batch 62 loss: 0.0003898788127116859\n",
      "  batch 63 loss: 0.00026566581800580025\n",
      "  batch 64 loss: 0.000294413766823709\n",
      "  batch 65 loss: 0.00032642658334225416\n",
      "  batch 66 loss: 0.00030384788988158107\n",
      "  batch 67 loss: 0.0002737749309744686\n",
      "  batch 68 loss: 0.0003790839109569788\n",
      "  batch 69 loss: 0.0003545028157532215\n",
      "  batch 70 loss: 0.0004118387878406793\n",
      "  batch 71 loss: 0.0003031246014870703\n",
      "  batch 72 loss: 0.0002979004057124257\n",
      "  batch 73 loss: 0.00029861234361305833\n",
      "  batch 74 loss: 0.00032240277505479753\n",
      "  batch 75 loss: 0.00045308892731554806\n",
      "  batch 76 loss: 0.0003665145777631551\n",
      "  batch 77 loss: 0.00032994861248880625\n",
      "  batch 78 loss: 0.0003420044085942209\n",
      "  batch 79 loss: 0.00033830461325123906\n",
      "  batch 80 loss: 0.0003763157583307475\n",
      "  batch 81 loss: 0.00035439259954728186\n",
      "  batch 82 loss: 0.00037419836735352874\n",
      "  batch 83 loss: 0.00038384099025279284\n",
      "  batch 84 loss: 0.00031910277903079987\n",
      "  batch 85 loss: 0.00034531549317762256\n",
      "  batch 86 loss: 0.0003924145712517202\n",
      "  batch 87 loss: 0.00041028091800399125\n",
      "  batch 88 loss: 0.000631354283541441\n",
      "  batch 89 loss: 0.00045094022061675787\n",
      "  batch 90 loss: 0.0005406859563663602\n",
      "  batch 91 loss: 0.0005128119373694062\n",
      "  batch 92 loss: 0.0004299088614061475\n",
      "  batch 93 loss: 0.0004138949152547866\n",
      "  batch 94 loss: 0.0004054596647620201\n",
      "  batch 95 loss: 0.0006391817005351186\n",
      "LOSS train 0.0006391817005351186 valid 0.0007688234327360988\n",
      "LOSS train 0.0006391817005351186 valid 0.0009049525833688676\n",
      "LOSS train 0.0006391817005351186 valid 0.000833848724141717\n",
      "LOSS train 0.0006391817005351186 valid 0.000811718637123704\n",
      "LOSS train 0.0006391817005351186 valid 0.000798709865193814\n",
      "LOSS train 0.0006391817005351186 valid 0.0007735625840723515\n",
      "LOSS train 0.0006391817005351186 valid 0.0008208160870708525\n",
      "LOSS train 0.0006391817005351186 valid 0.0008406630367971957\n",
      "LOSS train 0.0006391817005351186 valid 0.0008152790833264589\n",
      "LOSS train 0.0006391817005351186 valid 0.0008344277157448232\n",
      "LOSS train 0.0006391817005351186 valid 0.0008701271144673228\n",
      "LOSS train 0.0006391817005351186 valid 0.0008767500403337181\n",
      "LOSS train 0.0006391817005351186 valid 0.0008646925562061369\n",
      "LOSS train 0.0006391817005351186 valid 0.0008824897813610733\n",
      "LOSS train 0.0006391817005351186 valid 0.0009347363375127316\n",
      "LOSS train 0.0006391817005351186 valid 0.0009622948127798736\n",
      "LOSS train 0.0006391817005351186 valid 0.0009603265789337456\n",
      "LOSS train 0.0006391817005351186 valid 0.0009643241064622998\n",
      "LOSS train 0.0006391817005351186 valid 0.0009662871598266065\n",
      "LOSS train 0.0006391817005351186 valid 0.0009752484038472176\n",
      "LOSS train 0.0006391817005351186 valid 0.000967764004599303\n",
      "LOSS train 0.0006391817005351186 valid 0.0009753130143508315\n",
      "LOSS train 0.0006391817005351186 valid 0.0009809436742216349\n",
      "LOSS train 0.0006391817005351186 valid 0.0009820135310292244\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 0.0004731150693260133\n",
      "  batch 2 loss: 0.0004281401343178004\n",
      "  batch 3 loss: 0.0005737320752814412\n",
      "  batch 4 loss: 0.00047122256364673376\n",
      "  batch 5 loss: 0.0004130234010517597\n",
      "  batch 6 loss: 0.00039860623655840755\n",
      "  batch 7 loss: 0.0003605480887927115\n",
      "  batch 8 loss: 0.00038107653381302953\n",
      "  batch 9 loss: 0.00036491500213742256\n",
      "  batch 10 loss: 0.00033170508686453104\n",
      "  batch 11 loss: 0.0006194195011630654\n",
      "  batch 12 loss: 0.0005849623121321201\n",
      "  batch 13 loss: 0.00048426538705825806\n",
      "  batch 14 loss: 0.0010065489914268255\n",
      "  batch 15 loss: 0.0006761218537576497\n",
      "  batch 16 loss: 0.0005040981341153383\n",
      "  batch 17 loss: 0.000616622855886817\n",
      "  batch 18 loss: 0.0005021124379709363\n",
      "  batch 19 loss: 0.0010228169849142432\n",
      "  batch 20 loss: 0.0008686196524649858\n",
      "  batch 21 loss: 0.0009083698969334364\n",
      "  batch 22 loss: 0.0007805366767570376\n",
      "  batch 23 loss: 0.0008228945080190897\n",
      "  batch 24 loss: 0.0007298061391338706\n",
      "  batch 25 loss: 0.0008731182315386832\n",
      "  batch 26 loss: 0.000610367686022073\n",
      "  batch 27 loss: 0.00047812770935706794\n",
      "  batch 28 loss: 0.0006079076556488872\n",
      "  batch 29 loss: 0.00041829823749139905\n",
      "  batch 30 loss: 0.0004606946895364672\n",
      "  batch 31 loss: 0.000432311266195029\n",
      "  batch 32 loss: 0.0004938805941492319\n",
      "  batch 33 loss: 0.0005344350938685238\n",
      "  batch 34 loss: 0.000488432589918375\n",
      "  batch 35 loss: 0.00041805877117440104\n",
      "  batch 36 loss: 0.00043768499745056033\n",
      "  batch 37 loss: 0.00037704824353568256\n",
      "  batch 38 loss: 0.0003743593115359545\n",
      "  batch 39 loss: 0.00041976425563916564\n",
      "  batch 40 loss: 0.000517786480486393\n",
      "  batch 41 loss: 0.00042254984145984054\n",
      "  batch 42 loss: 0.00043403590098023415\n",
      "  batch 43 loss: 0.00040594610618427396\n",
      "  batch 44 loss: 0.000320034334436059\n",
      "  batch 45 loss: 0.0003604697703849524\n",
      "  batch 46 loss: 0.0004661503480747342\n",
      "  batch 47 loss: 0.00039358605863526464\n",
      "  batch 48 loss: 0.0002932690840680152\n",
      "  batch 49 loss: 0.00034497963497415185\n",
      "  batch 50 loss: 0.00036651547998189926\n",
      "  batch 51 loss: 0.0004315273545216769\n",
      "  batch 52 loss: 0.0003424082533456385\n",
      "  batch 53 loss: 0.0003535630239639431\n",
      "  batch 54 loss: 0.0004259509441908449\n",
      "  batch 55 loss: 0.00039948103949427605\n",
      "  batch 56 loss: 0.0003539525205269456\n",
      "  batch 57 loss: 0.0003068131918553263\n",
      "  batch 58 loss: 0.0005507890600711107\n",
      "  batch 59 loss: 0.0008130376227200031\n",
      "  batch 60 loss: 0.0005026332219131291\n",
      "  batch 61 loss: 0.0003967790980823338\n",
      "  batch 62 loss: 0.0003922007163055241\n",
      "  batch 63 loss: 0.0002648243971634656\n",
      "  batch 64 loss: 0.00028614766779355705\n",
      "  batch 65 loss: 0.00032034164178185165\n",
      "  batch 66 loss: 0.00030456820968538523\n",
      "  batch 67 loss: 0.0002977762487716973\n",
      "  batch 68 loss: 0.000377932854462415\n",
      "  batch 69 loss: 0.0003510303795337677\n",
      "  batch 70 loss: 0.00041011424036696553\n",
      "  batch 71 loss: 0.00030418208916671574\n",
      "  batch 72 loss: 0.00028900429606437683\n",
      "  batch 73 loss: 0.00029787662788294256\n",
      "  batch 74 loss: 0.0003148011746816337\n",
      "  batch 75 loss: 0.0004489237326197326\n",
      "  batch 76 loss: 0.00038239543209783733\n",
      "  batch 77 loss: 0.00033459908445365727\n",
      "  batch 78 loss: 0.00034010730450972915\n",
      "  batch 79 loss: 0.0003375446249265224\n",
      "  batch 80 loss: 0.00037822729791514575\n",
      "  batch 81 loss: 0.0003560165350791067\n",
      "  batch 82 loss: 0.0003798584220930934\n",
      "  batch 83 loss: 0.0003884260659106076\n",
      "  batch 84 loss: 0.00031878112349659204\n",
      "  batch 85 loss: 0.0003503888437990099\n",
      "  batch 86 loss: 0.00038904580287635326\n",
      "  batch 87 loss: 0.000407197221647948\n",
      "  batch 88 loss: 0.0006547264056280255\n",
      "  batch 89 loss: 0.000449292769189924\n",
      "  batch 90 loss: 0.0005353144370019436\n",
      "  batch 91 loss: 0.0005023291450925171\n",
      "  batch 92 loss: 0.0004304798203520477\n",
      "  batch 93 loss: 0.0004078282508999109\n",
      "  batch 94 loss: 0.0003973941202275455\n",
      "  batch 95 loss: 0.0006281276582740247\n",
      "LOSS train 0.0006281276582740247 valid 0.00072758604073897\n",
      "LOSS train 0.0006281276582740247 valid 0.0008682819316163659\n",
      "LOSS train 0.0006281276582740247 valid 0.0008041525143198669\n",
      "LOSS train 0.0006281276582740247 valid 0.0007894044974818826\n",
      "LOSS train 0.0006281276582740247 valid 0.0007806610083207488\n",
      "LOSS train 0.0006281276582740247 valid 0.0007543129613623023\n",
      "LOSS train 0.0006281276582740247 valid 0.0007984913536347449\n",
      "LOSS train 0.0006281276582740247 valid 0.0008175641414709389\n",
      "LOSS train 0.0006281276582740247 valid 0.0007926820544525981\n",
      "LOSS train 0.0006281276582740247 valid 0.0008097115787677467\n",
      "LOSS train 0.0006281276582740247 valid 0.000841634813696146\n",
      "LOSS train 0.0006281276582740247 valid 0.000847478280775249\n",
      "LOSS train 0.0006281276582740247 valid 0.0008375159231945872\n",
      "LOSS train 0.0006281276582740247 valid 0.0008546477765776217\n",
      "LOSS train 0.0006281276582740247 valid 0.0009069401421584189\n",
      "LOSS train 0.0006281276582740247 valid 0.0009359074756503105\n",
      "LOSS train 0.0006281276582740247 valid 0.0009349040337838233\n",
      "LOSS train 0.0006281276582740247 valid 0.000941821897868067\n",
      "LOSS train 0.0006281276582740247 valid 0.0009468392236158252\n",
      "LOSS train 0.0006281276582740247 valid 0.0009555299766361713\n",
      "LOSS train 0.0006281276582740247 valid 0.0009488165378570557\n",
      "LOSS train 0.0006281276582740247 valid 0.0009584858198650181\n",
      "LOSS train 0.0006281276582740247 valid 0.0009646390099078417\n",
      "LOSS train 0.0006281276582740247 valid 0.0009637900511734188\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 0.00047407124657183886\n",
      "  batch 2 loss: 0.0004276008694432676\n",
      "  batch 3 loss: 0.0005766794783994555\n",
      "  batch 4 loss: 0.00046413607196882367\n",
      "  batch 5 loss: 0.00040963993524201214\n",
      "  batch 6 loss: 0.00039864389691501856\n",
      "  batch 7 loss: 0.00035597430542111397\n",
      "  batch 8 loss: 0.00038094219053164124\n",
      "  batch 9 loss: 0.0003599417977966368\n",
      "  batch 10 loss: 0.0003160523483529687\n",
      "  batch 11 loss: 0.0006292483303695917\n",
      "  batch 12 loss: 0.0005848110886290669\n",
      "  batch 13 loss: 0.0004928482230752707\n",
      "  batch 14 loss: 0.0009884508326649666\n",
      "  batch 15 loss: 0.0006459917058236897\n",
      "  batch 16 loss: 0.00047663136501796544\n",
      "  batch 17 loss: 0.000625216867774725\n",
      "  batch 18 loss: 0.0004816347500309348\n",
      "  batch 19 loss: 0.0009554126299917698\n",
      "  batch 20 loss: 0.0008365338435396552\n",
      "  batch 21 loss: 0.0008955529192462564\n",
      "  batch 22 loss: 0.0007692526560276747\n",
      "  batch 23 loss: 0.0008029359742067754\n",
      "  batch 24 loss: 0.000735679583158344\n",
      "  batch 25 loss: 0.0008584598544985056\n",
      "  batch 26 loss: 0.000603998196311295\n",
      "  batch 27 loss: 0.00046008179197087884\n",
      "  batch 28 loss: 0.0005963743897154927\n",
      "  batch 29 loss: 0.00040465156780555844\n",
      "  batch 30 loss: 0.0004504708922468126\n",
      "  batch 31 loss: 0.00042284998926334083\n",
      "  batch 32 loss: 0.0004836532170884311\n",
      "  batch 33 loss: 0.0005224780179560184\n",
      "  batch 34 loss: 0.0004932014271616936\n",
      "  batch 35 loss: 0.00042613071855157614\n",
      "  batch 36 loss: 0.0004307595663703978\n",
      "  batch 37 loss: 0.0003791886265389621\n",
      "  batch 38 loss: 0.00038168346509337425\n",
      "  batch 39 loss: 0.00041962266550399363\n",
      "  batch 40 loss: 0.0005199229926802218\n",
      "  batch 41 loss: 0.00041745591443032026\n",
      "  batch 42 loss: 0.0004554547485895455\n",
      "  batch 43 loss: 0.0004169199091847986\n",
      "  batch 44 loss: 0.0003185907262377441\n",
      "  batch 45 loss: 0.0003661146038211882\n",
      "  batch 46 loss: 0.00044662790605798364\n",
      "  batch 47 loss: 0.00039215077413246036\n",
      "  batch 48 loss: 0.0002962895086966455\n",
      "  batch 49 loss: 0.00034130766289308667\n",
      "  batch 50 loss: 0.000360350648406893\n",
      "  batch 51 loss: 0.0004247189499437809\n",
      "  batch 52 loss: 0.00033252299181185663\n",
      "  batch 53 loss: 0.00034216290805488825\n",
      "  batch 54 loss: 0.0004100907826796174\n",
      "  batch 55 loss: 0.0004085982800461352\n",
      "  batch 56 loss: 0.00035651103826239705\n",
      "  batch 57 loss: 0.0003007615450769663\n",
      "  batch 58 loss: 0.0005476183723658323\n",
      "  batch 59 loss: 0.000756183871999383\n",
      "  batch 60 loss: 0.0005142044974491\n",
      "  batch 61 loss: 0.00039408638258464634\n",
      "  batch 62 loss: 0.0003977078013122082\n",
      "  batch 63 loss: 0.00026533048367127776\n",
      "  batch 64 loss: 0.00028642197139561176\n",
      "  batch 65 loss: 0.00032341270707547665\n",
      "  batch 66 loss: 0.00030188116943463683\n",
      "  batch 67 loss: 0.0002705068909563124\n",
      "  batch 68 loss: 0.00037685438292101026\n",
      "  batch 69 loss: 0.0003342607233207673\n",
      "  batch 70 loss: 0.0003991450066678226\n",
      "  batch 71 loss: 0.0002971102949231863\n",
      "  batch 72 loss: 0.000297080900054425\n",
      "  batch 73 loss: 0.00029685336630791426\n",
      "  batch 74 loss: 0.0003130757831968367\n",
      "  batch 75 loss: 0.00043523928616195917\n",
      "  batch 76 loss: 0.0003452007076703012\n",
      "  batch 77 loss: 0.00032157462555915117\n",
      "  batch 78 loss: 0.0003263667749706656\n",
      "  batch 79 loss: 0.00033041241113096476\n",
      "  batch 80 loss: 0.000362358900019899\n",
      "  batch 81 loss: 0.000344722589943558\n",
      "  batch 82 loss: 0.00037165015237405896\n",
      "  batch 83 loss: 0.0003790578921325505\n",
      "  batch 84 loss: 0.00030874318326823413\n",
      "  batch 85 loss: 0.00033807329600676894\n",
      "  batch 86 loss: 0.0003863144083879888\n",
      "  batch 87 loss: 0.00040091807022690773\n",
      "  batch 88 loss: 0.0006687359418720007\n",
      "  batch 89 loss: 0.00045433995546773076\n",
      "  batch 90 loss: 0.0005283302161842585\n",
      "  batch 91 loss: 0.00047952478053048253\n",
      "  batch 92 loss: 0.00042873615166172385\n",
      "  batch 93 loss: 0.00040729332249611616\n",
      "  batch 94 loss: 0.00038274156395345926\n",
      "  batch 95 loss: 0.0006244060932658613\n",
      "LOSS train 0.0006244060932658613 valid 0.0007649053004570305\n",
      "LOSS train 0.0006244060932658613 valid 0.000911894254386425\n",
      "LOSS train 0.0006244060932658613 valid 0.0008410966256633401\n",
      "LOSS train 0.0006244060932658613 valid 0.0008205206249840558\n",
      "LOSS train 0.0006244060932658613 valid 0.0008038444793783128\n",
      "LOSS train 0.0006244060932658613 valid 0.000776864995714277\n",
      "LOSS train 0.0006244060932658613 valid 0.0008176295668818057\n",
      "LOSS train 0.0006244060932658613 valid 0.0008368862909264863\n",
      "LOSS train 0.0006244060932658613 valid 0.0008138873963616788\n",
      "LOSS train 0.0006244060932658613 valid 0.0008295059087686241\n",
      "LOSS train 0.0006244060932658613 valid 0.0008553529041819274\n",
      "LOSS train 0.0006244060932658613 valid 0.0008594868122600019\n",
      "LOSS train 0.0006244060932658613 valid 0.0008493228815495968\n",
      "LOSS train 0.0006244060932658613 valid 0.0008641313761472702\n",
      "LOSS train 0.0006244060932658613 valid 0.0009139744797721505\n",
      "LOSS train 0.0006244060932658613 valid 0.0009418271365575492\n",
      "LOSS train 0.0006244060932658613 valid 0.0009391414350830019\n",
      "LOSS train 0.0006244060932658613 valid 0.000942687620408833\n",
      "LOSS train 0.0006244060932658613 valid 0.0009445627802051604\n",
      "LOSS train 0.0006244060932658613 valid 0.000950891466345638\n",
      "LOSS train 0.0006244060932658613 valid 0.0009444281458854675\n",
      "LOSS train 0.0006244060932658613 valid 0.0009545970242470503\n",
      "LOSS train 0.0006244060932658613 valid 0.0009608655236661434\n",
      "LOSS train 0.0006244060932658613 valid 0.0009612109279260039\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 0.0004811032849829644\n",
      "  batch 2 loss: 0.0004326223279349506\n",
      "  batch 3 loss: 0.0005717169842682779\n",
      "  batch 4 loss: 0.00044934271136298776\n",
      "  batch 5 loss: 0.0004194430075585842\n",
      "  batch 6 loss: 0.0004011915298178792\n",
      "  batch 7 loss: 0.00034881269675679505\n",
      "  batch 8 loss: 0.0003758129314519465\n",
      "  batch 9 loss: 0.00035341037437319756\n",
      "  batch 10 loss: 0.00031597912311553955\n",
      "  batch 11 loss: 0.0006245216936804354\n",
      "  batch 12 loss: 0.0005636663408949971\n",
      "  batch 13 loss: 0.00047471042489632964\n",
      "  batch 14 loss: 0.0009845509193837643\n",
      "  batch 15 loss: 0.0006371394265443087\n",
      "  batch 16 loss: 0.0004814659187104553\n",
      "  batch 17 loss: 0.0006325629074126482\n",
      "  batch 18 loss: 0.0004952652379870415\n",
      "  batch 19 loss: 0.0009571819682605565\n",
      "  batch 20 loss: 0.0008377342019230127\n",
      "  batch 21 loss: 0.000878792954608798\n",
      "  batch 22 loss: 0.0007614128408022225\n",
      "  batch 23 loss: 0.0007834853604435921\n",
      "  batch 24 loss: 0.0007245474844239652\n",
      "  batch 25 loss: 0.0008370623691007495\n",
      "  batch 26 loss: 0.0005925921723246574\n",
      "  batch 27 loss: 0.0004463896038942039\n",
      "  batch 28 loss: 0.0005822831299155951\n",
      "  batch 29 loss: 0.00038429826963692904\n",
      "  batch 30 loss: 0.00042955984827131033\n",
      "  batch 31 loss: 0.00041873299051076174\n",
      "  batch 32 loss: 0.0004716493422165513\n",
      "  batch 33 loss: 0.0005195956910029054\n",
      "  batch 34 loss: 0.0004797873261850327\n",
      "  batch 35 loss: 0.00041939012589864433\n",
      "  batch 36 loss: 0.00042961875442415476\n",
      "  batch 37 loss: 0.00036741612711921334\n",
      "  batch 38 loss: 0.0003772836353164166\n",
      "  batch 39 loss: 0.00042445032158866525\n",
      "  batch 40 loss: 0.0005036920192651451\n",
      "  batch 41 loss: 0.00040978158358484507\n",
      "  batch 42 loss: 0.00044121441897004843\n",
      "  batch 43 loss: 0.0003957256849389523\n",
      "  batch 44 loss: 0.000313350697979331\n",
      "  batch 45 loss: 0.0003554568102117628\n",
      "  batch 46 loss: 0.0004353193216957152\n",
      "  batch 47 loss: 0.00038425519596785307\n",
      "  batch 48 loss: 0.0002948409819509834\n",
      "  batch 49 loss: 0.0003392077924218029\n",
      "  batch 50 loss: 0.00036207609809935093\n",
      "  batch 51 loss: 0.00041939778020605445\n",
      "  batch 52 loss: 0.00032219363492913544\n",
      "  batch 53 loss: 0.00033354677725583315\n",
      "  batch 54 loss: 0.0004028757684864104\n",
      "  batch 55 loss: 0.00040599139174446464\n",
      "  batch 56 loss: 0.00035790729452855885\n",
      "  batch 57 loss: 0.0002917747478932142\n",
      "  batch 58 loss: 0.0005384105606935918\n",
      "  batch 59 loss: 0.0007491447031497955\n",
      "  batch 60 loss: 0.0005066791200079024\n",
      "  batch 61 loss: 0.000400427496060729\n",
      "  batch 62 loss: 0.00039666061638854444\n",
      "  batch 63 loss: 0.00025739078409969807\n",
      "  batch 64 loss: 0.0002831852762028575\n",
      "  batch 65 loss: 0.0003197587502654642\n",
      "  batch 66 loss: 0.00029196898685768247\n",
      "  batch 67 loss: 0.0002763353113550693\n",
      "  batch 68 loss: 0.0003794205258600414\n",
      "  batch 69 loss: 0.0003309407620690763\n",
      "  batch 70 loss: 0.00039329423452727497\n",
      "  batch 71 loss: 0.00029829569393768907\n",
      "  batch 72 loss: 0.00028727107564918697\n",
      "  batch 73 loss: 0.0002928761823568493\n",
      "  batch 74 loss: 0.0003024307079613209\n",
      "  batch 75 loss: 0.0004264092422090471\n",
      "  batch 76 loss: 0.0003415920655243099\n",
      "  batch 77 loss: 0.00032006195397116244\n",
      "  batch 78 loss: 0.0003230697475373745\n",
      "  batch 79 loss: 0.0003256743657402694\n",
      "  batch 80 loss: 0.00036071124486625195\n",
      "  batch 81 loss: 0.00033668684773147106\n",
      "  batch 82 loss: 0.00036955878022126853\n",
      "  batch 83 loss: 0.00038231475627981126\n",
      "  batch 84 loss: 0.00030399992829188704\n",
      "  batch 85 loss: 0.00032859048224054277\n",
      "  batch 86 loss: 0.0003755386278498918\n",
      "  batch 87 loss: 0.0003871659282594919\n",
      "  batch 88 loss: 0.0006407282780855894\n",
      "  batch 89 loss: 0.00045278447214514017\n",
      "  batch 90 loss: 0.0005121569847688079\n",
      "  batch 91 loss: 0.0004883877700194716\n",
      "  batch 92 loss: 0.0004254705272614956\n",
      "  batch 93 loss: 0.00039940926944836974\n",
      "  batch 94 loss: 0.0003772271447815001\n",
      "  batch 95 loss: 0.000611895346082747\n",
      "LOSS train 0.000611895346082747 valid 0.0007299453718587756\n",
      "LOSS train 0.000611895346082747 valid 0.0008776384638622403\n",
      "LOSS train 0.000611895346082747 valid 0.0008216509595513344\n",
      "LOSS train 0.000611895346082747 valid 0.0008082404383458197\n",
      "LOSS train 0.000611895346082747 valid 0.0007899018819443882\n",
      "LOSS train 0.000611895346082747 valid 0.0007570101879537106\n",
      "LOSS train 0.000611895346082747 valid 0.0007911373977549374\n",
      "LOSS train 0.000611895346082747 valid 0.0008105206652544439\n",
      "LOSS train 0.000611895346082747 valid 0.0007870838744565845\n",
      "LOSS train 0.000611895346082747 valid 0.0008011019672267139\n",
      "LOSS train 0.000611895346082747 valid 0.0008274928550235927\n",
      "LOSS train 0.000611895346082747 valid 0.0008307114476338029\n",
      "LOSS train 0.000611895346082747 valid 0.00082237838068977\n",
      "LOSS train 0.000611895346082747 valid 0.0008354961173608899\n",
      "LOSS train 0.000611895346082747 valid 0.0008840782684274018\n",
      "LOSS train 0.000611895346082747 valid 0.0009127595112659037\n",
      "LOSS train 0.000611895346082747 valid 0.0009095693822018802\n",
      "LOSS train 0.000611895346082747 valid 0.0009136989829130471\n",
      "LOSS train 0.000611895346082747 valid 0.0009173957514576614\n",
      "LOSS train 0.000611895346082747 valid 0.000925735745113343\n",
      "LOSS train 0.000611895346082747 valid 0.0009207982802763581\n",
      "LOSS train 0.000611895346082747 valid 0.0009315598290413618\n",
      "LOSS train 0.000611895346082747 valid 0.0009385853772982955\n",
      "LOSS train 0.000611895346082747 valid 0.0009400638518854976\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 0.0004693649825640023\n",
      "  batch 2 loss: 0.0004212708445265889\n",
      "  batch 3 loss: 0.0005449493182823062\n",
      "  batch 4 loss: 0.0004414452996570617\n",
      "  batch 5 loss: 0.00041439285269007087\n",
      "  batch 6 loss: 0.00040707914740778506\n",
      "  batch 7 loss: 0.00035041020601056516\n",
      "  batch 8 loss: 0.00037708767922595143\n",
      "  batch 9 loss: 0.0003449357463978231\n",
      "  batch 10 loss: 0.0003101578913629055\n",
      "  batch 11 loss: 0.0006317198276519775\n",
      "  batch 12 loss: 0.0005809831782244146\n",
      "  batch 13 loss: 0.0004611072363331914\n",
      "  batch 14 loss: 0.000939853023737669\n",
      "  batch 15 loss: 0.0006321787368506193\n",
      "  batch 16 loss: 0.0004857294843532145\n",
      "  batch 17 loss: 0.0006274140905588865\n",
      "  batch 18 loss: 0.0004800265305675566\n",
      "  batch 19 loss: 0.0009519613813608885\n",
      "  batch 20 loss: 0.0008132599759846926\n",
      "  batch 21 loss: 0.0008712747367098927\n",
      "  batch 22 loss: 0.0007680336711928248\n",
      "  batch 23 loss: 0.0007842436898499727\n",
      "  batch 24 loss: 0.0007245497545227408\n",
      "  batch 25 loss: 0.000840319087728858\n",
      "  batch 26 loss: 0.0005969927879050374\n",
      "  batch 27 loss: 0.00043763458961620927\n",
      "  batch 28 loss: 0.00056820223107934\n",
      "  batch 29 loss: 0.0003733219054993242\n",
      "  batch 30 loss: 0.00042131368536502123\n",
      "  batch 31 loss: 0.0004001089255325496\n",
      "  batch 32 loss: 0.0004574352642521262\n",
      "  batch 33 loss: 0.0005084865842945874\n",
      "  batch 34 loss: 0.00047275162069126964\n",
      "  batch 35 loss: 0.0004014442674815655\n",
      "  batch 36 loss: 0.00041824980871751904\n",
      "  batch 37 loss: 0.00036430073669180274\n",
      "  batch 38 loss: 0.00036962208105251193\n",
      "  batch 39 loss: 0.0004170798056293279\n",
      "  batch 40 loss: 0.0005054172361269593\n",
      "  batch 41 loss: 0.0004083926323801279\n",
      "  batch 42 loss: 0.00044423743383958936\n",
      "  batch 43 loss: 0.00038397597381845117\n",
      "  batch 44 loss: 0.0003013456880580634\n",
      "  batch 45 loss: 0.00034524593502283096\n",
      "  batch 46 loss: 0.0004202713316772133\n",
      "  batch 47 loss: 0.0003752104821614921\n",
      "  batch 48 loss: 0.000285541609628126\n",
      "  batch 49 loss: 0.000338402867782861\n",
      "  batch 50 loss: 0.0003532038535922766\n",
      "  batch 51 loss: 0.0004177836235612631\n",
      "  batch 52 loss: 0.00032174302032217383\n",
      "  batch 53 loss: 0.0003286429273430258\n",
      "  batch 54 loss: 0.00038686435436829925\n",
      "  batch 55 loss: 0.000384833721909672\n",
      "  batch 56 loss: 0.00034043160849250853\n",
      "  batch 57 loss: 0.00028752320213243365\n",
      "  batch 58 loss: 0.0005150026408955455\n",
      "  batch 59 loss: 0.0007507335394620895\n",
      "  batch 60 loss: 0.0005147861666046083\n",
      "  batch 61 loss: 0.0003924700431525707\n",
      "  batch 62 loss: 0.00037712755147367716\n",
      "  batch 63 loss: 0.0002499852853361517\n",
      "  batch 64 loss: 0.0002745902456808835\n",
      "  batch 65 loss: 0.0003123399510513991\n",
      "  batch 66 loss: 0.00028796421247534454\n",
      "  batch 67 loss: 0.00026838629855774343\n",
      "  batch 68 loss: 0.0003606431419029832\n",
      "  batch 69 loss: 0.0003258350188843906\n",
      "  batch 70 loss: 0.0003938473528251052\n",
      "  batch 71 loss: 0.00028625759296119213\n",
      "  batch 72 loss: 0.00028198937070555985\n",
      "  batch 73 loss: 0.0002857187355402857\n",
      "  batch 74 loss: 0.00029925155104137957\n",
      "  batch 75 loss: 0.0004187687300145626\n",
      "  batch 76 loss: 0.00033759980578906834\n",
      "  batch 77 loss: 0.00030972674721851945\n",
      "  batch 78 loss: 0.00031437817960977554\n",
      "  batch 79 loss: 0.00031554087763652205\n",
      "  batch 80 loss: 0.00034972873982042074\n",
      "  batch 81 loss: 0.0003321291005704552\n",
      "  batch 82 loss: 0.0003595238667912781\n",
      "  batch 83 loss: 0.00037161033833399415\n",
      "  batch 84 loss: 0.0002999990829266608\n",
      "  batch 85 loss: 0.0003208815469406545\n",
      "  batch 86 loss: 0.00036476520472206175\n",
      "  batch 87 loss: 0.0003841693978756666\n",
      "  batch 88 loss: 0.0006255090702325106\n",
      "  batch 89 loss: 0.00045232631964609027\n",
      "  batch 90 loss: 0.0005063411663286388\n",
      "  batch 91 loss: 0.0004734887625090778\n",
      "  batch 92 loss: 0.00040994977462105453\n",
      "  batch 93 loss: 0.0003875895927194506\n",
      "  batch 94 loss: 0.000370952911907807\n",
      "  batch 95 loss: 0.0006029722280800343\n",
      "LOSS train 0.0006029722280800343 valid 0.0007530927541665733\n",
      "LOSS train 0.0006029722280800343 valid 0.0009126727236434817\n",
      "LOSS train 0.0006029722280800343 valid 0.0008371893200092018\n",
      "LOSS train 0.0006029722280800343 valid 0.0008152657537721097\n",
      "LOSS train 0.0006029722280800343 valid 0.0008016005158424377\n",
      "LOSS train 0.0006029722280800343 valid 0.000770886312238872\n",
      "LOSS train 0.0006029722280800343 valid 0.00080961111234501\n",
      "LOSS train 0.0006029722280800343 valid 0.0008313579601235688\n",
      "LOSS train 0.0006029722280800343 valid 0.0008053536876104772\n",
      "LOSS train 0.0006029722280800343 valid 0.0008248098311014473\n",
      "LOSS train 0.0006029722280800343 valid 0.0008565864409320056\n",
      "LOSS train 0.0006029722280800343 valid 0.0008619907312095165\n",
      "LOSS train 0.0006029722280800343 valid 0.0008505480946041644\n",
      "LOSS train 0.0006029722280800343 valid 0.0008660138701088727\n",
      "LOSS train 0.0006029722280800343 valid 0.0009239420760422945\n",
      "LOSS train 0.0006029722280800343 valid 0.0009577797027304769\n",
      "LOSS train 0.0006029722280800343 valid 0.000956408039201051\n",
      "LOSS train 0.0006029722280800343 valid 0.000961822341196239\n",
      "LOSS train 0.0006029722280800343 valid 0.000963648606557399\n",
      "LOSS train 0.0006029722280800343 valid 0.0009712918545119464\n",
      "LOSS train 0.0006029722280800343 valid 0.00096426613163203\n",
      "LOSS train 0.0006029722280800343 valid 0.000973822723608464\n",
      "LOSS train 0.0006029722280800343 valid 0.000979978358373046\n",
      "LOSS train 0.0006029722280800343 valid 0.0009822975844144821\n",
      "EPOCH 17:\n",
      "  batch 1 loss: 0.00048403191613033414\n",
      "  batch 2 loss: 0.00041611757478676736\n",
      "  batch 3 loss: 0.0005611374508589506\n",
      "  batch 4 loss: 0.00042233249405398965\n",
      "  batch 5 loss: 0.00041007614345289767\n",
      "  batch 6 loss: 0.00040095660369843245\n",
      "  batch 7 loss: 0.00034660595702007413\n",
      "  batch 8 loss: 0.0003739765379577875\n",
      "  batch 9 loss: 0.00034022991894744337\n",
      "  batch 10 loss: 0.00029953400371596217\n",
      "  batch 11 loss: 0.0006182583747431636\n",
      "  batch 12 loss: 0.0005554793169721961\n",
      "  batch 13 loss: 0.00047046272084116936\n",
      "  batch 14 loss: 0.0009410065831616521\n",
      "  batch 15 loss: 0.0006087434012442827\n",
      "  batch 16 loss: 0.00046811363426968455\n",
      "  batch 17 loss: 0.0006262988317757845\n",
      "  batch 18 loss: 0.0004620372492354363\n",
      "  batch 19 loss: 0.0009119845344685018\n",
      "  batch 20 loss: 0.0008035050705075264\n",
      "  batch 21 loss: 0.0008611232624389231\n",
      "  batch 22 loss: 0.0007446437375620008\n",
      "  batch 23 loss: 0.0007712069200351834\n",
      "  batch 24 loss: 0.0007093703607097268\n",
      "  batch 25 loss: 0.0008304496295750141\n",
      "  batch 26 loss: 0.0005799102946184576\n",
      "  batch 27 loss: 0.0004226562741678208\n",
      "  batch 28 loss: 0.0005604706821031868\n",
      "  batch 29 loss: 0.00036505580646917224\n",
      "  batch 30 loss: 0.0004204055876471102\n",
      "  batch 31 loss: 0.0003985673247370869\n",
      "  batch 32 loss: 0.00045452610356733203\n",
      "  batch 33 loss: 0.0004961915547028184\n",
      "  batch 34 loss: 0.0004775446723215282\n",
      "  batch 35 loss: 0.00040865776827558875\n",
      "  batch 36 loss: 0.00041499815415591\n",
      "  batch 37 loss: 0.0003565045481082052\n",
      "  batch 38 loss: 0.0003599287592805922\n",
      "  batch 39 loss: 0.00041046104161068797\n",
      "  batch 40 loss: 0.0004952890449203551\n",
      "  batch 41 loss: 0.0004053049487993121\n",
      "  batch 42 loss: 0.00044735369738191366\n",
      "  batch 43 loss: 0.00038306170608848333\n",
      "  batch 44 loss: 0.0002965908497571945\n",
      "  batch 45 loss: 0.00034034845884889364\n",
      "  batch 46 loss: 0.00040824615280143917\n",
      "  batch 47 loss: 0.00038455307367257774\n",
      "  batch 48 loss: 0.00029347551753744483\n",
      "  batch 49 loss: 0.00033634499413892627\n",
      "  batch 50 loss: 0.0003648992860689759\n",
      "  batch 51 loss: 0.00040997331961989403\n",
      "  batch 52 loss: 0.00031489512184634805\n",
      "  batch 53 loss: 0.0003282602410763502\n",
      "  batch 54 loss: 0.0003757498343475163\n",
      "  batch 55 loss: 0.0003863712481688708\n",
      "  batch 56 loss: 0.0003468561917543411\n",
      "  batch 57 loss: 0.00028049031971022487\n",
      "  batch 58 loss: 0.000509462843183428\n",
      "  batch 59 loss: 0.0007348846411332488\n",
      "  batch 60 loss: 0.0005061771953478456\n",
      "  batch 61 loss: 0.00038487586425617337\n",
      "  batch 62 loss: 0.0003742123080883175\n",
      "  batch 63 loss: 0.0002480914117768407\n",
      "  batch 64 loss: 0.00027537235291674733\n",
      "  batch 65 loss: 0.00031472338014282286\n",
      "  batch 66 loss: 0.0002821334928739816\n",
      "  batch 67 loss: 0.00030158882145769894\n",
      "  batch 68 loss: 0.00035733432741835713\n",
      "  batch 69 loss: 0.0003204216482117772\n",
      "  batch 70 loss: 0.00039184611523523927\n",
      "  batch 71 loss: 0.000283720379229635\n",
      "  batch 72 loss: 0.0002784767420962453\n",
      "  batch 73 loss: 0.0002859917585738003\n",
      "  batch 74 loss: 0.00029770791297778487\n",
      "  batch 75 loss: 0.0004244603042025119\n",
      "  batch 76 loss: 0.0003335561486892402\n",
      "  batch 77 loss: 0.0003111693076789379\n",
      "  batch 78 loss: 0.0003151758573949337\n",
      "  batch 79 loss: 0.0003157721948809922\n",
      "  batch 80 loss: 0.0003564420621842146\n",
      "  batch 81 loss: 0.0003320596588309854\n",
      "  batch 82 loss: 0.00036340445512905717\n",
      "  batch 83 loss: 0.0003709474694915116\n",
      "  batch 84 loss: 0.0002974728122353554\n",
      "  batch 85 loss: 0.00031779520213603973\n",
      "  batch 86 loss: 0.0003623907105065882\n",
      "  batch 87 loss: 0.0003770751936826855\n",
      "  batch 88 loss: 0.0006848285556770861\n",
      "  batch 89 loss: 0.00045526190660893917\n",
      "  batch 90 loss: 0.0005095169181004167\n",
      "  batch 91 loss: 0.00048137499834410846\n",
      "  batch 92 loss: 0.0003968477249145508\n",
      "  batch 93 loss: 0.00037607987178489566\n",
      "  batch 94 loss: 0.0003670398727990687\n",
      "  batch 95 loss: 0.000600141822360456\n",
      "LOSS train 0.000600141822360456 valid 0.0007417143788188696\n",
      "LOSS train 0.000600141822360456 valid 0.0008746295934543014\n",
      "LOSS train 0.000600141822360456 valid 0.0008169549982994795\n",
      "LOSS train 0.000600141822360456 valid 0.0008084080764092505\n",
      "LOSS train 0.000600141822360456 valid 0.0007871704292483628\n",
      "LOSS train 0.000600141822360456 valid 0.0007515886100009084\n",
      "LOSS train 0.000600141822360456 valid 0.0007819001912139356\n",
      "LOSS train 0.000600141822360456 valid 0.0007957709603942931\n",
      "LOSS train 0.000600141822360456 valid 0.0007722012815065682\n",
      "LOSS train 0.000600141822360456 valid 0.0007835404830984771\n",
      "LOSS train 0.000600141822360456 valid 0.000806363474112004\n",
      "LOSS train 0.000600141822360456 valid 0.000809504184871912\n",
      "LOSS train 0.000600141822360456 valid 0.0008031016332097352\n",
      "LOSS train 0.000600141822360456 valid 0.000815759995020926\n",
      "LOSS train 0.000600141822360456 valid 0.0008613324607722461\n",
      "LOSS train 0.000600141822360456 valid 0.0008871683967299759\n",
      "LOSS train 0.000600141822360456 valid 0.0008841042290441692\n",
      "LOSS train 0.000600141822360456 valid 0.0008867043652571738\n",
      "LOSS train 0.000600141822360456 valid 0.0008897786610759795\n",
      "LOSS train 0.000600141822360456 valid 0.0008984747692011297\n",
      "LOSS train 0.000600141822360456 valid 0.0008945353329181671\n",
      "LOSS train 0.000600141822360456 valid 0.0009066601051017642\n",
      "LOSS train 0.000600141822360456 valid 0.0009130738326348364\n",
      "LOSS train 0.000600141822360456 valid 0.0009164043585769832\n",
      "EPOCH 18:\n",
      "  batch 1 loss: 0.00048557016998529434\n",
      "  batch 2 loss: 0.0004255098756402731\n",
      "  batch 3 loss: 0.0005366936093196273\n",
      "  batch 4 loss: 0.0004384198400657624\n",
      "  batch 5 loss: 0.00041979545494541526\n",
      "  batch 6 loss: 0.00041809299727901816\n",
      "  batch 7 loss: 0.00035357882734388113\n",
      "  batch 8 loss: 0.0003877558629028499\n",
      "  batch 9 loss: 0.000335326069034636\n",
      "  batch 10 loss: 0.0003059149894397706\n",
      "  batch 11 loss: 0.0006233683088794351\n",
      "  batch 12 loss: 0.0005592548986896873\n",
      "  batch 13 loss: 0.0004347575013525784\n",
      "  batch 14 loss: 0.0009091077954508364\n",
      "  batch 15 loss: 0.0006255490006878972\n",
      "  batch 16 loss: 0.0004587003495544195\n",
      "  batch 17 loss: 0.0006085757631808519\n",
      "  batch 18 loss: 0.00045769120333716273\n",
      "  batch 19 loss: 0.0009253880707547069\n",
      "  batch 20 loss: 0.0008114200318232179\n",
      "  batch 21 loss: 0.0008884173585101962\n",
      "  batch 22 loss: 0.0007368059596046805\n",
      "  batch 23 loss: 0.000778353598434478\n",
      "  batch 24 loss: 0.0007205153815448284\n",
      "  batch 25 loss: 0.000841587083414197\n",
      "  batch 26 loss: 0.000593792530708015\n",
      "  batch 27 loss: 0.0004487863043323159\n",
      "  batch 28 loss: 0.0005605851765722036\n",
      "  batch 29 loss: 0.0003696419298648834\n",
      "  batch 30 loss: 0.00043193361489102244\n",
      "  batch 31 loss: 0.0004016491584479809\n",
      "  batch 32 loss: 0.0004524872056208551\n",
      "  batch 33 loss: 0.0004958235658705235\n",
      "  batch 34 loss: 0.00046258122893050313\n",
      "  batch 35 loss: 0.00040067319059744477\n",
      "  batch 36 loss: 0.00041112001053988934\n",
      "  batch 37 loss: 0.00035255064722150564\n",
      "  batch 38 loss: 0.00037507974775508046\n",
      "  batch 39 loss: 0.0004131619061809033\n",
      "  batch 40 loss: 0.0004934793105348945\n",
      "  batch 41 loss: 0.0003815115778706968\n",
      "  batch 42 loss: 0.0004302415472920984\n",
      "  batch 43 loss: 0.0003611812135204673\n",
      "  batch 44 loss: 0.00029823638033121824\n",
      "  batch 45 loss: 0.0003332669148221612\n",
      "  batch 46 loss: 0.0004096252378076315\n",
      "  batch 47 loss: 0.00037510693073272705\n",
      "  batch 48 loss: 0.0002748703118413687\n",
      "  batch 49 loss: 0.00032649090280756354\n",
      "  batch 50 loss: 0.00034613453317433596\n",
      "  batch 51 loss: 0.0004125176928937435\n",
      "  batch 52 loss: 0.0003083669871557504\n",
      "  batch 53 loss: 0.0003274183254688978\n",
      "  batch 54 loss: 0.00036980712320655584\n",
      "  batch 55 loss: 0.0003735824138857424\n",
      "  batch 56 loss: 0.00032905314583331347\n",
      "  batch 57 loss: 0.00027326878625899553\n",
      "  batch 58 loss: 0.0004999965894967318\n",
      "  batch 59 loss: 0.0007719961577095091\n",
      "  batch 60 loss: 0.0004938453203067183\n",
      "  batch 61 loss: 0.00038723694160580635\n",
      "  batch 62 loss: 0.00037113300641067326\n",
      "  batch 63 loss: 0.0002435068745398894\n",
      "  batch 64 loss: 0.0002587184717413038\n",
      "  batch 65 loss: 0.0003012617817148566\n",
      "  batch 66 loss: 0.00027658068574965\n",
      "  batch 67 loss: 0.0002768412814475596\n",
      "  batch 68 loss: 0.00034579657949507236\n",
      "  batch 69 loss: 0.0003292079200036824\n",
      "  batch 70 loss: 0.00038624851731583476\n",
      "  batch 71 loss: 0.0002941528509836644\n",
      "  batch 72 loss: 0.00027991458773612976\n",
      "  batch 73 loss: 0.00028099986957386136\n",
      "  batch 74 loss: 0.0002962973667308688\n",
      "  batch 75 loss: 0.0004155530477873981\n",
      "  batch 76 loss: 0.0003292931360192597\n",
      "  batch 77 loss: 0.00031225394923239946\n",
      "  batch 78 loss: 0.00030868916655890644\n",
      "  batch 79 loss: 0.00031246605794876814\n",
      "  batch 80 loss: 0.00034874817356467247\n",
      "  batch 81 loss: 0.0003278008953202516\n",
      "  batch 82 loss: 0.0003560274781193584\n",
      "  batch 83 loss: 0.000372410228010267\n",
      "  batch 84 loss: 0.0002942752616945654\n",
      "  batch 85 loss: 0.0003179267223458737\n",
      "  batch 86 loss: 0.0003593952569644898\n",
      "  batch 87 loss: 0.00037631328450515866\n",
      "  batch 88 loss: 0.0006749614840373397\n",
      "  batch 89 loss: 0.0004601369146257639\n",
      "  batch 90 loss: 0.0004894979647360742\n",
      "  batch 91 loss: 0.00046291190665215254\n",
      "  batch 92 loss: 0.0003844725142698735\n",
      "  batch 93 loss: 0.0003713803016580641\n",
      "  batch 94 loss: 0.0003582889912649989\n",
      "  batch 95 loss: 0.0005771411233581603\n",
      "LOSS train 0.0005771411233581603 valid 0.0007443169597536325\n",
      "LOSS train 0.0005771411233581603 valid 0.0009088434744626284\n",
      "LOSS train 0.0005771411233581603 valid 0.0008313250727951527\n",
      "LOSS train 0.0005771411233581603 valid 0.0008060270338319242\n",
      "LOSS train 0.0005771411233581603 valid 0.0007923133671283722\n",
      "LOSS train 0.0005771411233581603 valid 0.0007551812450401485\n",
      "LOSS train 0.0005771411233581603 valid 0.0007934055756777525\n",
      "LOSS train 0.0005771411233581603 valid 0.000817011168692261\n",
      "LOSS train 0.0005771411233581603 valid 0.0007895521121099591\n",
      "LOSS train 0.0005771411233581603 valid 0.0008047975716181099\n",
      "LOSS train 0.0005771411233581603 valid 0.0008346694521605968\n",
      "LOSS train 0.0005771411233581603 valid 0.0008373401942662895\n",
      "LOSS train 0.0005771411233581603 valid 0.0008277252200059593\n",
      "LOSS train 0.0005771411233581603 valid 0.0008424255647696555\n",
      "LOSS train 0.0005771411233581603 valid 0.0008985190652310848\n",
      "LOSS train 0.0005771411233581603 valid 0.0009325938299298286\n",
      "LOSS train 0.0005771411233581603 valid 0.0009306173888035119\n",
      "LOSS train 0.0005771411233581603 valid 0.0009365992737002671\n",
      "LOSS train 0.0005771411233581603 valid 0.0009411697974428535\n",
      "LOSS train 0.0005771411233581603 valid 0.0009495980921201408\n",
      "LOSS train 0.0005771411233581603 valid 0.0009417621186003089\n",
      "LOSS train 0.0005771411233581603 valid 0.0009516830323264003\n",
      "LOSS train 0.0005771411233581603 valid 0.000957305368501693\n",
      "LOSS train 0.0005771411233581603 valid 0.0009621296776458621\n",
      "EPOCH 19:\n",
      "  batch 1 loss: 0.0004570124438032508\n",
      "  batch 2 loss: 0.0004034679150208831\n",
      "  batch 3 loss: 0.0005085830925963819\n",
      "  batch 4 loss: 0.00042191153625026345\n",
      "  batch 5 loss: 0.0004164389683865011\n",
      "  batch 6 loss: 0.00041055408655665815\n",
      "  batch 7 loss: 0.00034606328699737787\n",
      "  batch 8 loss: 0.0003764771099667996\n",
      "  batch 9 loss: 0.0003345392760820687\n",
      "  batch 10 loss: 0.0003140092012472451\n",
      "  batch 11 loss: 0.000628502166364342\n",
      "  batch 12 loss: 0.0005490798503160477\n",
      "  batch 13 loss: 0.0004383442283142358\n",
      "  batch 14 loss: 0.0009074217523448169\n",
      "  batch 15 loss: 0.0005922296550124884\n",
      "  batch 16 loss: 0.0004420938203111291\n",
      "  batch 17 loss: 0.0006080841412767768\n",
      "  batch 18 loss: 0.0004322479653637856\n",
      "  batch 19 loss: 0.0008968538604676723\n",
      "  batch 20 loss: 0.0007649972103536129\n",
      "  batch 21 loss: 0.0007926207035779953\n",
      "  batch 22 loss: 0.0006851661019027233\n",
      "  batch 23 loss: 0.000720889656804502\n",
      "  batch 24 loss: 0.0006602052599191666\n",
      "  batch 25 loss: 0.0007396832806989551\n",
      "  batch 26 loss: 0.0006260800291784108\n",
      "  batch 27 loss: 0.0004448540566954762\n",
      "  batch 28 loss: 0.0006089049857109785\n",
      "  batch 29 loss: 0.00035741517785936594\n",
      "  batch 30 loss: 0.00045047153253108263\n",
      "  batch 31 loss: 0.0004149266751483083\n",
      "  batch 32 loss: 0.0004463373916223645\n",
      "  batch 33 loss: 0.0005334342131391168\n",
      "  batch 34 loss: 0.00045857077930122614\n",
      "  batch 35 loss: 0.0003724982962012291\n",
      "  batch 36 loss: 0.00038522991235367954\n",
      "  batch 37 loss: 0.00033997633727267385\n",
      "  batch 38 loss: 0.0003217845514882356\n",
      "  batch 39 loss: 0.00039932498475536704\n",
      "  batch 40 loss: 0.0005524114822037518\n",
      "  batch 41 loss: 0.00033939306740649045\n",
      "  batch 42 loss: 0.00037110812263563275\n",
      "  batch 43 loss: 0.00036093697417527437\n",
      "  batch 44 loss: 0.00027561315800994635\n",
      "  batch 45 loss: 0.00032150920014828444\n",
      "  batch 46 loss: 0.000425392237957567\n",
      "  batch 47 loss: 0.0003953593550249934\n",
      "  batch 48 loss: 0.00028610180015675724\n",
      "  batch 49 loss: 0.00032598094549030066\n",
      "  batch 50 loss: 0.00033623151830397546\n",
      "  batch 51 loss: 0.0004167082952335477\n",
      "  batch 52 loss: 0.0003085357020609081\n",
      "  batch 53 loss: 0.0003230943693779409\n",
      "  batch 54 loss: 0.0003527657245285809\n",
      "  batch 55 loss: 0.0003620583738666028\n",
      "  batch 56 loss: 0.00032201086287386715\n",
      "  batch 57 loss: 0.00026204067398793995\n",
      "  batch 58 loss: 0.0004564856644719839\n",
      "  batch 59 loss: 0.0007190838805399835\n",
      "  batch 60 loss: 0.000471819716040045\n",
      "  batch 61 loss: 0.0003496962017379701\n",
      "  batch 62 loss: 0.00036350139998830855\n",
      "  batch 63 loss: 0.00024100849987007678\n",
      "  batch 64 loss: 0.00023221757146529853\n",
      "  batch 65 loss: 0.0002767978294286877\n",
      "  batch 66 loss: 0.0002812011225614697\n",
      "  batch 67 loss: 0.0002801489899866283\n",
      "  batch 68 loss: 0.000376217212760821\n",
      "  batch 69 loss: 0.00032641930738464\n",
      "  batch 70 loss: 0.0003787398454733193\n",
      "  batch 71 loss: 0.00029475660994648933\n",
      "  batch 72 loss: 0.00027493451489135623\n",
      "  batch 73 loss: 0.00028602092061191797\n",
      "  batch 74 loss: 0.0002895434445235878\n",
      "  batch 75 loss: 0.00041397439781576395\n",
      "  batch 76 loss: 0.00031585857504978776\n",
      "  batch 77 loss: 0.00030102391610853374\n",
      "  batch 78 loss: 0.0003034707624465227\n",
      "  batch 79 loss: 0.00031657650833949447\n",
      "  batch 80 loss: 0.00035470054717734456\n",
      "  batch 81 loss: 0.0003286491264589131\n",
      "  batch 82 loss: 0.00035922834649682045\n",
      "  batch 83 loss: 0.00036887102760374546\n",
      "  batch 84 loss: 0.00029454322066158056\n",
      "  batch 85 loss: 0.00031440245220437646\n",
      "  batch 86 loss: 0.00034764042356982827\n",
      "  batch 87 loss: 0.0003694204206112772\n",
      "  batch 88 loss: 0.0006579953478649259\n",
      "  batch 89 loss: 0.00044927376438863575\n",
      "  batch 90 loss: 0.0004876643361058086\n",
      "  batch 91 loss: 0.00044435294694267213\n",
      "  batch 92 loss: 0.00038831436540931463\n",
      "  batch 93 loss: 0.000365065032383427\n",
      "  batch 94 loss: 0.0003608735860325396\n",
      "  batch 95 loss: 0.000575368118006736\n",
      "LOSS train 0.000575368118006736 valid 0.0007128863944672048\n",
      "LOSS train 0.000575368118006736 valid 0.0008602876914665103\n",
      "LOSS train 0.000575368118006736 valid 0.00080698705278337\n",
      "LOSS train 0.000575368118006736 valid 0.0007895812159404159\n",
      "LOSS train 0.000575368118006736 valid 0.0007708676857873797\n",
      "LOSS train 0.000575368118006736 valid 0.0007305810577236116\n",
      "LOSS train 0.000575368118006736 valid 0.000764072930905968\n",
      "LOSS train 0.000575368118006736 valid 0.000782726623583585\n",
      "LOSS train 0.000575368118006736 valid 0.0007602840196341276\n",
      "LOSS train 0.000575368118006736 valid 0.0007741944282315671\n",
      "LOSS train 0.000575368118006736 valid 0.0008017753134481609\n",
      "LOSS train 0.000575368118006736 valid 0.0008046982693485916\n",
      "LOSS train 0.000575368118006736 valid 0.0007985521806403995\n",
      "LOSS train 0.000575368118006736 valid 0.0008115141536109149\n",
      "LOSS train 0.000575368118006736 valid 0.0008614892140030861\n",
      "LOSS train 0.000575368118006736 valid 0.0008923381101340055\n",
      "LOSS train 0.000575368118006736 valid 0.0008897667867131531\n",
      "LOSS train 0.000575368118006736 valid 0.000893286953214556\n",
      "LOSS train 0.000575368118006736 valid 0.0008972450159490108\n",
      "LOSS train 0.000575368118006736 valid 0.0009041784214787185\n",
      "LOSS train 0.000575368118006736 valid 0.0008993849623948336\n",
      "LOSS train 0.000575368118006736 valid 0.0009096430148929358\n",
      "LOSS train 0.000575368118006736 valid 0.0009149997495114803\n",
      "LOSS train 0.000575368118006736 valid 0.0009191144490614533\n",
      "EPOCH 20:\n",
      "  batch 1 loss: 0.00046301085967570543\n",
      "  batch 2 loss: 0.00039103219751268625\n",
      "  batch 3 loss: 0.0004915382014587522\n",
      "  batch 4 loss: 0.0004037749022245407\n",
      "  batch 5 loss: 0.00039986040792427957\n",
      "  batch 6 loss: 0.00040564185474067926\n",
      "  batch 7 loss: 0.00033773318864405155\n",
      "  batch 8 loss: 0.0003678249195218086\n",
      "  batch 9 loss: 0.00032702169846743345\n",
      "  batch 10 loss: 0.0003130864351987839\n",
      "  batch 11 loss: 0.0006205028621479869\n",
      "  batch 12 loss: 0.0005406211712397635\n",
      "  batch 13 loss: 0.0004223759169690311\n",
      "  batch 14 loss: 0.0008371652220375836\n",
      "  batch 15 loss: 0.0005688747623935342\n",
      "  batch 16 loss: 0.00044999364763498306\n",
      "  batch 17 loss: 0.0006170353153720498\n",
      "  batch 18 loss: 0.0004507414996623993\n",
      "  batch 19 loss: 0.0009308712324127555\n",
      "  batch 20 loss: 0.0007780330488458276\n",
      "  batch 21 loss: 0.0008212992688640952\n",
      "  batch 22 loss: 0.0007086886907927692\n",
      "  batch 23 loss: 0.0007519457722082734\n",
      "  batch 24 loss: 0.000698329065926373\n",
      "  batch 25 loss: 0.000805579184088856\n",
      "  batch 26 loss: 0.000563988636713475\n",
      "  batch 27 loss: 0.00040510663529857993\n",
      "  batch 28 loss: 0.0005359319038689137\n",
      "  batch 29 loss: 0.00033734890166670084\n",
      "  batch 30 loss: 0.0003973391139879823\n",
      "  batch 31 loss: 0.000372516515199095\n",
      "  batch 32 loss: 0.00043087475933134556\n",
      "  batch 33 loss: 0.00048824140685610473\n",
      "  batch 34 loss: 0.0004498144844546914\n",
      "  batch 35 loss: 0.00038177118403837085\n",
      "  batch 36 loss: 0.0003973256389144808\n",
      "  batch 37 loss: 0.0003368122852407396\n",
      "  batch 38 loss: 0.00035498914076015353\n",
      "  batch 39 loss: 0.00039235781878232956\n",
      "  batch 40 loss: 0.0004732817178592086\n",
      "  batch 41 loss: 0.00037257387884892523\n",
      "  batch 42 loss: 0.00043718190863728523\n",
      "  batch 43 loss: 0.0003479431616142392\n",
      "  batch 44 loss: 0.0002750999992713332\n",
      "  batch 45 loss: 0.0003165339003317058\n",
      "  batch 46 loss: 0.000390593777410686\n",
      "  batch 47 loss: 0.0003615369787439704\n",
      "  batch 48 loss: 0.0002612062089610845\n",
      "  batch 49 loss: 0.00031764409504830837\n",
      "  batch 50 loss: 0.0003271960304118693\n",
      "  batch 51 loss: 0.000405096507165581\n",
      "  batch 52 loss: 0.00030439047259278595\n",
      "  batch 53 loss: 0.0003278612275607884\n",
      "  batch 54 loss: 0.000347229273756966\n",
      "  batch 55 loss: 0.0003549804678186774\n",
      "  batch 56 loss: 0.00031447919900529087\n",
      "  batch 57 loss: 0.000258977641351521\n",
      "  batch 58 loss: 0.0004629160976037383\n",
      "  batch 59 loss: 0.000720420153811574\n",
      "  batch 60 loss: 0.00048244159552268684\n",
      "  batch 61 loss: 0.00035541909164749086\n",
      "  batch 62 loss: 0.0003482347237877548\n",
      "  batch 63 loss: 0.00023344243527390063\n",
      "  batch 64 loss: 0.000244427181314677\n",
      "  batch 65 loss: 0.0002866046270355582\n",
      "  batch 66 loss: 0.0002629969676490873\n",
      "  batch 67 loss: 0.0002459014067426324\n",
      "  batch 68 loss: 0.0003370021586306393\n",
      "  batch 69 loss: 0.0003028981445822865\n",
      "  batch 70 loss: 0.00037764207809232175\n",
      "  batch 71 loss: 0.00027979098376818\n",
      "  batch 72 loss: 0.00027073477394878864\n",
      "  batch 73 loss: 0.0002740327036008239\n",
      "  batch 74 loss: 0.00029453745810315013\n",
      "  batch 75 loss: 0.00040655466727912426\n",
      "  batch 76 loss: 0.0003098917077295482\n",
      "  batch 77 loss: 0.00029258400900289416\n",
      "  batch 78 loss: 0.0002937329118140042\n",
      "  batch 79 loss: 0.00029825905221514404\n",
      "  batch 80 loss: 0.00032911766902543604\n",
      "  batch 81 loss: 0.00031555411987937987\n",
      "  batch 82 loss: 0.0003413581580389291\n",
      "  batch 83 loss: 0.00035199528792873025\n",
      "  batch 84 loss: 0.00028176431078463793\n",
      "  batch 85 loss: 0.00029831676511093974\n",
      "  batch 86 loss: 0.0003405290190130472\n",
      "  batch 87 loss: 0.000364656385499984\n",
      "  batch 88 loss: 0.0006328190211206675\n",
      "  batch 89 loss: 0.00044907897245138884\n",
      "  batch 90 loss: 0.00048254133434966207\n",
      "  batch 91 loss: 0.0004230002814438194\n",
      "  batch 92 loss: 0.00038454035529866815\n",
      "  batch 93 loss: 0.0003649726277217269\n",
      "  batch 94 loss: 0.0003499052254483104\n",
      "  batch 95 loss: 0.0005649401573464274\n",
      "LOSS train 0.0005649401573464274 valid 0.0007427267846651375\n",
      "LOSS train 0.0005649401573464274 valid 0.0009061916498467326\n",
      "LOSS train 0.0005649401573464274 valid 0.0008362446678802371\n",
      "LOSS train 0.0005649401573464274 valid 0.0008103878935799003\n",
      "LOSS train 0.0005649401573464274 valid 0.0007956179906614125\n",
      "LOSS train 0.0005649401573464274 valid 0.0007621934055350721\n",
      "LOSS train 0.0005649401573464274 valid 0.0007951929001137614\n",
      "LOSS train 0.0005649401573464274 valid 0.0008164187893271446\n",
      "LOSS train 0.0005649401573464274 valid 0.0007943589589558542\n",
      "LOSS train 0.0005649401573464274 valid 0.0008089832263067365\n",
      "LOSS train 0.0005649401573464274 valid 0.0008405009284615517\n",
      "LOSS train 0.0005649401573464274 valid 0.0008446629508398473\n",
      "LOSS train 0.0005649401573464274 valid 0.0008340523345395923\n",
      "LOSS train 0.0005649401573464274 valid 0.0008500118856318295\n",
      "LOSS train 0.0005649401573464274 valid 0.0009083974873647094\n",
      "LOSS train 0.0005649401573464274 valid 0.0009428323828615248\n",
      "LOSS train 0.0005649401573464274 valid 0.0009407639736309648\n",
      "LOSS train 0.0005649401573464274 valid 0.0009459149441681802\n",
      "LOSS train 0.0005649401573464274 valid 0.0009486856870353222\n",
      "LOSS train 0.0005649401573464274 valid 0.000955891446210444\n",
      "LOSS train 0.0005649401573464274 valid 0.0009484250331297517\n",
      "LOSS train 0.0005649401573464274 valid 0.0009567291708663106\n",
      "LOSS train 0.0005649401573464274 valid 0.0009632071014493704\n",
      "LOSS train 0.0005649401573464274 valid 0.0009667772683314979\n",
      "EPOCH 21:\n",
      "  batch 1 loss: 0.0004699454002548009\n",
      "  batch 2 loss: 0.0003938371955882758\n",
      "  batch 3 loss: 0.0004894117591902614\n",
      "  batch 4 loss: 0.0003924005141016096\n",
      "  batch 5 loss: 0.0004013479338027537\n",
      "  batch 6 loss: 0.00041166707524098456\n",
      "  batch 7 loss: 0.00033807416912168264\n",
      "  batch 8 loss: 0.0003667254641186446\n",
      "  batch 9 loss: 0.0003145375521853566\n",
      "  batch 10 loss: 0.00029709728551097214\n",
      "  batch 11 loss: 0.0006044915062375367\n",
      "  batch 12 loss: 0.0005451616598293185\n",
      "  batch 13 loss: 0.0004265999305061996\n",
      "  batch 14 loss: 0.000826797098852694\n",
      "  batch 15 loss: 0.0005609325598925352\n",
      "  batch 16 loss: 0.0004370941605884582\n",
      "  batch 17 loss: 0.0006101892213337123\n",
      "  batch 18 loss: 0.0004238467081449926\n",
      "  batch 19 loss: 0.000903994485270232\n",
      "  batch 20 loss: 0.0007623408455401659\n",
      "  batch 21 loss: 0.0007972806924954057\n",
      "  batch 22 loss: 0.000709769781678915\n",
      "  batch 23 loss: 0.0007421240443363786\n",
      "  batch 24 loss: 0.0006934845587238669\n",
      "  batch 25 loss: 0.0007932789158076048\n",
      "  batch 26 loss: 0.0005632671527564526\n",
      "  batch 27 loss: 0.0004020069900434464\n",
      "  batch 28 loss: 0.0005291635170578957\n",
      "  batch 29 loss: 0.00033524056198075414\n",
      "  batch 30 loss: 0.0003920443414244801\n",
      "  batch 31 loss: 0.0003705571871250868\n",
      "  batch 32 loss: 0.00041674141539260745\n",
      "  batch 33 loss: 0.000483874959172681\n",
      "  batch 34 loss: 0.0004427966778166592\n",
      "  batch 35 loss: 0.00038061925442889333\n",
      "  batch 36 loss: 0.0003898566646967083\n",
      "  batch 37 loss: 0.0003323430137243122\n",
      "  batch 38 loss: 0.0003445680486038327\n",
      "  batch 39 loss: 0.00038721723831258714\n",
      "  batch 40 loss: 0.00046270067105069757\n",
      "  batch 41 loss: 0.0003746083821170032\n",
      "  batch 42 loss: 0.00044192562927491963\n",
      "  batch 43 loss: 0.00034609093563631177\n",
      "  batch 44 loss: 0.00026248014182783663\n",
      "  batch 45 loss: 0.00030742341186851263\n",
      "  batch 46 loss: 0.00036382139660418034\n",
      "  batch 47 loss: 0.0003485835622996092\n",
      "  batch 48 loss: 0.0002691838308237493\n",
      "  batch 49 loss: 0.0003184286179021001\n",
      "  batch 50 loss: 0.0003242226375732571\n",
      "  batch 51 loss: 0.00039637787267565727\n",
      "  batch 52 loss: 0.0003022480814252049\n",
      "  batch 53 loss: 0.0003209671122021973\n",
      "  batch 54 loss: 0.00034729440812952816\n",
      "  batch 55 loss: 0.0003495702985674143\n",
      "  batch 56 loss: 0.0003049365186598152\n",
      "  batch 57 loss: 0.00025477755116298795\n",
      "  batch 58 loss: 0.00045786838745698333\n",
      "  batch 59 loss: 0.0007162624970078468\n",
      "  batch 60 loss: 0.0004701682482846081\n",
      "  batch 61 loss: 0.000352709146682173\n",
      "  batch 62 loss: 0.00033992284443229437\n",
      "  batch 63 loss: 0.00023387337569147348\n",
      "  batch 64 loss: 0.0002451303880661726\n",
      "  batch 65 loss: 0.00028181320521980524\n",
      "  batch 66 loss: 0.00026183557929471135\n",
      "  batch 67 loss: 0.0002359748468734324\n",
      "  batch 68 loss: 0.0003226051339879632\n",
      "  batch 69 loss: 0.00029735013958998024\n",
      "  batch 70 loss: 0.0003772249328903854\n",
      "  batch 71 loss: 0.0002761090290732682\n",
      "  batch 72 loss: 0.0002594965626485646\n",
      "  batch 73 loss: 0.0002625375927891582\n",
      "  batch 74 loss: 0.0002871933102142066\n",
      "  batch 75 loss: 0.00040073812124319375\n",
      "  batch 76 loss: 0.00030283391242846847\n",
      "  batch 77 loss: 0.00028669333551079035\n",
      "  batch 78 loss: 0.0002844010596163571\n",
      "  batch 79 loss: 0.000291290576569736\n",
      "  batch 80 loss: 0.0003231126756872982\n",
      "  batch 81 loss: 0.00030987479840405285\n",
      "  batch 82 loss: 0.0003367997123859823\n",
      "  batch 83 loss: 0.0003518494195304811\n",
      "  batch 84 loss: 0.0002773798769339919\n",
      "  batch 85 loss: 0.00029226968763396144\n",
      "  batch 86 loss: 0.00033465740852989256\n",
      "  batch 87 loss: 0.0003579150652512908\n",
      "  batch 88 loss: 0.0006098924204707146\n",
      "  batch 89 loss: 0.0004464858502615243\n",
      "  batch 90 loss: 0.0004740468575619161\n",
      "  batch 91 loss: 0.0004139883676543832\n",
      "  batch 92 loss: 0.00037919875467196107\n",
      "  batch 93 loss: 0.00036311050644144416\n",
      "  batch 94 loss: 0.00034869828959926963\n",
      "  batch 95 loss: 0.0005640436429530382\n",
      "LOSS train 0.0005640436429530382 valid 0.000723204342648387\n",
      "LOSS train 0.0005640436429530382 valid 0.0008815590990707278\n",
      "LOSS train 0.0005640436429530382 valid 0.0008187051862478256\n",
      "LOSS train 0.0005640436429530382 valid 0.00079413375351578\n",
      "LOSS train 0.0005640436429530382 valid 0.0007750117219984531\n",
      "LOSS train 0.0005640436429530382 valid 0.0007377745932899415\n",
      "LOSS train 0.0005640436429530382 valid 0.0007676883251406252\n",
      "LOSS train 0.0005640436429530382 valid 0.0007854445138946176\n",
      "LOSS train 0.0005640436429530382 valid 0.0007643149583600461\n",
      "LOSS train 0.0005640436429530382 valid 0.0007786597707308829\n",
      "LOSS train 0.0005640436429530382 valid 0.0008126794709824026\n",
      "LOSS train 0.0005640436429530382 valid 0.0008154399110935628\n",
      "LOSS train 0.0005640436429530382 valid 0.0008070921758189797\n",
      "LOSS train 0.0005640436429530382 valid 0.0008206248749047518\n",
      "LOSS train 0.0005640436429530382 valid 0.0008752718567848206\n",
      "LOSS train 0.0005640436429530382 valid 0.0009081084281206131\n",
      "LOSS train 0.0005640436429530382 valid 0.0009063281468115747\n",
      "LOSS train 0.0005640436429530382 valid 0.0009109944803640246\n",
      "LOSS train 0.0005640436429530382 valid 0.0009137953747995198\n",
      "LOSS train 0.0005640436429530382 valid 0.0009216928738169372\n",
      "LOSS train 0.0005640436429530382 valid 0.0009156137239187956\n",
      "LOSS train 0.0005640436429530382 valid 0.0009251435985788703\n",
      "LOSS train 0.0005640436429530382 valid 0.0009326717699877918\n",
      "LOSS train 0.0005640436429530382 valid 0.0009358454262837768\n",
      "EPOCH 22:\n",
      "  batch 1 loss: 0.0004722950397990644\n",
      "  batch 2 loss: 0.0003892943204846233\n",
      "  batch 3 loss: 0.0004820551839657128\n",
      "  batch 4 loss: 0.00038893427699804306\n",
      "  batch 5 loss: 0.00040170643478631973\n",
      "  batch 6 loss: 0.00042019132524728775\n",
      "  batch 7 loss: 0.0003359984839335084\n",
      "  batch 8 loss: 0.00036986503982916474\n",
      "  batch 9 loss: 0.0003139374894089997\n",
      "  batch 10 loss: 0.00029235181864351034\n",
      "  batch 11 loss: 0.0005922684795223176\n",
      "  batch 12 loss: 0.0005366984405554831\n",
      "  batch 13 loss: 0.00042767421109601855\n",
      "  batch 14 loss: 0.0008122864528559148\n",
      "  batch 15 loss: 0.0005324844387359917\n",
      "  batch 16 loss: 0.0004328035283833742\n",
      "  batch 17 loss: 0.0006147717940621078\n",
      "  batch 18 loss: 0.00041262945160269737\n",
      "  batch 19 loss: 0.0008918932289816439\n",
      "  batch 20 loss: 0.0007408685050904751\n",
      "  batch 21 loss: 0.0007809767266735435\n",
      "  batch 22 loss: 0.0006908439681865275\n",
      "  batch 23 loss: 0.0007299934513866901\n",
      "  batch 24 loss: 0.0006786597659811378\n",
      "  batch 25 loss: 0.0008005188428796828\n",
      "  batch 26 loss: 0.0005491090705618262\n",
      "  batch 27 loss: 0.00038652285002171993\n",
      "  batch 28 loss: 0.0005170668591745198\n",
      "  batch 29 loss: 0.0003286088758613914\n",
      "  batch 30 loss: 0.0003853779344353825\n",
      "  batch 31 loss: 0.0003604174416977912\n",
      "  batch 32 loss: 0.00041613311623223126\n",
      "  batch 33 loss: 0.0004639018443413079\n",
      "  batch 34 loss: 0.00043865281622856855\n",
      "  batch 35 loss: 0.0003752576594706625\n",
      "  batch 36 loss: 0.0003752716875169426\n",
      "  batch 37 loss: 0.0003245755797252059\n",
      "  batch 38 loss: 0.00033582409378141165\n",
      "  batch 39 loss: 0.00038820080226287246\n",
      "  batch 40 loss: 0.00046204208047129214\n",
      "  batch 41 loss: 0.0003587604151107371\n",
      "  batch 42 loss: 0.0004239454574417323\n",
      "  batch 43 loss: 0.00033795315539464355\n",
      "  batch 44 loss: 0.0002557947300374508\n",
      "  batch 45 loss: 0.00030513372621499\n",
      "  batch 46 loss: 0.00036559312138706446\n",
      "  batch 47 loss: 0.0003508450463414192\n",
      "  batch 48 loss: 0.00026103033451363444\n",
      "  batch 49 loss: 0.0003131194971501827\n",
      "  batch 50 loss: 0.0003208451089449227\n",
      "  batch 51 loss: 0.00039293189183808863\n",
      "  batch 52 loss: 0.00029483294929377735\n",
      "  batch 53 loss: 0.00030940008582547307\n",
      "  batch 54 loss: 0.0003360731643624604\n",
      "  batch 55 loss: 0.0003454273391980678\n",
      "  batch 56 loss: 0.00030026270542293787\n",
      "  batch 57 loss: 0.00024321464297827333\n",
      "  batch 58 loss: 0.00043723132694140077\n",
      "  batch 59 loss: 0.0007004424696788192\n",
      "  batch 60 loss: 0.0004606067086569965\n",
      "  batch 61 loss: 0.00034884049091488123\n",
      "  batch 62 loss: 0.0003398156550247222\n",
      "  batch 63 loss: 0.00023155366943683475\n",
      "  batch 64 loss: 0.0002379659708822146\n",
      "  batch 65 loss: 0.0002753864973783493\n",
      "  batch 66 loss: 0.0002591307566035539\n",
      "  batch 67 loss: 0.0002355078177060932\n",
      "  batch 68 loss: 0.0003145576920360327\n",
      "  batch 69 loss: 0.0002912039344664663\n",
      "  batch 70 loss: 0.0003702922258526087\n",
      "  batch 71 loss: 0.00027463791775517166\n",
      "  batch 72 loss: 0.0002613378455862403\n",
      "  batch 73 loss: 0.0002636971476022154\n",
      "  batch 74 loss: 0.0002875220379792154\n",
      "  batch 75 loss: 0.00039898877730593085\n",
      "  batch 76 loss: 0.0002950722409877926\n",
      "  batch 77 loss: 0.00027953239623457193\n",
      "  batch 78 loss: 0.0002797689812723547\n",
      "  batch 79 loss: 0.00028523511718958616\n",
      "  batch 80 loss: 0.0003165917587466538\n",
      "  batch 81 loss: 0.0003076770808547735\n",
      "  batch 82 loss: 0.0003301773685961962\n",
      "  batch 83 loss: 0.00034750488703139126\n",
      "  batch 84 loss: 0.00027076000696979463\n",
      "  batch 85 loss: 0.00028552644653245807\n",
      "  batch 86 loss: 0.00032672658562660217\n",
      "  batch 87 loss: 0.0003513424890115857\n",
      "  batch 88 loss: 0.0005957997054792941\n",
      "  batch 89 loss: 0.00043853322858922184\n",
      "  batch 90 loss: 0.0004642460262402892\n",
      "  batch 91 loss: 0.00040471850661560893\n",
      "  batch 92 loss: 0.00037612486630678177\n",
      "  batch 93 loss: 0.00035272451350465417\n",
      "  batch 94 loss: 0.0003401456924621016\n",
      "  batch 95 loss: 0.0005448653246276081\n",
      "LOSS train 0.0005448653246276081 valid 0.0007145651616156101\n",
      "LOSS train 0.0005448653246276081 valid 0.0008794751483947039\n",
      "LOSS train 0.0005448653246276081 valid 0.0008165268227458\n",
      "LOSS train 0.0005448653246276081 valid 0.0007944913231767714\n",
      "LOSS train 0.0005448653246276081 valid 0.0007748472853563726\n",
      "LOSS train 0.0005448653246276081 valid 0.0007342576282098889\n",
      "LOSS train 0.0005448653246276081 valid 0.0007648791070096195\n",
      "LOSS train 0.0005448653246276081 valid 0.0007844177889637649\n",
      "LOSS train 0.0005448653246276081 valid 0.0007631911430507898\n",
      "LOSS train 0.0005448653246276081 valid 0.0007768256473354995\n",
      "LOSS train 0.0005448653246276081 valid 0.0008090995252132416\n",
      "LOSS train 0.0005448653246276081 valid 0.0008115725358948112\n",
      "LOSS train 0.0005448653246276081 valid 0.0008034761413000524\n",
      "LOSS train 0.0005448653246276081 valid 0.0008167914929799736\n",
      "LOSS train 0.0005448653246276081 valid 0.0008722794009372592\n",
      "LOSS train 0.0005448653246276081 valid 0.0009040534496307373\n",
      "LOSS train 0.0005448653246276081 valid 0.0009031259105540812\n",
      "LOSS train 0.0005448653246276081 valid 0.0009075815905816853\n",
      "LOSS train 0.0005448653246276081 valid 0.0009100036113522947\n",
      "LOSS train 0.0005448653246276081 valid 0.0009177564643323421\n",
      "LOSS train 0.0005448653246276081 valid 0.0009127167286351323\n",
      "LOSS train 0.0005448653246276081 valid 0.0009234221070073545\n",
      "LOSS train 0.0005448653246276081 valid 0.0009301898535341024\n",
      "LOSS train 0.0005448653246276081 valid 0.0009343341807834804\n",
      "EPOCH 23:\n",
      "  batch 1 loss: 0.0004543715622276068\n",
      "  batch 2 loss: 0.00037236345815472305\n",
      "  batch 3 loss: 0.00046988273970782757\n",
      "  batch 4 loss: 0.00037738223909400403\n",
      "  batch 5 loss: 0.00039573200047016144\n",
      "  batch 6 loss: 0.000409914820920676\n",
      "  batch 7 loss: 0.0003312960034236312\n",
      "  batch 8 loss: 0.0003608312108553946\n",
      "  batch 9 loss: 0.00030767146381549537\n",
      "  batch 10 loss: 0.000296490645268932\n",
      "  batch 11 loss: 0.0005884827696718276\n",
      "  batch 12 loss: 0.0005346717080101371\n",
      "  batch 13 loss: 0.0004186939331702888\n",
      "  batch 14 loss: 0.0007926328107714653\n",
      "  batch 15 loss: 0.000529073178768158\n",
      "  batch 16 loss: 0.00040961650665849447\n",
      "  batch 17 loss: 0.000597543315961957\n",
      "  batch 18 loss: 0.0004006576200481504\n",
      "  batch 19 loss: 0.0008613539976067841\n",
      "  batch 20 loss: 0.0007174626807682216\n",
      "  batch 21 loss: 0.0007445180090144277\n",
      "  batch 22 loss: 0.000675257935654372\n",
      "  batch 23 loss: 0.0007027727551758289\n",
      "  batch 24 loss: 0.0006662947707809508\n",
      "  batch 25 loss: 0.0007797004655003548\n",
      "  batch 26 loss: 0.0005517143290489912\n",
      "  batch 27 loss: 0.00038051631418056786\n",
      "  batch 28 loss: 0.000511827296577394\n",
      "  batch 29 loss: 0.000319321290589869\n",
      "  batch 30 loss: 0.0003803381114266813\n",
      "  batch 31 loss: 0.00034863752080127597\n",
      "  batch 32 loss: 0.00040706811705604196\n",
      "  batch 33 loss: 0.0004595864156726748\n",
      "  batch 34 loss: 0.00043391319923102856\n",
      "  batch 35 loss: 0.00036430227919481695\n",
      "  batch 36 loss: 0.0003583824436645955\n",
      "  batch 37 loss: 0.00031726632732897997\n",
      "  batch 38 loss: 0.0003317556402180344\n",
      "  batch 39 loss: 0.00036679720506072044\n",
      "  batch 40 loss: 0.00044349831296131015\n",
      "  batch 41 loss: 0.00035285353078506887\n",
      "  batch 42 loss: 0.00042401172686368227\n",
      "  batch 43 loss: 0.0003258345532231033\n",
      "  batch 44 loss: 0.00024952663807198405\n",
      "  batch 45 loss: 0.0003021117008756846\n",
      "  batch 46 loss: 0.00033869879553094506\n",
      "  batch 47 loss: 0.0003388756886124611\n",
      "  batch 48 loss: 0.00025725323939695954\n",
      "  batch 49 loss: 0.0003085029311478138\n",
      "  batch 50 loss: 0.00031932187266647816\n",
      "  batch 51 loss: 0.00038824198418296874\n",
      "  batch 52 loss: 0.0002969799970742315\n",
      "  batch 53 loss: 0.00030931230867281556\n",
      "  batch 54 loss: 0.00033230692497454584\n",
      "  batch 55 loss: 0.0003472835524007678\n",
      "  batch 56 loss: 0.0003034257679246366\n",
      "  batch 57 loss: 0.00023709220113232732\n",
      "  batch 58 loss: 0.00043076969450339675\n",
      "  batch 59 loss: 0.0007073479937389493\n",
      "  batch 60 loss: 0.00045696456800214946\n",
      "  batch 61 loss: 0.00033722101943567395\n",
      "  batch 62 loss: 0.00032643781742081046\n",
      "  batch 63 loss: 0.00022944985539652407\n",
      "  batch 64 loss: 0.00023461018281523138\n",
      "  batch 65 loss: 0.00027013616636395454\n",
      "  batch 66 loss: 0.00025499449111521244\n",
      "  batch 67 loss: 0.000245255243498832\n",
      "  batch 68 loss: 0.0003070257371291518\n",
      "  batch 69 loss: 0.0002848343283403665\n",
      "  batch 70 loss: 0.0003675194748211652\n",
      "  batch 71 loss: 0.0002652268158271909\n",
      "  batch 72 loss: 0.0002608508220873773\n",
      "  batch 73 loss: 0.00026297132717445493\n",
      "  batch 74 loss: 0.00028645971906371415\n",
      "  batch 75 loss: 0.0003990276309195906\n",
      "  batch 76 loss: 0.00030363607220351696\n",
      "  batch 77 loss: 0.00027948059141635895\n",
      "  batch 78 loss: 0.00027680626953952014\n",
      "  batch 79 loss: 0.0002851984463632107\n",
      "  batch 80 loss: 0.00032045264379121363\n",
      "  batch 81 loss: 0.0003078925656154752\n",
      "  batch 82 loss: 0.0003335429937578738\n",
      "  batch 83 loss: 0.0003528097877278924\n",
      "  batch 84 loss: 0.00026955193607136607\n",
      "  batch 85 loss: 0.0002845192211680114\n",
      "  batch 86 loss: 0.00032835433376021683\n",
      "  batch 87 loss: 0.0003504767082631588\n",
      "  batch 88 loss: 0.0006235168548300862\n",
      "  batch 89 loss: 0.0004385487409308553\n",
      "  batch 90 loss: 0.0004641717823687941\n",
      "  batch 91 loss: 0.0004163738922215998\n",
      "  batch 92 loss: 0.0003607949474826455\n",
      "  batch 93 loss: 0.0003533497219905257\n",
      "  batch 94 loss: 0.00034560292260721326\n",
      "  batch 95 loss: 0.0005548225017264485\n",
      "LOSS train 0.0005548225017264485 valid 0.0007264642044901848\n",
      "LOSS train 0.0005548225017264485 valid 0.000902861705981195\n",
      "LOSS train 0.0005548225017264485 valid 0.0008235551649704576\n",
      "LOSS train 0.0005548225017264485 valid 0.0008052845951169729\n",
      "LOSS train 0.0005548225017264485 valid 0.0007889365078881383\n",
      "LOSS train 0.0005548225017264485 valid 0.0007458089385181665\n",
      "LOSS train 0.0005548225017264485 valid 0.0007828425150364637\n",
      "LOSS train 0.0005548225017264485 valid 0.000803366128820926\n",
      "LOSS train 0.0005548225017264485 valid 0.0007777520222589374\n",
      "LOSS train 0.0005548225017264485 valid 0.0007967736455611885\n",
      "LOSS train 0.0005548225017264485 valid 0.0008386015542782843\n",
      "LOSS train 0.0005548225017264485 valid 0.0008444434497505426\n",
      "LOSS train 0.0005548225017264485 valid 0.0008341828943230212\n",
      "LOSS train 0.0005548225017264485 valid 0.0008481641416437924\n",
      "LOSS train 0.0005548225017264485 valid 0.0009048338397406042\n",
      "LOSS train 0.0005548225017264485 valid 0.0009367194725200534\n",
      "LOSS train 0.0005548225017264485 valid 0.0009369957842864096\n",
      "LOSS train 0.0005548225017264485 valid 0.0009413785883225501\n",
      "LOSS train 0.0005548225017264485 valid 0.0009442406008020043\n",
      "LOSS train 0.0005548225017264485 valid 0.0009549211827106774\n",
      "LOSS train 0.0005548225017264485 valid 0.0009508056100457907\n",
      "LOSS train 0.0005548225017264485 valid 0.0009635410970076919\n",
      "LOSS train 0.0005548225017264485 valid 0.0009724984411150217\n",
      "LOSS train 0.0005548225017264485 valid 0.0009775805519893765\n",
      "EPOCH 24:\n",
      "  batch 1 loss: 0.0004522922681644559\n",
      "  batch 2 loss: 0.00037046053330413997\n",
      "  batch 3 loss: 0.00047329734661616385\n",
      "  batch 4 loss: 0.0003776009543798864\n",
      "  batch 5 loss: 0.00038905837573111057\n",
      "  batch 6 loss: 0.0004105492844246328\n",
      "  batch 7 loss: 0.0003285317216068506\n",
      "  batch 8 loss: 0.000361437676474452\n",
      "  batch 9 loss: 0.0002965704770758748\n",
      "  batch 10 loss: 0.00029827497201040387\n",
      "  batch 11 loss: 0.0005796172190457582\n",
      "  batch 12 loss: 0.0005228102672845125\n",
      "  batch 13 loss: 0.0004225530428811908\n",
      "  batch 14 loss: 0.0008000584202818573\n",
      "  batch 15 loss: 0.0005144904716871679\n",
      "  batch 16 loss: 0.00040189377614296973\n",
      "  batch 17 loss: 0.0005943815922364593\n",
      "  batch 18 loss: 0.00038567493902519345\n",
      "  batch 19 loss: 0.0008445590501651168\n",
      "  batch 20 loss: 0.0007214805809780955\n",
      "  batch 21 loss: 0.000739441136829555\n",
      "  batch 22 loss: 0.00067543750628829\n",
      "  batch 23 loss: 0.0006899720174260437\n",
      "  batch 24 loss: 0.0006536415894515812\n",
      "  batch 25 loss: 0.0007924531237222254\n",
      "  batch 26 loss: 0.0005504584987647831\n",
      "  batch 27 loss: 0.00038072012830525637\n",
      "  batch 28 loss: 0.0005225412314757705\n",
      "  batch 29 loss: 0.00031790940556675196\n",
      "  batch 30 loss: 0.0003833526570815593\n",
      "  batch 31 loss: 0.0003465960326138884\n",
      "  batch 32 loss: 0.0004034339217469096\n",
      "  batch 33 loss: 0.0004595773061737418\n",
      "  batch 34 loss: 0.0004318045102991164\n",
      "  batch 35 loss: 0.0003659577341750264\n",
      "  batch 36 loss: 0.0003591869317460805\n",
      "  batch 37 loss: 0.0003109626122750342\n",
      "  batch 38 loss: 0.0003363459836691618\n",
      "  batch 39 loss: 0.0003641158400569111\n",
      "  batch 40 loss: 0.00043842181912623346\n",
      "  batch 41 loss: 0.0003447249182499945\n",
      "  batch 42 loss: 0.00042066595051437616\n",
      "  batch 43 loss: 0.0003184079541824758\n",
      "  batch 44 loss: 0.00024806795408949256\n",
      "  batch 45 loss: 0.00030542563763447106\n",
      "  batch 46 loss: 0.00034597632475197315\n",
      "  batch 47 loss: 0.00033086477196775377\n",
      "  batch 48 loss: 0.00025260241818614304\n",
      "  batch 49 loss: 0.0003043767355848104\n",
      "  batch 50 loss: 0.00031594891333952546\n",
      "  batch 51 loss: 0.0003894423134624958\n",
      "  batch 52 loss: 0.0002934649237431586\n",
      "  batch 53 loss: 0.00030633321148343384\n",
      "  batch 54 loss: 0.000327702728100121\n",
      "  batch 55 loss: 0.0003404555900488049\n",
      "  batch 56 loss: 0.0002956262032967061\n",
      "  batch 57 loss: 0.00024096235574688762\n",
      "  batch 58 loss: 0.0004206238081678748\n",
      "  batch 59 loss: 0.0006749412859790027\n",
      "  batch 60 loss: 0.0004582837864290923\n",
      "  batch 61 loss: 0.0003361869021318853\n",
      "  batch 62 loss: 0.00032978097442537546\n",
      "  batch 63 loss: 0.00023078714730218053\n",
      "  batch 64 loss: 0.00023482136020902544\n",
      "  batch 65 loss: 0.000270624557742849\n",
      "  batch 66 loss: 0.0002578988205641508\n",
      "  batch 67 loss: 0.00024105666670948267\n",
      "  batch 68 loss: 0.0003203100641258061\n",
      "  batch 69 loss: 0.0002906525041908026\n",
      "  batch 70 loss: 0.0003652612504083663\n",
      "  batch 71 loss: 0.00026216256082989275\n",
      "  batch 72 loss: 0.00025630969321355224\n",
      "  batch 73 loss: 0.0002580750733613968\n",
      "  batch 74 loss: 0.00028688032762147486\n",
      "  batch 75 loss: 0.000396200135583058\n",
      "  batch 76 loss: 0.00028832355746999383\n",
      "  batch 77 loss: 0.0002764618839137256\n",
      "  batch 78 loss: 0.0002748886472545564\n",
      "  batch 79 loss: 0.00027832662453874946\n",
      "  batch 80 loss: 0.00030337763018906116\n",
      "  batch 81 loss: 0.00030432542553171515\n",
      "  batch 82 loss: 0.0003267123829573393\n",
      "  batch 83 loss: 0.00034326882450841367\n",
      "  batch 84 loss: 0.0002632347750477493\n",
      "  batch 85 loss: 0.00027712262817658484\n",
      "  batch 86 loss: 0.0003285264247097075\n",
      "  batch 87 loss: 0.0003465469926595688\n",
      "  batch 88 loss: 0.0005804951651953161\n",
      "  batch 89 loss: 0.0004282234003767371\n",
      "  batch 90 loss: 0.000463374046375975\n",
      "  batch 91 loss: 0.00039234099676832557\n",
      "  batch 92 loss: 0.000365604180842638\n",
      "  batch 93 loss: 0.0003446997725404799\n",
      "  batch 94 loss: 0.0003316476650070399\n",
      "  batch 95 loss: 0.0005336182075552642\n",
      "LOSS train 0.0005336182075552642 valid 0.0007150736055336893\n",
      "LOSS train 0.0005336182075552642 valid 0.0008716689189895988\n",
      "LOSS train 0.0005336182075552642 valid 0.0008102841093204916\n",
      "LOSS train 0.0005336182075552642 valid 0.000788453733548522\n",
      "LOSS train 0.0005336182075552642 valid 0.0007703474839217961\n",
      "LOSS train 0.0005336182075552642 valid 0.0007293177768588066\n",
      "LOSS train 0.0005336182075552642 valid 0.0007593128248117864\n",
      "LOSS train 0.0005336182075552642 valid 0.0007795148994773626\n",
      "LOSS train 0.0005336182075552642 valid 0.0007579177035950124\n",
      "LOSS train 0.0005336182075552642 valid 0.0007715534302406013\n",
      "LOSS train 0.0005336182075552642 valid 0.0008038377272896469\n",
      "LOSS train 0.0005336182075552642 valid 0.0008073392091318965\n",
      "LOSS train 0.0005336182075552642 valid 0.0007992701139301062\n",
      "LOSS train 0.0005336182075552642 valid 0.0008119078702293336\n",
      "LOSS train 0.0005336182075552642 valid 0.0008678935118950903\n",
      "LOSS train 0.0005336182075552642 valid 0.0008993754163384438\n",
      "LOSS train 0.0005336182075552642 valid 0.0008983881562016904\n",
      "LOSS train 0.0005336182075552642 valid 0.0009012550581246614\n",
      "LOSS train 0.0005336182075552642 valid 0.0009013937669806182\n",
      "LOSS train 0.0005336182075552642 valid 0.000908084970433265\n",
      "LOSS train 0.0005336182075552642 valid 0.0009023177553899586\n",
      "LOSS train 0.0005336182075552642 valid 0.0009122883202508092\n",
      "LOSS train 0.0005336182075552642 valid 0.0009172642603516579\n",
      "LOSS train 0.0005336182075552642 valid 0.0009199115447700024\n",
      "EPOCH 25:\n",
      "  batch 1 loss: 0.0004420395416673273\n",
      "  batch 2 loss: 0.00034590906579978764\n",
      "  batch 3 loss: 0.0004429795953910798\n",
      "  batch 4 loss: 0.00036245054798200727\n",
      "  batch 5 loss: 0.0003871360095217824\n",
      "  batch 6 loss: 0.0004172875196672976\n",
      "  batch 7 loss: 0.00031979032792150974\n",
      "  batch 8 loss: 0.00035181379644200206\n",
      "  batch 9 loss: 0.0002948049805127084\n",
      "  batch 10 loss: 0.00029560772236436605\n",
      "  batch 11 loss: 0.0005674834828823805\n",
      "  batch 12 loss: 0.0005092194769531488\n",
      "  batch 13 loss: 0.0004194574721623212\n",
      "  batch 14 loss: 0.0007777904975228012\n",
      "  batch 15 loss: 0.0004928588168695569\n",
      "  batch 16 loss: 0.00040129368426278234\n",
      "  batch 17 loss: 0.0005956899840384722\n",
      "  batch 18 loss: 0.0003742256958503276\n",
      "  batch 19 loss: 0.0008618929423391819\n",
      "  batch 20 loss: 0.0007133737672120333\n",
      "  batch 21 loss: 0.0007250041817314923\n",
      "  batch 22 loss: 0.0006662638625130057\n",
      "  batch 23 loss: 0.0007114945910871029\n",
      "  batch 24 loss: 0.0006498844595625997\n",
      "  batch 25 loss: 0.0007732646772637963\n",
      "  batch 26 loss: 0.0005455415230244398\n",
      "  batch 27 loss: 0.00037967239040881395\n",
      "  batch 28 loss: 0.0005176319973543286\n",
      "  batch 29 loss: 0.00031200615921989083\n",
      "  batch 30 loss: 0.0003750785836018622\n",
      "  batch 31 loss: 0.0003431696677580476\n",
      "  batch 32 loss: 0.0003981107147410512\n",
      "  batch 33 loss: 0.0004614085191860795\n",
      "  batch 34 loss: 0.0004330861265771091\n",
      "  batch 35 loss: 0.00036029922193847597\n",
      "  batch 36 loss: 0.0003532804548740387\n",
      "  batch 37 loss: 0.00031442410545423627\n",
      "  batch 38 loss: 0.0003285687998868525\n",
      "  batch 39 loss: 0.0003593075380194932\n",
      "  batch 40 loss: 0.00042972719529643655\n",
      "  batch 41 loss: 0.00034395878901705146\n",
      "  batch 42 loss: 0.0004245965101290494\n",
      "  batch 43 loss: 0.00031956867314875126\n",
      "  batch 44 loss: 0.0002381726517342031\n",
      "  batch 45 loss: 0.00030102109303697944\n",
      "  batch 46 loss: 0.00033475959207862616\n",
      "  batch 47 loss: 0.00032486539566889405\n",
      "  batch 48 loss: 0.0002460263785906136\n",
      "  batch 49 loss: 0.00030217948369681835\n",
      "  batch 50 loss: 0.0003155622398480773\n",
      "  batch 51 loss: 0.00038185459561645985\n",
      "  batch 52 loss: 0.00028904678765684366\n",
      "  batch 53 loss: 0.00030526204500347376\n",
      "  batch 54 loss: 0.00032726192148402333\n",
      "  batch 55 loss: 0.0003528365050442517\n",
      "  batch 56 loss: 0.00029861112125217915\n",
      "  batch 57 loss: 0.00023426776169799268\n",
      "  batch 58 loss: 0.0004266418982297182\n",
      "  batch 59 loss: 0.0006710399175062776\n",
      "  batch 60 loss: 0.00044992612674832344\n",
      "  batch 61 loss: 0.00032210073550231755\n",
      "  batch 62 loss: 0.00032943845144473016\n",
      "  batch 63 loss: 0.00022863264894112945\n",
      "  batch 64 loss: 0.00023187324404716492\n",
      "  batch 65 loss: 0.0002603571629151702\n",
      "  batch 66 loss: 0.0002614085387904197\n",
      "  batch 67 loss: 0.00023547938326373696\n",
      "  batch 68 loss: 0.0003075571730732918\n",
      "  batch 69 loss: 0.00028755853418260813\n",
      "  batch 70 loss: 0.00035671493969857693\n",
      "  batch 71 loss: 0.00025720271514728665\n",
      "  batch 72 loss: 0.00025306263705715537\n",
      "  batch 73 loss: 0.0002532945654820651\n",
      "  batch 74 loss: 0.0002804317628033459\n",
      "  batch 75 loss: 0.00038971781032159925\n",
      "  batch 76 loss: 0.00028353300876915455\n",
      "  batch 77 loss: 0.0002725679660215974\n",
      "  batch 78 loss: 0.00027523472090251744\n",
      "  batch 79 loss: 0.0002830008161254227\n",
      "  batch 80 loss: 0.0003079650632571429\n",
      "  batch 81 loss: 0.00030766846612095833\n",
      "  batch 82 loss: 0.00032414146699011326\n",
      "  batch 83 loss: 0.00034073618007823825\n",
      "  batch 84 loss: 0.00026151203201152384\n",
      "  batch 85 loss: 0.00027621095068752766\n",
      "  batch 86 loss: 0.00031556509202346206\n",
      "  batch 87 loss: 0.00033826963044703007\n",
      "  batch 88 loss: 0.0005714739672839642\n",
      "  batch 89 loss: 0.00042852890328504145\n",
      "  batch 90 loss: 0.0004580382665153593\n",
      "  batch 91 loss: 0.00039410925819538534\n",
      "  batch 92 loss: 0.00035526533611118793\n",
      "  batch 93 loss: 0.00033937013358809054\n",
      "  batch 94 loss: 0.00032616627868264914\n",
      "  batch 95 loss: 0.0005252271075733006\n",
      "LOSS train 0.0005252271075733006 valid 0.0007466942770406604\n",
      "LOSS train 0.0005252271075733006 valid 0.0008941613486967981\n",
      "LOSS train 0.0005252271075733006 valid 0.0008256789296865463\n",
      "LOSS train 0.0005252271075733006 valid 0.0007967322017066181\n",
      "LOSS train 0.0005252271075733006 valid 0.0007804497145116329\n",
      "LOSS train 0.0005252271075733006 valid 0.0007406172808259726\n",
      "LOSS train 0.0005252271075733006 valid 0.0007741985027678311\n",
      "LOSS train 0.0005252271075733006 valid 0.0007953059393912554\n",
      "LOSS train 0.0005252271075733006 valid 0.0007714772946201265\n",
      "LOSS train 0.0005252271075733006 valid 0.0007858120952732861\n",
      "LOSS train 0.0005252271075733006 valid 0.0008205463527701795\n",
      "LOSS train 0.0005252271075733006 valid 0.0008238457376137376\n",
      "LOSS train 0.0005252271075733006 valid 0.0008134609670378268\n",
      "LOSS train 0.0005252271075733006 valid 0.0008266213117167354\n",
      "LOSS train 0.0005252271075733006 valid 0.0008864396950230002\n",
      "LOSS train 0.0005252271075733006 valid 0.0009172284044325352\n",
      "LOSS train 0.0005252271075733006 valid 0.0009168211836367846\n",
      "LOSS train 0.0005252271075733006 valid 0.0009210359421558678\n",
      "LOSS train 0.0005252271075733006 valid 0.0009207278490066528\n",
      "LOSS train 0.0005252271075733006 valid 0.0009305833955295384\n",
      "LOSS train 0.0005252271075733006 valid 0.0009246151894330978\n",
      "LOSS train 0.0005252271075733006 valid 0.0009338983800262213\n",
      "LOSS train 0.0005252271075733006 valid 0.00093834288418293\n",
      "LOSS train 0.0005252271075733006 valid 0.0009408624609932303\n",
      "EPOCH 26:\n",
      "  batch 1 loss: 0.0004431125125847757\n",
      "  batch 2 loss: 0.00034013899858109653\n",
      "  batch 3 loss: 0.00044805993093177676\n",
      "  batch 4 loss: 0.00036020175321027637\n",
      "  batch 5 loss: 0.0003667423443403095\n",
      "  batch 6 loss: 0.00042084127198904753\n",
      "  batch 7 loss: 0.00030464326846413314\n",
      "  batch 8 loss: 0.0003457017010077834\n",
      "  batch 9 loss: 0.0002867042494472116\n",
      "  batch 10 loss: 0.0002945999149233103\n",
      "  batch 11 loss: 0.0005723204812966287\n",
      "  batch 12 loss: 0.0005078369285911322\n",
      "  batch 13 loss: 0.00041175464866682887\n",
      "  batch 14 loss: 0.0007431530393660069\n",
      "  batch 15 loss: 0.0005092155188322067\n",
      "  batch 16 loss: 0.0004027390677947551\n",
      "  batch 17 loss: 0.0005597600247710943\n",
      "  batch 18 loss: 0.00037243199767544866\n",
      "  batch 19 loss: 0.0008231733809225261\n",
      "  batch 20 loss: 0.0006856678519397974\n",
      "  batch 21 loss: 0.0007003146456554532\n",
      "  batch 22 loss: 0.000648784392978996\n",
      "  batch 23 loss: 0.0006526232464239001\n",
      "  batch 24 loss: 0.0006256905035115778\n",
      "  batch 25 loss: 0.0007694846717640758\n",
      "  batch 26 loss: 0.0005313552683219314\n",
      "  batch 27 loss: 0.0003598293988034129\n",
      "  batch 28 loss: 0.0005164651665836573\n",
      "  batch 29 loss: 0.00032341794576495886\n",
      "  batch 30 loss: 0.00037714411155320704\n",
      "  batch 31 loss: 0.00034719324321486056\n",
      "  batch 32 loss: 0.0003942664770875126\n",
      "  batch 33 loss: 0.0004674975643865764\n",
      "  batch 34 loss: 0.0004282630980014801\n",
      "  batch 35 loss: 0.00036696193274110556\n",
      "  batch 36 loss: 0.00034284949651919305\n",
      "  batch 37 loss: 0.000302004482364282\n",
      "  batch 38 loss: 0.000321195024298504\n",
      "  batch 39 loss: 0.0003581488854251802\n",
      "  batch 40 loss: 0.00042830509482882917\n",
      "  batch 41 loss: 0.00035467330599203706\n",
      "  batch 42 loss: 0.00043704459676519036\n",
      "  batch 43 loss: 0.0003196106990799308\n",
      "  batch 44 loss: 0.00023775224690325558\n",
      "  batch 45 loss: 0.00029503711266443133\n",
      "  batch 46 loss: 0.00033101640292443335\n",
      "  batch 47 loss: 0.0003176604222971946\n",
      "  batch 48 loss: 0.0002512642531655729\n",
      "  batch 49 loss: 0.0003035725385416299\n",
      "  batch 50 loss: 0.0003144104266539216\n",
      "  batch 51 loss: 0.00038141116965562105\n",
      "  batch 52 loss: 0.00028227228904142976\n",
      "  batch 53 loss: 0.00030729168793186545\n",
      "  batch 54 loss: 0.0003281089011579752\n",
      "  batch 55 loss: 0.000345387845300138\n",
      "  batch 56 loss: 0.00028939632466062903\n",
      "  batch 57 loss: 0.00023729669919703156\n",
      "  batch 58 loss: 0.0004175308276899159\n",
      "  batch 59 loss: 0.0006709076697006822\n",
      "  batch 60 loss: 0.00044601806439459324\n",
      "  batch 61 loss: 0.0003254193579778075\n",
      "  batch 62 loss: 0.0003254550974816084\n",
      "  batch 63 loss: 0.00022237212397158146\n",
      "  batch 64 loss: 0.00023369475093204528\n",
      "  batch 65 loss: 0.00027245975797995925\n",
      "  batch 66 loss: 0.00025419925805181265\n",
      "  batch 67 loss: 0.0002244071802124381\n",
      "  batch 68 loss: 0.0002855990023817867\n",
      "  batch 69 loss: 0.0002823921968229115\n",
      "  batch 70 loss: 0.00035750301321968436\n",
      "  batch 71 loss: 0.0002554402453824878\n",
      "  batch 72 loss: 0.0002525717136450112\n",
      "  batch 73 loss: 0.0002563095185905695\n",
      "  batch 74 loss: 0.0002848700387403369\n",
      "  batch 75 loss: 0.0003840383724309504\n",
      "  batch 76 loss: 0.00028457387816160917\n",
      "  batch 77 loss: 0.00027264488744549453\n",
      "  batch 78 loss: 0.0002648443914949894\n",
      "  batch 79 loss: 0.00027364050038158894\n",
      "  batch 80 loss: 0.0002984029706567526\n",
      "  batch 81 loss: 0.0002979781711474061\n",
      "  batch 82 loss: 0.0003178415645379573\n",
      "  batch 83 loss: 0.000338382669724524\n",
      "  batch 84 loss: 0.00025585075491108\n",
      "  batch 85 loss: 0.00027121580205857754\n",
      "  batch 86 loss: 0.0003155627055093646\n",
      "  batch 87 loss: 0.0003354443470016122\n",
      "  batch 88 loss: 0.0005519457627087831\n",
      "  batch 89 loss: 0.0004250004712957889\n",
      "  batch 90 loss: 0.00045537238474935293\n",
      "  batch 91 loss: 0.00038975357892923057\n",
      "  batch 92 loss: 0.00034236980718560517\n",
      "  batch 93 loss: 0.0003393874503672123\n",
      "  batch 94 loss: 0.00031884509371593595\n",
      "  batch 95 loss: 0.0005107782781124115\n",
      "LOSS train 0.0005107782781124115 valid 0.0007537677884101868\n",
      "LOSS train 0.0005107782781124115 valid 0.0008956210222095251\n",
      "LOSS train 0.0005107782781124115 valid 0.0008169397478923202\n",
      "LOSS train 0.0005107782781124115 valid 0.0007868403336033225\n",
      "LOSS train 0.0005107782781124115 valid 0.0007761679589748383\n",
      "LOSS train 0.0005107782781124115 valid 0.0007367521757259965\n",
      "LOSS train 0.0005107782781124115 valid 0.0007731652585789561\n",
      "LOSS train 0.0005107782781124115 valid 0.0007979058427736163\n",
      "LOSS train 0.0005107782781124115 valid 0.0007726511103101075\n",
      "LOSS train 0.0005107782781124115 valid 0.000788821023888886\n",
      "LOSS train 0.0005107782781124115 valid 0.0008269355748780072\n",
      "LOSS train 0.0005107782781124115 valid 0.0008320763590745628\n",
      "LOSS train 0.0005107782781124115 valid 0.0008206813363358378\n",
      "LOSS train 0.0005107782781124115 valid 0.0008356005419045687\n",
      "LOSS train 0.0005107782781124115 valid 0.000896962417755276\n",
      "LOSS train 0.0005107782781124115 valid 0.0009288067230954766\n",
      "LOSS train 0.0005107782781124115 valid 0.0009273618343286216\n",
      "LOSS train 0.0005107782781124115 valid 0.0009328555315732956\n",
      "LOSS train 0.0005107782781124115 valid 0.0009332651970908046\n",
      "LOSS train 0.0005107782781124115 valid 0.0009437071275897324\n",
      "LOSS train 0.0005107782781124115 valid 0.0009376968955621123\n",
      "LOSS train 0.0005107782781124115 valid 0.0009472136152908206\n",
      "LOSS train 0.0005107782781124115 valid 0.0009519885643385351\n",
      "LOSS train 0.0005107782781124115 valid 0.0009540330502204597\n",
      "EPOCH 27:\n",
      "  batch 1 loss: 0.0004315317783039063\n",
      "  batch 2 loss: 0.00032457575434818864\n",
      "  batch 3 loss: 0.0004227528115734458\n",
      "  batch 4 loss: 0.0003503703628666699\n",
      "  batch 5 loss: 0.0003606579848565161\n",
      "  batch 6 loss: 0.0004079859936609864\n",
      "  batch 7 loss: 0.0003023521276190877\n",
      "  batch 8 loss: 0.00033621274633333087\n",
      "  batch 9 loss: 0.00028262162231840193\n",
      "  batch 10 loss: 0.0002850143937394023\n",
      "  batch 11 loss: 0.0005474539939314127\n",
      "  batch 12 loss: 0.0004828004166483879\n",
      "  batch 13 loss: 0.0004052871954627335\n",
      "  batch 14 loss: 0.0007342444732785225\n",
      "  batch 15 loss: 0.00048555067041888833\n",
      "  batch 16 loss: 0.0003943583578802645\n",
      "  batch 17 loss: 0.0005417010979726911\n",
      "  batch 18 loss: 0.0003843853482976556\n",
      "  batch 19 loss: 0.0007998347282409668\n",
      "  batch 20 loss: 0.0006655449979007244\n",
      "  batch 21 loss: 0.0006882811430841684\n",
      "  batch 22 loss: 0.0006255190819501877\n",
      "  batch 23 loss: 0.0006461087032221258\n",
      "  batch 24 loss: 0.0006152993883006275\n",
      "  batch 25 loss: 0.0007674405351281166\n",
      "  batch 26 loss: 0.0005326292011886835\n",
      "  batch 27 loss: 0.0003497873549349606\n",
      "  batch 28 loss: 0.0004977487842552364\n",
      "  batch 29 loss: 0.0003102837363258004\n",
      "  batch 30 loss: 0.0003624025557655841\n",
      "  batch 31 loss: 0.0003441074804868549\n",
      "  batch 32 loss: 0.00039224146166816354\n",
      "  batch 33 loss: 0.00046705000568181276\n",
      "  batch 34 loss: 0.0004256049869582057\n",
      "  batch 35 loss: 0.0003521183389239013\n",
      "  batch 36 loss: 0.00033811532193794847\n",
      "  batch 37 loss: 0.000300624524243176\n",
      "  batch 38 loss: 0.0003268136933911592\n",
      "  batch 39 loss: 0.0003626010147854686\n",
      "  batch 40 loss: 0.00042262033093720675\n",
      "  batch 41 loss: 0.00034996436443179846\n",
      "  batch 42 loss: 0.0004201715928502381\n",
      "  batch 43 loss: 0.0003048393700737506\n",
      "  batch 44 loss: 0.00022564205573871732\n",
      "  batch 45 loss: 0.00028743280563503504\n",
      "  batch 46 loss: 0.00032789522083476186\n",
      "  batch 47 loss: 0.0003425102331675589\n",
      "  batch 48 loss: 0.00025712838396430016\n",
      "  batch 49 loss: 0.00030863028950989246\n",
      "  batch 50 loss: 0.0003129201359115541\n",
      "  batch 51 loss: 0.0003701437672134489\n",
      "  batch 52 loss: 0.00027302332455292344\n",
      "  batch 53 loss: 0.0002948890905827284\n",
      "  batch 54 loss: 0.0003311290347483009\n",
      "  batch 55 loss: 0.0003396247630007565\n",
      "  batch 56 loss: 0.0002841349341906607\n",
      "  batch 57 loss: 0.00023212991072796285\n",
      "  batch 58 loss: 0.0004141233512200415\n",
      "  batch 59 loss: 0.0006492140237241983\n",
      "  batch 60 loss: 0.0004309748765081167\n",
      "  batch 61 loss: 0.0003143655485473573\n",
      "  batch 62 loss: 0.0003308776649646461\n",
      "  batch 63 loss: 0.00022736834944225848\n",
      "  batch 64 loss: 0.00022549917048308998\n",
      "  batch 65 loss: 0.000265047128777951\n",
      "  batch 66 loss: 0.00025250017642974854\n",
      "  batch 67 loss: 0.00022007568622939289\n",
      "  batch 68 loss: 0.0002805906697176397\n",
      "  batch 69 loss: 0.00027541708550415933\n",
      "  batch 70 loss: 0.0003604140074457973\n",
      "  batch 71 loss: 0.00025499670300632715\n",
      "  batch 72 loss: 0.00024179636966437101\n",
      "  batch 73 loss: 0.00024380101240240037\n",
      "  batch 74 loss: 0.0002842348185367882\n",
      "  batch 75 loss: 0.0003825827152468264\n",
      "  batch 76 loss: 0.0002824380062520504\n",
      "  batch 77 loss: 0.000272515433607623\n",
      "  batch 78 loss: 0.0002654367417562753\n",
      "  batch 79 loss: 0.00027306232368573546\n",
      "  batch 80 loss: 0.0002889938186854124\n",
      "  batch 81 loss: 0.00028968695551157\n",
      "  batch 82 loss: 0.0003114237333647907\n",
      "  batch 83 loss: 0.0003261971287429333\n",
      "  batch 84 loss: 0.00025163183454424143\n",
      "  batch 85 loss: 0.00026756085571832955\n",
      "  batch 86 loss: 0.00031359383137896657\n",
      "  batch 87 loss: 0.0003316762449685484\n",
      "  batch 88 loss: 0.0005326485843397677\n",
      "  batch 89 loss: 0.00042122413287870586\n",
      "  batch 90 loss: 0.0004437136522028595\n",
      "  batch 91 loss: 0.000392208865378052\n",
      "  batch 92 loss: 0.0003410892968531698\n",
      "  batch 93 loss: 0.00033242252538912\n",
      "  batch 94 loss: 0.00032088588341139257\n",
      "  batch 95 loss: 0.0005069305188953876\n",
      "LOSS train 0.0005069305188953876 valid 0.0007375485729426146\n",
      "LOSS train 0.0005069305188953876 valid 0.0008838993380777538\n",
      "LOSS train 0.0005069305188953876 valid 0.0008096733945421875\n",
      "LOSS train 0.0005069305188953876 valid 0.0007783706532791257\n",
      "LOSS train 0.0005069305188953876 valid 0.0007653419743292034\n",
      "LOSS train 0.0005069305188953876 valid 0.0007275472162291408\n",
      "LOSS train 0.0005069305188953876 valid 0.0007608559681102633\n",
      "LOSS train 0.0005069305188953876 valid 0.0007807216607034206\n",
      "LOSS train 0.0005069305188953876 valid 0.000755501794628799\n",
      "LOSS train 0.0005069305188953876 valid 0.0007698943954892457\n",
      "LOSS train 0.0005069305188953876 valid 0.0008099335827864707\n",
      "LOSS train 0.0005069305188953876 valid 0.0008121665450744331\n",
      "LOSS train 0.0005069305188953876 valid 0.0008016304927878082\n",
      "LOSS train 0.0005069305188953876 valid 0.000813787046354264\n",
      "LOSS train 0.0005069305188953876 valid 0.000876080768648535\n",
      "LOSS train 0.0005069305188953876 valid 0.0009082740871235728\n",
      "LOSS train 0.0005069305188953876 valid 0.0009068067884072661\n",
      "LOSS train 0.0005069305188953876 valid 0.000912760675419122\n",
      "LOSS train 0.0005069305188953876 valid 0.0009140623151324689\n",
      "LOSS train 0.0005069305188953876 valid 0.0009235874167643487\n",
      "LOSS train 0.0005069305188953876 valid 0.0009195059537887573\n",
      "LOSS train 0.0005069305188953876 valid 0.0009302071412093937\n",
      "LOSS train 0.0005069305188953876 valid 0.0009377262904308736\n",
      "LOSS train 0.0005069305188953876 valid 0.0009440082358196378\n",
      "EPOCH 28:\n",
      "  batch 1 loss: 0.0004287626361474395\n",
      "  batch 2 loss: 0.0003302781260572374\n",
      "  batch 3 loss: 0.00043122252100147307\n",
      "  batch 4 loss: 0.0003497597062960267\n",
      "  batch 5 loss: 0.00034842826426029205\n",
      "  batch 6 loss: 0.0004112403839826584\n",
      "  batch 7 loss: 0.0003043759788852185\n",
      "  batch 8 loss: 0.00033720332430675626\n",
      "  batch 9 loss: 0.0002843888942152262\n",
      "  batch 10 loss: 0.00026713378611020744\n",
      "  batch 11 loss: 0.0005397028871811926\n",
      "  batch 12 loss: 0.00048501029959879816\n",
      "  batch 13 loss: 0.0004086523549631238\n",
      "  batch 14 loss: 0.0007394059211947024\n",
      "  batch 15 loss: 0.0004911423893645406\n",
      "  batch 16 loss: 0.0004083467065356672\n",
      "  batch 17 loss: 0.0005403578979894519\n",
      "  batch 18 loss: 0.0003740175743587315\n",
      "  batch 19 loss: 0.0007987039280124009\n",
      "  batch 20 loss: 0.0006850156933069229\n",
      "  batch 21 loss: 0.0006800232222303748\n",
      "  batch 22 loss: 0.0006041800952516496\n",
      "  batch 23 loss: 0.0006433191010728478\n",
      "  batch 24 loss: 0.0006120132748037577\n",
      "  batch 25 loss: 0.0007422593189403415\n",
      "  batch 26 loss: 0.0005236560828052461\n",
      "  batch 27 loss: 0.0003381169808562845\n",
      "  batch 28 loss: 0.00047515484038740396\n",
      "  batch 29 loss: 0.00029000226641073823\n",
      "  batch 30 loss: 0.0003430555807426572\n",
      "  batch 31 loss: 0.0003200247301720083\n",
      "  batch 32 loss: 0.0003746981674339622\n",
      "  batch 33 loss: 0.00044956288184039295\n",
      "  batch 34 loss: 0.0004099717189092189\n",
      "  batch 35 loss: 0.00034262798726558685\n",
      "  batch 36 loss: 0.0003242639359086752\n",
      "  batch 37 loss: 0.00029450777219608426\n",
      "  batch 38 loss: 0.0003218714555259794\n",
      "  batch 39 loss: 0.0003527952067088336\n",
      "  batch 40 loss: 0.0004259870038367808\n",
      "  batch 41 loss: 0.000331026065396145\n",
      "  batch 42 loss: 0.0004127368447370827\n",
      "  batch 43 loss: 0.0003010325599461794\n",
      "  batch 44 loss: 0.00021060460130684078\n",
      "  batch 45 loss: 0.0002829826553352177\n",
      "  batch 46 loss: 0.00030621097539551556\n",
      "  batch 47 loss: 0.00031214056070894003\n",
      "  batch 48 loss: 0.00024858120013959706\n",
      "  batch 49 loss: 0.00030151745886541903\n",
      "  batch 50 loss: 0.0003079113957937807\n",
      "  batch 51 loss: 0.00036537315463647246\n",
      "  batch 52 loss: 0.00026660063304007053\n",
      "  batch 53 loss: 0.00029685659683309495\n",
      "  batch 54 loss: 0.0003188271075487137\n",
      "  batch 55 loss: 0.0003376590320840478\n",
      "  batch 56 loss: 0.0002805738477036357\n",
      "  batch 57 loss: 0.00022608764993492514\n",
      "  batch 58 loss: 0.00039955589454621077\n",
      "  batch 59 loss: 0.0006256054621189833\n",
      "  batch 60 loss: 0.00042575105908326805\n",
      "  batch 61 loss: 0.00029315368738025427\n",
      "  batch 62 loss: 0.00030712102307006717\n",
      "  batch 63 loss: 0.00021885099704377353\n",
      "  batch 64 loss: 0.00022037228336557746\n",
      "  batch 65 loss: 0.000254741869866848\n",
      "  batch 66 loss: 0.0002508876787032932\n",
      "  batch 67 loss: 0.00024803917040117085\n",
      "  batch 68 loss: 0.00027777496143244207\n",
      "  batch 69 loss: 0.00026701585738919675\n",
      "  batch 70 loss: 0.0003535370924510062\n",
      "  batch 71 loss: 0.00024477409897372127\n",
      "  batch 72 loss: 0.00023469593725167215\n",
      "  batch 73 loss: 0.00024200163898058236\n",
      "  batch 74 loss: 0.00027860698173753917\n",
      "  batch 75 loss: 0.00038901506923139095\n",
      "  batch 76 loss: 0.0002864787238650024\n",
      "  batch 77 loss: 0.0002688103122636676\n",
      "  batch 78 loss: 0.00026230979710817337\n",
      "  batch 79 loss: 0.0002745305246207863\n",
      "  batch 80 loss: 0.00029231520602479577\n",
      "  batch 81 loss: 0.00029404438100755215\n",
      "  batch 82 loss: 0.00031377829145640135\n",
      "  batch 83 loss: 0.00032617582473903894\n",
      "  batch 84 loss: 0.00024492511874996126\n",
      "  batch 85 loss: 0.0002656382857821882\n",
      "  batch 86 loss: 0.0003163740038871765\n",
      "  batch 87 loss: 0.00033191710826940835\n",
      "  batch 88 loss: 0.0005791785661131144\n",
      "  batch 89 loss: 0.00042313567246310413\n",
      "  batch 90 loss: 0.00044510234147310257\n",
      "  batch 91 loss: 0.0004107372951693833\n",
      "  batch 92 loss: 0.0003372838837094605\n",
      "  batch 93 loss: 0.0003316561342217028\n",
      "  batch 94 loss: 0.00032402476062998176\n",
      "  batch 95 loss: 0.0004963121609762311\n",
      "LOSS train 0.0004963121609762311 valid 0.0007381641771644354\n",
      "LOSS train 0.0004963121609762311 valid 0.0009277607314288616\n",
      "LOSS train 0.0004963121609762311 valid 0.0008369659772142768\n",
      "LOSS train 0.0004963121609762311 valid 0.0008124401792883873\n",
      "LOSS train 0.0004963121609762311 valid 0.0007943880627863109\n",
      "LOSS train 0.0004963121609762311 valid 0.0007469106931239367\n",
      "LOSS train 0.0004963121609762311 valid 0.000776127737481147\n",
      "LOSS train 0.0004963121609762311 valid 0.0007943823584355414\n",
      "LOSS train 0.0004963121609762311 valid 0.0007661885465495288\n",
      "LOSS train 0.0004963121609762311 valid 0.0007787916692905128\n",
      "LOSS train 0.0004963121609762311 valid 0.0008197849383577704\n",
      "LOSS train 0.0004963121609762311 valid 0.0008207327919080853\n",
      "LOSS train 0.0004963121609762311 valid 0.0008080156985670328\n",
      "LOSS train 0.0004963121609762311 valid 0.0008176451083272696\n",
      "LOSS train 0.0004963121609762311 valid 0.0008742380305193365\n",
      "LOSS train 0.0004963121609762311 valid 0.000906603701878339\n",
      "LOSS train 0.0004963121609762311 valid 0.0009049843647517264\n",
      "LOSS train 0.0004963121609762311 valid 0.0009091441752389073\n",
      "LOSS train 0.0004963121609762311 valid 0.0009117033332586288\n",
      "LOSS train 0.0004963121609762311 valid 0.0009224723326042295\n",
      "LOSS train 0.0004963121609762311 valid 0.0009213390876539052\n",
      "LOSS train 0.0004963121609762311 valid 0.0009349511819891632\n",
      "LOSS train 0.0004963121609762311 valid 0.0009421363938599825\n",
      "LOSS train 0.0004963121609762311 valid 0.0009516870486550033\n",
      "EPOCH 29:\n",
      "  batch 1 loss: 0.0004156326176598668\n",
      "  batch 2 loss: 0.00032909854780882597\n",
      "  batch 3 loss: 0.0004378900630399585\n",
      "  batch 4 loss: 0.00035563501296564937\n",
      "  batch 5 loss: 0.00035101635148748755\n",
      "  batch 6 loss: 0.0004162946715950966\n",
      "  batch 7 loss: 0.0002906263980548829\n",
      "  batch 8 loss: 0.00033363301190547645\n",
      "  batch 9 loss: 0.000276219128863886\n",
      "  batch 10 loss: 0.00026438641361892223\n",
      "  batch 11 loss: 0.0005244988715276122\n",
      "  batch 12 loss: 0.00047977425856515765\n",
      "  batch 13 loss: 0.0003993258869741112\n",
      "  batch 14 loss: 0.0007239889237098396\n",
      "  batch 15 loss: 0.0004752894747070968\n",
      "  batch 16 loss: 0.00038508945726789534\n",
      "  batch 17 loss: 0.0005032724002376199\n",
      "  batch 18 loss: 0.0003651905572041869\n",
      "  batch 19 loss: 0.0007355049601756036\n",
      "  batch 20 loss: 0.0006493214750662446\n",
      "  batch 21 loss: 0.0006487752543762326\n",
      "  batch 22 loss: 0.0006113626295700669\n",
      "  batch 23 loss: 0.0006057454738765955\n",
      "  batch 24 loss: 0.000591304968111217\n",
      "  batch 25 loss: 0.0007111952872946858\n",
      "  batch 26 loss: 0.0005062676500529051\n",
      "  batch 27 loss: 0.00032731430837884545\n",
      "  batch 28 loss: 0.00048551271902397275\n",
      "  batch 29 loss: 0.00028360544820316136\n",
      "  batch 30 loss: 0.000320502178510651\n",
      "  batch 31 loss: 0.00030132124084047973\n",
      "  batch 32 loss: 0.00034496805164963007\n",
      "  batch 33 loss: 0.00044295843690633774\n",
      "  batch 34 loss: 0.0003999207983724773\n",
      "  batch 35 loss: 0.0003128154785372317\n",
      "  batch 36 loss: 0.00030884830630384386\n",
      "  batch 37 loss: 0.0002827128628268838\n",
      "  batch 38 loss: 0.00029940911917947233\n",
      "  batch 39 loss: 0.00033081413130275905\n",
      "  batch 40 loss: 0.0004214025684632361\n",
      "  batch 41 loss: 0.0003160347696393728\n",
      "  batch 42 loss: 0.00039934515370987356\n",
      "  batch 43 loss: 0.00028327092877589166\n",
      "  batch 44 loss: 0.00020578497787937522\n",
      "  batch 45 loss: 0.0002692085108719766\n",
      "  batch 46 loss: 0.0002970497007481754\n",
      "  batch 47 loss: 0.00030048590269871056\n",
      "  batch 48 loss: 0.00023869653523433954\n",
      "  batch 49 loss: 0.00029506575083360076\n",
      "  batch 50 loss: 0.00030316138872876763\n",
      "  batch 51 loss: 0.00036459381226450205\n",
      "  batch 52 loss: 0.0002705574734136462\n",
      "  batch 53 loss: 0.000295632635243237\n",
      "  batch 54 loss: 0.0003118002787232399\n",
      "  batch 55 loss: 0.0003444491885602474\n",
      "  batch 56 loss: 0.0002771836007013917\n",
      "  batch 57 loss: 0.00022346006880979985\n",
      "  batch 58 loss: 0.00037484115455299616\n",
      "  batch 59 loss: 0.0006056549027562141\n",
      "  batch 60 loss: 0.00042703805956989527\n",
      "  batch 61 loss: 0.0002767624973785132\n",
      "  batch 62 loss: 0.0002988850756082684\n",
      "  batch 63 loss: 0.00021803815616294742\n",
      "  batch 64 loss: 0.0002155563561245799\n",
      "  batch 65 loss: 0.00024138635490089655\n",
      "  batch 66 loss: 0.0002402105601504445\n",
      "  batch 67 loss: 0.0002071638882625848\n",
      "  batch 68 loss: 0.0002648492227308452\n",
      "  batch 69 loss: 0.0002607518690638244\n",
      "  batch 70 loss: 0.0003524371422827244\n",
      "  batch 71 loss: 0.00024483868037350476\n",
      "  batch 72 loss: 0.00023394585878122598\n",
      "  batch 73 loss: 0.0002364903048146516\n",
      "  batch 74 loss: 0.00027561443857848644\n",
      "  batch 75 loss: 0.00037118938053026795\n",
      "  batch 76 loss: 0.0002593474928289652\n",
      "  batch 77 loss: 0.0002603326574899256\n",
      "  batch 78 loss: 0.0002569995995145291\n",
      "  batch 79 loss: 0.00025876302970573306\n",
      "  batch 80 loss: 0.0002828021242748946\n",
      "  batch 81 loss: 0.00028847274370491505\n",
      "  batch 82 loss: 0.00030544179026037455\n",
      "  batch 83 loss: 0.0003222749219276011\n",
      "  batch 84 loss: 0.00024335843045264482\n",
      "  batch 85 loss: 0.0002551318029873073\n",
      "  batch 86 loss: 0.0002924954751506448\n",
      "  batch 87 loss: 0.0003198670456185937\n",
      "  batch 88 loss: 0.0005016989889554679\n",
      "  batch 89 loss: 0.0004113760660402477\n",
      "  batch 90 loss: 0.0004349105875007808\n",
      "  batch 91 loss: 0.0003756347578018904\n",
      "  batch 92 loss: 0.00034097355091944337\n",
      "  batch 93 loss: 0.00031733224750496447\n",
      "  batch 94 loss: 0.0003117431188002229\n",
      "  batch 95 loss: 0.0004918092745356262\n",
      "LOSS train 0.0004918092745356262 valid 0.0007572043687105179\n",
      "LOSS train 0.0004918092745356262 valid 0.0008872742764651775\n",
      "LOSS train 0.0004918092745356262 valid 0.0008128129411488771\n",
      "LOSS train 0.0004918092745356262 valid 0.0007962558302097023\n",
      "LOSS train 0.0004918092745356262 valid 0.0007789937662892044\n",
      "LOSS train 0.0004918092745356262 valid 0.0007425674120895565\n",
      "LOSS train 0.0004918092745356262 valid 0.0007754817488603294\n",
      "LOSS train 0.0004918092745356262 valid 0.0007978695211932063\n",
      "LOSS train 0.0004918092745356262 valid 0.0007728255586698651\n",
      "LOSS train 0.0004918092745356262 valid 0.0007852836861275136\n",
      "LOSS train 0.0004918092745356262 valid 0.0008248704252764583\n",
      "LOSS train 0.0004918092745356262 valid 0.0008285049698315561\n",
      "LOSS train 0.0004918092745356262 valid 0.0008172384696081281\n",
      "LOSS train 0.0004918092745356262 valid 0.0008285028743557632\n",
      "LOSS train 0.0004918092745356262 valid 0.0008966387249529362\n",
      "LOSS train 0.0004918092745356262 valid 0.000928559573367238\n",
      "LOSS train 0.0004918092745356262 valid 0.000925398722756654\n",
      "LOSS train 0.0004918092745356262 valid 0.0009261215454898775\n",
      "LOSS train 0.0004918092745356262 valid 0.0009227875270880759\n",
      "LOSS train 0.0004918092745356262 valid 0.0009325958671979606\n",
      "LOSS train 0.0004918092745356262 valid 0.0009285951382480562\n",
      "LOSS train 0.0004918092745356262 valid 0.00093933375319466\n",
      "LOSS train 0.0004918092745356262 valid 0.000946098705753684\n",
      "LOSS train 0.0004918092745356262 valid 0.0009564195643179119\n",
      "EPOCH 30:\n",
      "  batch 1 loss: 0.0004158742376603186\n",
      "  batch 2 loss: 0.00030385798891074955\n",
      "  batch 3 loss: 0.00040014757541939616\n",
      "  batch 4 loss: 0.0003290809690952301\n",
      "  batch 5 loss: 0.0003366376331541687\n",
      "  batch 6 loss: 0.0004012675490230322\n",
      "  batch 7 loss: 0.00027612148551270366\n",
      "  batch 8 loss: 0.0003167993272654712\n",
      "  batch 9 loss: 0.00027333421166986227\n",
      "  batch 10 loss: 0.00026127410819754004\n",
      "  batch 11 loss: 0.0005240596365183592\n",
      "  batch 12 loss: 0.00045615254202857614\n",
      "  batch 13 loss: 0.0003872184897772968\n",
      "  batch 14 loss: 0.0006963299820199609\n",
      "  batch 15 loss: 0.0004537399217952043\n",
      "  batch 16 loss: 0.00037139144842512906\n",
      "  batch 17 loss: 0.0005152842495590448\n",
      "  batch 18 loss: 0.00036815673229284585\n",
      "  batch 19 loss: 0.0007662949501536787\n",
      "  batch 20 loss: 0.0006350814364850521\n",
      "  batch 21 loss: 0.000634819152764976\n",
      "  batch 22 loss: 0.0005669879028573632\n",
      "  batch 23 loss: 0.0005759351188316941\n",
      "  batch 24 loss: 0.0005794252501800656\n",
      "  batch 25 loss: 0.0007169690798036754\n",
      "  batch 26 loss: 0.0005148023483343422\n",
      "  batch 27 loss: 0.0003381873539183289\n",
      "  batch 28 loss: 0.0004782032920047641\n",
      "  batch 29 loss: 0.00028294455842114985\n",
      "  batch 30 loss: 0.0003358749090693891\n",
      "  batch 31 loss: 0.00030416593654081225\n",
      "  batch 32 loss: 0.00034785232855938375\n",
      "  batch 33 loss: 0.0004669983172789216\n",
      "  batch 34 loss: 0.00040424722828902304\n",
      "  batch 35 loss: 0.0003212156589142978\n",
      "  batch 36 loss: 0.0003016943810507655\n",
      "  batch 37 loss: 0.00028323809965513647\n",
      "  batch 38 loss: 0.0003052614629268646\n",
      "  batch 39 loss: 0.0003315104404464364\n",
      "  batch 40 loss: 0.0004195626825094223\n",
      "  batch 41 loss: 0.0003049842780455947\n",
      "  batch 42 loss: 0.0003932612016797066\n",
      "  batch 43 loss: 0.00028311117785051465\n",
      "  batch 44 loss: 0.00020253483671694994\n",
      "  batch 45 loss: 0.00026905298000201583\n",
      "  batch 46 loss: 0.00029814097797498107\n",
      "  batch 47 loss: 0.0002990399952977896\n",
      "  batch 48 loss: 0.0002387050335528329\n",
      "  batch 49 loss: 0.00029418084886856377\n",
      "  batch 50 loss: 0.00030130494269542396\n",
      "  batch 51 loss: 0.00036333349999040365\n",
      "  batch 52 loss: 0.0002709303516894579\n",
      "  batch 53 loss: 0.0002958050463348627\n",
      "  batch 54 loss: 0.00030257762409746647\n",
      "  batch 55 loss: 0.00033272855216637254\n",
      "  batch 56 loss: 0.0002649222733452916\n",
      "  batch 57 loss: 0.00022839494340587407\n",
      "  batch 58 loss: 0.00039434689097106457\n",
      "  batch 59 loss: 0.0005746597889810801\n",
      "  batch 60 loss: 0.000419141782913357\n",
      "  batch 61 loss: 0.0002846498682629317\n",
      "  batch 62 loss: 0.00029574037762358785\n",
      "  batch 63 loss: 0.00020894146291539073\n",
      "  batch 64 loss: 0.00021224358351901174\n",
      "  batch 65 loss: 0.00024402045528404415\n",
      "  batch 66 loss: 0.00024138425942510366\n",
      "  batch 67 loss: 0.00020392420992720872\n",
      "  batch 68 loss: 0.0002636316930875182\n",
      "  batch 69 loss: 0.00026016373885795474\n",
      "  batch 70 loss: 0.0003620180650614202\n",
      "  batch 71 loss: 0.00024491496151313186\n",
      "  batch 72 loss: 0.00023971877817530185\n",
      "  batch 73 loss: 0.0002301344065926969\n",
      "  batch 74 loss: 0.0002765508834272623\n",
      "  batch 75 loss: 0.00036771834129467607\n",
      "  batch 76 loss: 0.0002590795629657805\n",
      "  batch 77 loss: 0.0002575858379714191\n",
      "  batch 78 loss: 0.00025522286887280643\n",
      "  batch 79 loss: 0.0002626812201924622\n",
      "  batch 80 loss: 0.000276348611805588\n",
      "  batch 81 loss: 0.00028293218929320574\n",
      "  batch 82 loss: 0.0003078579902648926\n",
      "  batch 83 loss: 0.00031351688085123897\n",
      "  batch 84 loss: 0.00023827096447348595\n",
      "  batch 85 loss: 0.00025447725784033537\n",
      "  batch 86 loss: 0.00028948410181328654\n",
      "  batch 87 loss: 0.0003136764862574637\n",
      "  batch 88 loss: 0.0005142624140717089\n",
      "  batch 89 loss: 0.00040336305391974747\n",
      "  batch 90 loss: 0.0004322962777223438\n",
      "  batch 91 loss: 0.0003582702192943543\n",
      "  batch 92 loss: 0.00032420892966911197\n",
      "  batch 93 loss: 0.0003137725288979709\n",
      "  batch 94 loss: 0.0003078692825511098\n",
      "  batch 95 loss: 0.00046523165656253695\n",
      "LOSS train 0.00046523165656253695 valid 0.0007870650733821094\n",
      "LOSS train 0.00046523165656253695 valid 0.0009281854145228863\n",
      "LOSS train 0.00046523165656253695 valid 0.000841026077978313\n",
      "LOSS train 0.00046523165656253695 valid 0.0008170129731297493\n",
      "LOSS train 0.00046523165656253695 valid 0.0007956775953061879\n",
      "LOSS train 0.00046523165656253695 valid 0.0007559093064628541\n",
      "LOSS train 0.00046523165656253695 valid 0.0007913988665677607\n",
      "LOSS train 0.00046523165656253695 valid 0.0008102668798528612\n",
      "LOSS train 0.00046523165656253695 valid 0.0007825715583749115\n",
      "LOSS train 0.00046523165656253695 valid 0.0008000476518645883\n",
      "LOSS train 0.00046523165656253695 valid 0.0008481647819280624\n",
      "LOSS train 0.00046523165656253695 valid 0.0008503733552061021\n",
      "LOSS train 0.00046523165656253695 valid 0.0008358079358004034\n",
      "LOSS train 0.00046523165656253695 valid 0.0008512009517289698\n",
      "LOSS train 0.00046523165656253695 valid 0.0009185667149722576\n",
      "LOSS train 0.00046523165656253695 valid 0.0009504876798018813\n",
      "LOSS train 0.00046523165656253695 valid 0.0009473128593526781\n",
      "LOSS train 0.00046523165656253695 valid 0.000948365603107959\n",
      "LOSS train 0.00046523165656253695 valid 0.0009460219880566001\n",
      "LOSS train 0.00046523165656253695 valid 0.0009597856551408768\n",
      "LOSS train 0.00046523165656253695 valid 0.0009577794116921723\n",
      "LOSS train 0.00046523165656253695 valid 0.0009690362494438887\n",
      "LOSS train 0.00046523165656253695 valid 0.000975996779743582\n",
      "LOSS train 0.00046523165656253695 valid 0.0009837624384090304\n",
      "EPOCH 31:\n",
      "  batch 1 loss: 0.00039798993384465575\n",
      "  batch 2 loss: 0.00029697641730308533\n",
      "  batch 3 loss: 0.0004084508982487023\n",
      "  batch 4 loss: 0.0003220395010430366\n",
      "  batch 5 loss: 0.0003342475974932313\n",
      "  batch 6 loss: 0.00040613580495119095\n",
      "  batch 7 loss: 0.0002683166821952909\n",
      "  batch 8 loss: 0.00031911974656395614\n",
      "  batch 9 loss: 0.00027466961182653904\n",
      "  batch 10 loss: 0.0002731551940087229\n",
      "  batch 11 loss: 0.0005198349826969206\n",
      "  batch 12 loss: 0.0004430853296071291\n",
      "  batch 13 loss: 0.00038321694592013955\n",
      "  batch 14 loss: 0.0006753921043127775\n",
      "  batch 15 loss: 0.0004372786497697234\n",
      "  batch 16 loss: 0.0003626556135714054\n",
      "  batch 17 loss: 0.00046495674178004265\n",
      "  batch 18 loss: 0.000353700335836038\n",
      "  batch 19 loss: 0.0007404808420687914\n",
      "  batch 20 loss: 0.0006076340796425939\n",
      "  batch 21 loss: 0.0006003198213875294\n",
      "  batch 22 loss: 0.0005706639494746923\n",
      "  batch 23 loss: 0.0005767218535766006\n",
      "  batch 24 loss: 0.0005598197458311915\n",
      "  batch 25 loss: 0.000687278457917273\n",
      "  batch 26 loss: 0.0005041822441853583\n",
      "  batch 27 loss: 0.0003270176239311695\n",
      "  batch 28 loss: 0.0004819858877453953\n",
      "  batch 29 loss: 0.0002751725842244923\n",
      "  batch 30 loss: 0.00031599696376360953\n",
      "  batch 31 loss: 0.00029739324236288667\n",
      "  batch 32 loss: 0.0003397349501028657\n",
      "  batch 33 loss: 0.0004433628055267036\n",
      "  batch 34 loss: 0.0004030464915558696\n",
      "  batch 35 loss: 0.0002998540294356644\n",
      "  batch 36 loss: 0.00029028780409134924\n",
      "  batch 37 loss: 0.00027192351990379393\n",
      "  batch 38 loss: 0.00030025007436051965\n",
      "  batch 39 loss: 0.00032686933991499245\n",
      "  batch 40 loss: 0.00042406839202158153\n",
      "  batch 41 loss: 0.0003082707989960909\n",
      "  batch 42 loss: 0.000394789210986346\n",
      "  batch 43 loss: 0.0002794516331050545\n",
      "  batch 44 loss: 0.0001981185341719538\n",
      "  batch 45 loss: 0.0002710312546696514\n",
      "  batch 46 loss: 0.00027996813878417015\n",
      "  batch 47 loss: 0.00028516381280496716\n",
      "  batch 48 loss: 0.00023400751524604857\n",
      "  batch 49 loss: 0.0002890593314077705\n",
      "  batch 50 loss: 0.00030625503859482706\n",
      "  batch 51 loss: 0.0003604326630011201\n",
      "  batch 52 loss: 0.0002645084750838578\n",
      "  batch 53 loss: 0.00029810640262439847\n",
      "  batch 54 loss: 0.000299098901450634\n",
      "  batch 55 loss: 0.00033066875766962767\n",
      "  batch 56 loss: 0.0002592944074422121\n",
      "  batch 57 loss: 0.00021460992866195738\n",
      "  batch 58 loss: 0.0003774269134737551\n",
      "  batch 59 loss: 0.0005439248634502292\n",
      "  batch 60 loss: 0.00040707236621528864\n",
      "  batch 61 loss: 0.00026973028434440494\n",
      "  batch 62 loss: 0.00029107433510944247\n",
      "  batch 63 loss: 0.00020246420172043145\n",
      "  batch 64 loss: 0.00020065723219886422\n",
      "  batch 65 loss: 0.00023316872830037028\n",
      "  batch 66 loss: 0.00023381997016258538\n",
      "  batch 67 loss: 0.00019333446107339114\n",
      "  batch 68 loss: 0.0002542227157391608\n",
      "  batch 69 loss: 0.0002496566157788038\n",
      "  batch 70 loss: 0.0003515315183904022\n",
      "  batch 71 loss: 0.00023510665050707757\n",
      "  batch 72 loss: 0.00023365819652099162\n",
      "  batch 73 loss: 0.00022676873777527362\n",
      "  batch 74 loss: 0.0002808034187182784\n",
      "  batch 75 loss: 0.00037281925324350595\n",
      "  batch 76 loss: 0.0002533589431550354\n",
      "  batch 77 loss: 0.0002557644620537758\n",
      "  batch 78 loss: 0.0002476819499861449\n",
      "  batch 79 loss: 0.0002497089153621346\n",
      "  batch 80 loss: 0.0002649431407917291\n",
      "  batch 81 loss: 0.0002743780496530235\n",
      "  batch 82 loss: 0.00029866190743632615\n",
      "  batch 83 loss: 0.0003116646548733115\n",
      "  batch 84 loss: 0.00023759265604894608\n",
      "  batch 85 loss: 0.00024773733457550406\n",
      "  batch 86 loss: 0.00028508977266028523\n",
      "  batch 87 loss: 0.00030988146318122745\n",
      "  batch 88 loss: 0.0005145275499671698\n",
      "  batch 89 loss: 0.00040199561044573784\n",
      "  batch 90 loss: 0.0004285388276912272\n",
      "  batch 91 loss: 0.00035720650339499116\n",
      "  batch 92 loss: 0.00031035818392410874\n",
      "  batch 93 loss: 0.0003065464261453599\n",
      "  batch 94 loss: 0.0002987495972774923\n",
      "  batch 95 loss: 0.00044851191341876984\n",
      "LOSS train 0.00044851191341876984 valid 0.0008415539050474763\n",
      "LOSS train 0.00044851191341876984 valid 0.0009732228354550898\n",
      "LOSS train 0.00044851191341876984 valid 0.00087216857355088\n",
      "LOSS train 0.00044851191341876984 valid 0.0008371295407414436\n",
      "LOSS train 0.00044851191341876984 valid 0.0008183902245946229\n",
      "LOSS train 0.00044851191341876984 valid 0.0007764004403725266\n",
      "LOSS train 0.00044851191341876984 valid 0.0008129053167067468\n",
      "LOSS train 0.00044851191341876984 valid 0.0008362391963601112\n",
      "LOSS train 0.00044851191341876984 valid 0.000809011806268245\n",
      "LOSS train 0.00044851191341876984 valid 0.0008338454063050449\n",
      "LOSS train 0.00044851191341876984 valid 0.0008982618455775082\n",
      "LOSS train 0.00044851191341876984 valid 0.0009032117668539286\n",
      "LOSS train 0.00044851191341876984 valid 0.0008856268832460046\n",
      "LOSS train 0.00044851191341876984 valid 0.0008994180243462324\n",
      "LOSS train 0.00044851191341876984 valid 0.000973591348156333\n",
      "LOSS train 0.00044851191341876984 valid 0.001007620245218277\n",
      "LOSS train 0.00044851191341876984 valid 0.0010040905326604843\n",
      "LOSS train 0.00044851191341876984 valid 0.0010039926273748279\n",
      "LOSS train 0.00044851191341876984 valid 0.0009982467163354158\n",
      "LOSS train 0.00044851191341876984 valid 0.0010107679991051555\n",
      "LOSS train 0.00044851191341876984 valid 0.0010042042704299092\n",
      "LOSS train 0.00044851191341876984 valid 0.0010136310011148453\n",
      "LOSS train 0.00044851191341876984 valid 0.0010213700588792562\n",
      "LOSS train 0.00044851191341876984 valid 0.0010333361569792032\n",
      "EPOCH 32:\n",
      "  batch 1 loss: 0.00039114058017730713\n",
      "  batch 2 loss: 0.0002850486780516803\n",
      "  batch 3 loss: 0.00037847261410206556\n",
      "  batch 4 loss: 0.00031247211154550314\n",
      "  batch 5 loss: 0.0003123222850263119\n",
      "  batch 6 loss: 0.00038675550604239106\n",
      "  batch 7 loss: 0.0002570622891653329\n",
      "  batch 8 loss: 0.00029147419263608754\n",
      "  batch 9 loss: 0.0002700102049857378\n",
      "  batch 10 loss: 0.0002779508358798921\n",
      "  batch 11 loss: 0.0005166594637557864\n",
      "  batch 12 loss: 0.00043546606320887804\n",
      "  batch 13 loss: 0.00037858792347833514\n",
      "  batch 14 loss: 0.000641638704109937\n",
      "  batch 15 loss: 0.00042469234904274344\n",
      "  batch 16 loss: 0.00035334276617504656\n",
      "  batch 17 loss: 0.00045943912118673325\n",
      "  batch 18 loss: 0.0003414797829464078\n",
      "  batch 19 loss: 0.0007146165589801967\n",
      "  batch 20 loss: 0.0006043027970008552\n",
      "  batch 21 loss: 0.0005962232826277614\n",
      "  batch 22 loss: 0.000552416080608964\n",
      "  batch 23 loss: 0.0005761276697739959\n",
      "  batch 24 loss: 0.0005644349148496985\n",
      "  batch 25 loss: 0.0006685106782242656\n",
      "  batch 26 loss: 0.0005002855323255062\n",
      "  batch 27 loss: 0.00032045505940914154\n",
      "  batch 28 loss: 0.00046981635387055576\n",
      "  batch 29 loss: 0.00027239363407716155\n",
      "  batch 30 loss: 0.00031464238418266177\n",
      "  batch 31 loss: 0.00030084364698268473\n",
      "  batch 32 loss: 0.0003331570769660175\n",
      "  batch 33 loss: 0.0004475827736314386\n",
      "  batch 34 loss: 0.000399310200009495\n",
      "  batch 35 loss: 0.00029351378907449543\n",
      "  batch 36 loss: 0.0002772113657556474\n",
      "  batch 37 loss: 0.00027857598615810275\n",
      "  batch 38 loss: 0.0002909136819653213\n",
      "  batch 39 loss: 0.00032377205207012594\n",
      "  batch 40 loss: 0.00040849822107702494\n",
      "  batch 41 loss: 0.0002904497960116714\n",
      "  batch 42 loss: 0.00039329787250608206\n",
      "  batch 43 loss: 0.000272392644546926\n",
      "  batch 44 loss: 0.0001935460022650659\n",
      "  batch 45 loss: 0.0002543372684158385\n",
      "  batch 46 loss: 0.0002714740112423897\n",
      "  batch 47 loss: 0.0002812292077578604\n",
      "  batch 48 loss: 0.0002321092615602538\n",
      "  batch 49 loss: 0.00028207688592374325\n",
      "  batch 50 loss: 0.0002917935489676893\n",
      "  batch 51 loss: 0.0003427245537750423\n",
      "  batch 52 loss: 0.0002552043297328055\n",
      "  batch 53 loss: 0.00028839046717621386\n",
      "  batch 54 loss: 0.00030802114633843303\n",
      "  batch 55 loss: 0.00033090822398662567\n",
      "  batch 56 loss: 0.0002557224652264267\n",
      "  batch 57 loss: 0.00021004630252718925\n",
      "  batch 58 loss: 0.00037524272920563817\n",
      "  batch 59 loss: 0.0005486247828230262\n",
      "  batch 60 loss: 0.00038573500933125615\n",
      "  batch 61 loss: 0.00027532613603398204\n",
      "  batch 62 loss: 0.0002844558912329376\n",
      "  batch 63 loss: 0.0002096742537105456\n",
      "  batch 64 loss: 0.00020106218289583921\n",
      "  batch 65 loss: 0.0002321313659194857\n",
      "  batch 66 loss: 0.00023302840418182313\n",
      "  batch 67 loss: 0.0002012312615988776\n",
      "  batch 68 loss: 0.0002645392087288201\n",
      "  batch 69 loss: 0.0002547503972891718\n",
      "  batch 70 loss: 0.00034302647691220045\n",
      "  batch 71 loss: 0.00023296655854210258\n",
      "  batch 72 loss: 0.00023723801132291555\n",
      "  batch 73 loss: 0.0002297886530868709\n",
      "  batch 74 loss: 0.0002754521556198597\n",
      "  batch 75 loss: 0.00036448484752327204\n",
      "  batch 76 loss: 0.000259388645645231\n",
      "  batch 77 loss: 0.00025414256379008293\n",
      "  batch 78 loss: 0.00024545035557821393\n",
      "  batch 79 loss: 0.00025000376626849174\n",
      "  batch 80 loss: 0.0002660464961081743\n",
      "  batch 81 loss: 0.0002763679367490113\n",
      "  batch 82 loss: 0.0002974000235553831\n",
      "  batch 83 loss: 0.00031287583988159895\n",
      "  batch 84 loss: 0.00023133776267059147\n",
      "  batch 85 loss: 0.00024759327061474323\n",
      "  batch 86 loss: 0.00028431619284674525\n",
      "  batch 87 loss: 0.0003034864494111389\n",
      "  batch 88 loss: 0.0004841149493586272\n",
      "  batch 89 loss: 0.0003957587468903512\n",
      "  batch 90 loss: 0.0004146261780988425\n",
      "  batch 91 loss: 0.00037055936991237104\n",
      "  batch 92 loss: 0.0003180330968461931\n",
      "  batch 93 loss: 0.0003075015847571194\n",
      "  batch 94 loss: 0.0002892977208830416\n",
      "  batch 95 loss: 0.00046321438276208937\n",
      "LOSS train 0.00046321438276208937 valid 0.0008117852848954499\n",
      "LOSS train 0.00046321438276208937 valid 0.00093990215100348\n",
      "LOSS train 0.00046321438276208937 valid 0.0008723726496100426\n",
      "LOSS train 0.00046321438276208937 valid 0.0008430929738096893\n",
      "LOSS train 0.00046321438276208937 valid 0.0008226164500229061\n",
      "LOSS train 0.00046321438276208937 valid 0.0007776691927574575\n",
      "LOSS train 0.00046321438276208937 valid 0.0008094051154330373\n",
      "LOSS train 0.00046321438276208937 valid 0.0008284376817755401\n",
      "LOSS train 0.00046321438276208937 valid 0.0008085814188234508\n",
      "LOSS train 0.00046321438276208937 valid 0.0008282230119220912\n",
      "LOSS train 0.00046321438276208937 valid 0.0008862052927725017\n",
      "LOSS train 0.00046321438276208937 valid 0.0008905943832360208\n",
      "LOSS train 0.00046321438276208937 valid 0.0008775952737778425\n",
      "LOSS train 0.00046321438276208937 valid 0.0008936484227888286\n",
      "LOSS train 0.00046321438276208937 valid 0.0009548504604026675\n",
      "LOSS train 0.00046321438276208937 valid 0.0009845957392826676\n",
      "LOSS train 0.00046321438276208937 valid 0.0009811298223212361\n",
      "LOSS train 0.00046321438276208937 valid 0.0009816780220717192\n",
      "LOSS train 0.00046321438276208937 valid 0.0009761154651641846\n",
      "LOSS train 0.00046321438276208937 valid 0.000983119593001902\n",
      "LOSS train 0.00046321438276208937 valid 0.0009736756910569966\n",
      "LOSS train 0.00046321438276208937 valid 0.0009803121211007237\n",
      "LOSS train 0.00046321438276208937 valid 0.0009838411351665854\n",
      "LOSS train 0.00046321438276208937 valid 0.0009852659422904253\n",
      "EPOCH 33:\n",
      "  batch 1 loss: 0.000382881669793278\n",
      "  batch 2 loss: 0.0002851981553249061\n",
      "  batch 3 loss: 0.00037961441557854414\n",
      "  batch 4 loss: 0.00031931669218465686\n",
      "  batch 5 loss: 0.00031566625693812966\n",
      "  batch 6 loss: 0.0003826110332738608\n",
      "  batch 7 loss: 0.00026746513321995735\n",
      "  batch 8 loss: 0.00030743834213353693\n",
      "  batch 9 loss: 0.0002583133755251765\n",
      "  batch 10 loss: 0.00025899536558426917\n",
      "  batch 11 loss: 0.0004724235332105309\n",
      "  batch 12 loss: 0.0004160271491855383\n",
      "  batch 13 loss: 0.0003685175906866789\n",
      "  batch 14 loss: 0.000640478334389627\n",
      "  batch 15 loss: 0.0004013605648651719\n",
      "  batch 16 loss: 0.0003426523762755096\n",
      "  batch 17 loss: 0.0004432646674104035\n",
      "  batch 18 loss: 0.00033852836349979043\n",
      "  batch 19 loss: 0.0006866717012599111\n",
      "  batch 20 loss: 0.0005973015213385224\n",
      "  batch 21 loss: 0.0005829379661008716\n",
      "  batch 22 loss: 0.0005383621901273727\n",
      "  batch 23 loss: 0.0005560785066336393\n",
      "  batch 24 loss: 0.0005427250871434808\n",
      "  batch 25 loss: 0.0006596292369067669\n",
      "  batch 26 loss: 0.0005062817945145071\n",
      "  batch 27 loss: 0.0002991212531924248\n",
      "  batch 28 loss: 0.0004484541714191437\n",
      "  batch 29 loss: 0.00025799270952120423\n",
      "  batch 30 loss: 0.00030042079742997885\n",
      "  batch 31 loss: 0.00029807136161252856\n",
      "  batch 32 loss: 0.00032813113648444414\n",
      "  batch 33 loss: 0.00045011338079348207\n",
      "  batch 34 loss: 0.00041798659367486835\n",
      "  batch 35 loss: 0.0002913189819082618\n",
      "  batch 36 loss: 0.00026895530754700303\n",
      "  batch 37 loss: 0.0002651011454872787\n",
      "  batch 38 loss: 0.0002816829364746809\n",
      "  batch 39 loss: 0.0003245132393203676\n",
      "  batch 40 loss: 0.00041522138053551316\n",
      "  batch 41 loss: 0.00028482082416303456\n",
      "  batch 42 loss: 0.00038509711157530546\n",
      "  batch 43 loss: 0.00026987859746441245\n",
      "  batch 44 loss: 0.00019900778715964407\n",
      "  batch 45 loss: 0.0002525861491449177\n",
      "  batch 46 loss: 0.00025635238853283226\n",
      "  batch 47 loss: 0.0002680493635125458\n",
      "  batch 48 loss: 0.0002229628007626161\n",
      "  batch 49 loss: 0.0002776067121885717\n",
      "  batch 50 loss: 0.0002898400416597724\n",
      "  batch 51 loss: 0.00034083580248989165\n",
      "  batch 52 loss: 0.0002611805684864521\n",
      "  batch 53 loss: 0.00027762941317632794\n",
      "  batch 54 loss: 0.00027761905221268535\n",
      "  batch 55 loss: 0.0003072843828704208\n",
      "  batch 56 loss: 0.00024665938690304756\n",
      "  batch 57 loss: 0.00019956217147409916\n",
      "  batch 58 loss: 0.0003625755780376494\n",
      "  batch 59 loss: 0.0005240304162725806\n",
      "  batch 60 loss: 0.0003963705967180431\n",
      "  batch 61 loss: 0.00025985907996073365\n",
      "  batch 62 loss: 0.0002696227456908673\n",
      "  batch 63 loss: 0.0001981172536034137\n",
      "  batch 64 loss: 0.0001848595857154578\n",
      "  batch 65 loss: 0.0002162893651984632\n",
      "  batch 66 loss: 0.00023194676032289863\n",
      "  batch 67 loss: 0.0001949486613739282\n",
      "  batch 68 loss: 0.0002474529028404504\n",
      "  batch 69 loss: 0.0002506398886907846\n",
      "  batch 70 loss: 0.0003334457869641483\n",
      "  batch 71 loss: 0.00022984962561167777\n",
      "  batch 72 loss: 0.0002198195579694584\n",
      "  batch 73 loss: 0.00021726303384639323\n",
      "  batch 74 loss: 0.00027834210777655244\n",
      "  batch 75 loss: 0.0003651642764452845\n",
      "  batch 76 loss: 0.00024946907069534063\n",
      "  batch 77 loss: 0.00025173352332785726\n",
      "  batch 78 loss: 0.00024099097936414182\n",
      "  batch 79 loss: 0.00024033617228269577\n",
      "  batch 80 loss: 0.0002553650992922485\n",
      "  batch 81 loss: 0.0002688844397198409\n",
      "  batch 82 loss: 0.0002900460676755756\n",
      "  batch 83 loss: 0.00029936578357592225\n",
      "  batch 84 loss: 0.00022305933816824108\n",
      "  batch 85 loss: 0.00024140937603078783\n",
      "  batch 86 loss: 0.0002747615217231214\n",
      "  batch 87 loss: 0.0002978387929033488\n",
      "  batch 88 loss: 0.0004696149262599647\n",
      "  batch 89 loss: 0.00039109771023504436\n",
      "  batch 90 loss: 0.0004081319784745574\n",
      "  batch 91 loss: 0.00033848462044261396\n",
      "  batch 92 loss: 0.0003021945303771645\n",
      "  batch 93 loss: 0.00029157690005376935\n",
      "  batch 94 loss: 0.000282128865364939\n",
      "  batch 95 loss: 0.00043930826359428465\n",
      "LOSS train 0.00043930826359428465 valid 0.0008159918943420053\n",
      "LOSS train 0.00043930826359428465 valid 0.0009423779556527734\n",
      "LOSS train 0.00043930826359428465 valid 0.0008565618190914392\n",
      "LOSS train 0.00043930826359428465 valid 0.0008333989535458386\n",
      "LOSS train 0.00043930826359428465 valid 0.0008175436523742974\n",
      "LOSS train 0.00043930826359428465 valid 0.0007773411925882101\n",
      "LOSS train 0.00043930826359428465 valid 0.0008055848884396255\n",
      "LOSS train 0.00043930826359428465 valid 0.0008319169282913208\n",
      "LOSS train 0.00043930826359428465 valid 0.0008051805198192596\n",
      "LOSS train 0.00043930826359428465 valid 0.0008176018600352108\n",
      "LOSS train 0.00043930826359428465 valid 0.000873332261107862\n",
      "LOSS train 0.00043930826359428465 valid 0.0008810032159090042\n",
      "LOSS train 0.00043930826359428465 valid 0.0008702315972186625\n",
      "LOSS train 0.00043930826359428465 valid 0.0008861866663210094\n",
      "LOSS train 0.00043930826359428465 valid 0.0009763947455212474\n",
      "LOSS train 0.00043930826359428465 valid 0.001009755302220583\n",
      "LOSS train 0.00043930826359428465 valid 0.0010050631826743484\n",
      "LOSS train 0.00043930826359428465 valid 0.0010065736714750528\n",
      "LOSS train 0.00043930826359428465 valid 0.0010003304341807961\n",
      "LOSS train 0.00043930826359428465 valid 0.0010095088509842753\n",
      "LOSS train 0.00043930826359428465 valid 0.0010041288333013654\n",
      "LOSS train 0.00043930826359428465 valid 0.0010120851220563054\n",
      "LOSS train 0.00043930826359428465 valid 0.0010209113825112581\n",
      "LOSS train 0.00043930826359428465 valid 0.0010403792839497328\n",
      "EPOCH 34:\n",
      "  batch 1 loss: 0.00039345899131149054\n",
      "  batch 2 loss: 0.0002698428579606116\n",
      "  batch 3 loss: 0.00037464359775185585\n",
      "  batch 4 loss: 0.00031533153378404677\n",
      "  batch 5 loss: 0.00029907876159995794\n",
      "  batch 6 loss: 0.00035917278728447855\n",
      "  batch 7 loss: 0.00025089806877076626\n",
      "  batch 8 loss: 0.00027667530230246484\n",
      "  batch 9 loss: 0.0002579981810413301\n",
      "  batch 10 loss: 0.0002371786249568686\n",
      "  batch 11 loss: 0.00044232391519472003\n",
      "  batch 12 loss: 0.00039717770414426923\n",
      "  batch 13 loss: 0.0003597194154281169\n",
      "  batch 14 loss: 0.0006301990943029523\n",
      "  batch 15 loss: 0.00041423190850764513\n",
      "  batch 16 loss: 0.0003302808036096394\n",
      "  batch 17 loss: 0.0004317661514505744\n",
      "  batch 18 loss: 0.00032143984572030604\n",
      "  batch 19 loss: 0.0007002417696639895\n",
      "  batch 20 loss: 0.000584987283218652\n",
      "  batch 21 loss: 0.0005612988607026637\n",
      "  batch 22 loss: 0.0005265012150630355\n",
      "  batch 23 loss: 0.0005834579933434725\n",
      "  batch 24 loss: 0.0005452075856737792\n",
      "  batch 25 loss: 0.0006449948996305466\n",
      "  batch 26 loss: 0.0004992521135136485\n",
      "  batch 27 loss: 0.00031951454002410173\n",
      "  batch 28 loss: 0.0004834358114749193\n",
      "  batch 29 loss: 0.0002649249217938632\n",
      "  batch 30 loss: 0.0003208251728210598\n",
      "  batch 31 loss: 0.00030238833278417587\n",
      "  batch 32 loss: 0.00034219800727441907\n",
      "  batch 33 loss: 0.00044606346637010574\n",
      "  batch 34 loss: 0.000399232521886006\n",
      "  batch 35 loss: 0.00028168497374281287\n",
      "  batch 36 loss: 0.00027598251472227275\n",
      "  batch 37 loss: 0.00025878200540319085\n",
      "  batch 38 loss: 0.00027295545442029834\n",
      "  batch 39 loss: 0.0003395844832994044\n",
      "  batch 40 loss: 0.00043565870146267116\n",
      "  batch 41 loss: 0.00028021555044688284\n",
      "  batch 42 loss: 0.0003762240521609783\n",
      "  batch 43 loss: 0.0002643254993017763\n",
      "  batch 44 loss: 0.00019758197595365345\n",
      "  batch 45 loss: 0.00025759663549251854\n",
      "  batch 46 loss: 0.0002578353451099247\n",
      "  batch 47 loss: 0.00026809354312717915\n",
      "  batch 48 loss: 0.00021781219402328134\n",
      "  batch 49 loss: 0.0002790896687656641\n",
      "  batch 50 loss: 0.0002847992000170052\n",
      "  batch 51 loss: 0.000340782105922699\n",
      "  batch 52 loss: 0.0002569363568909466\n",
      "  batch 53 loss: 0.00027795234927907586\n",
      "  batch 54 loss: 0.0002834533806890249\n",
      "  batch 55 loss: 0.00031617225613445044\n",
      "  batch 56 loss: 0.00024283264065161347\n",
      "  batch 57 loss: 0.00019767633057199419\n",
      "  batch 58 loss: 0.00033752102172002196\n",
      "  batch 59 loss: 0.0005088604521006346\n",
      "  batch 60 loss: 0.0003863307647407055\n",
      "  batch 61 loss: 0.00026240761508233845\n",
      "  batch 62 loss: 0.0002776908513624221\n",
      "  batch 63 loss: 0.00020181895524729043\n",
      "  batch 64 loss: 0.000188785808859393\n",
      "  batch 65 loss: 0.00021353350894059986\n",
      "  batch 66 loss: 0.00022148247808218002\n",
      "  batch 67 loss: 0.00019426846120040864\n",
      "  batch 68 loss: 0.0002479146933183074\n",
      "  batch 69 loss: 0.0002464326098561287\n",
      "  batch 70 loss: 0.00033842737320810556\n",
      "  batch 71 loss: 0.00022875296417623758\n",
      "  batch 72 loss: 0.00021574081620201468\n",
      "  batch 73 loss: 0.00020708977535832673\n",
      "  batch 74 loss: 0.0002682118210941553\n",
      "  batch 75 loss: 0.0003570588887669146\n",
      "  batch 76 loss: 0.00024162857152987272\n",
      "  batch 77 loss: 0.0002531276550143957\n",
      "  batch 78 loss: 0.0002457315567880869\n",
      "  batch 79 loss: 0.0002385249244980514\n",
      "  batch 80 loss: 0.00025503142387606204\n",
      "  batch 81 loss: 0.00026962923584505916\n",
      "  batch 82 loss: 0.00028890627436339855\n",
      "  batch 83 loss: 0.0003018392017111182\n",
      "  batch 84 loss: 0.00022138691565487534\n",
      "  batch 85 loss: 0.0002389458240941167\n",
      "  batch 86 loss: 0.0002764164237305522\n",
      "  batch 87 loss: 0.00029104965506121516\n",
      "  batch 88 loss: 0.00044992711627855897\n",
      "  batch 89 loss: 0.00038729567313566804\n",
      "  batch 90 loss: 0.00040329061448574066\n",
      "  batch 91 loss: 0.00033976545091718435\n",
      "  batch 92 loss: 0.0003047520003747195\n",
      "  batch 93 loss: 0.0002960345009341836\n",
      "  batch 94 loss: 0.000282170542050153\n",
      "  batch 95 loss: 0.0004181933181826025\n",
      "LOSS train 0.0004181933181826025 valid 0.000786798307672143\n",
      "LOSS train 0.0004181933181826025 valid 0.000887470378074795\n",
      "LOSS train 0.0004181933181826025 valid 0.0008228444494307041\n",
      "LOSS train 0.0004181933181826025 valid 0.0007964568212628365\n",
      "LOSS train 0.0004181933181826025 valid 0.0007841898477636278\n",
      "LOSS train 0.0004181933181826025 valid 0.000744527205824852\n",
      "LOSS train 0.0004181933181826025 valid 0.0007800232851877809\n",
      "LOSS train 0.0004181933181826025 valid 0.0008031208999454975\n",
      "LOSS train 0.0004181933181826025 valid 0.0007827164372429252\n",
      "LOSS train 0.0004181933181826025 valid 0.0007945225806906819\n",
      "LOSS train 0.0004181933181826025 valid 0.0008412143215537071\n",
      "LOSS train 0.0004181933181826025 valid 0.0008466796716675162\n",
      "LOSS train 0.0004181933181826025 valid 0.0008381786174140871\n",
      "LOSS train 0.0004181933181826025 valid 0.0008550974307581782\n",
      "LOSS train 0.0004181933181826025 valid 0.0009247990674339235\n",
      "LOSS train 0.0004181933181826025 valid 0.0009594899602234364\n",
      "LOSS train 0.0004181933181826025 valid 0.0009571401751600206\n",
      "LOSS train 0.0004181933181826025 valid 0.0009575705626048148\n",
      "LOSS train 0.0004181933181826025 valid 0.000953163777012378\n",
      "LOSS train 0.0004181933181826025 valid 0.0009581693448126316\n",
      "LOSS train 0.0004181933181826025 valid 0.0009514889097772539\n",
      "LOSS train 0.0004181933181826025 valid 0.0009586024680174887\n",
      "LOSS train 0.0004181933181826025 valid 0.0009637646726332605\n",
      "LOSS train 0.0004181933181826025 valid 0.0009709442383609712\n",
      "EPOCH 35:\n",
      "  batch 1 loss: 0.0003827071632258594\n",
      "  batch 2 loss: 0.00025624415138736367\n",
      "  batch 3 loss: 0.0003568071115296334\n",
      "  batch 4 loss: 0.00031000591116026044\n",
      "  batch 5 loss: 0.00029277746216394007\n",
      "  batch 6 loss: 0.0003538933815434575\n",
      "  batch 7 loss: 0.0002449708990752697\n",
      "  batch 8 loss: 0.0002670581452548504\n",
      "  batch 9 loss: 0.0002582433808129281\n",
      "  batch 10 loss: 0.00023827409313526005\n",
      "  batch 11 loss: 0.0004358128644526005\n",
      "  batch 12 loss: 0.0003861396398860961\n",
      "  batch 13 loss: 0.0003481890889815986\n",
      "  batch 14 loss: 0.0006049193907529116\n",
      "  batch 15 loss: 0.0003898149007000029\n",
      "  batch 16 loss: 0.0003227288252674043\n",
      "  batch 17 loss: 0.0004011181299574673\n",
      "  batch 18 loss: 0.0003336860390845686\n",
      "  batch 19 loss: 0.0006741602555848658\n",
      "  batch 20 loss: 0.00054576201364398\n",
      "  batch 21 loss: 0.0005337399197742343\n",
      "  batch 22 loss: 0.0004824843490496278\n",
      "  batch 23 loss: 0.0005485985311679542\n",
      "  batch 24 loss: 0.0005329946288838983\n",
      "  batch 25 loss: 0.0006237479974515736\n",
      "  batch 26 loss: 0.0005075787776149809\n",
      "  batch 27 loss: 0.0002980261342599988\n",
      "  batch 28 loss: 0.00045267760287970304\n",
      "  batch 29 loss: 0.00024280940124299377\n",
      "  batch 30 loss: 0.000268012925516814\n",
      "  batch 31 loss: 0.0002903484273701906\n",
      "  batch 32 loss: 0.00031879445305094123\n",
      "  batch 33 loss: 0.0004446381935849786\n",
      "  batch 34 loss: 0.0004056765465065837\n",
      "  batch 35 loss: 0.0002688037930056453\n",
      "  batch 36 loss: 0.00024576467694714665\n",
      "  batch 37 loss: 0.0002547434123698622\n",
      "  batch 38 loss: 0.0002609700313769281\n",
      "  batch 39 loss: 0.00030897511169314384\n",
      "  batch 40 loss: 0.0004053927259519696\n",
      "  batch 41 loss: 0.00026030989829450846\n",
      "  batch 42 loss: 0.00034481711918488145\n",
      "  batch 43 loss: 0.0002556602703407407\n",
      "  batch 44 loss: 0.00018370177713222802\n",
      "  batch 45 loss: 0.00024234657757915556\n",
      "  batch 46 loss: 0.00024490116629749537\n",
      "  batch 47 loss: 0.00025464006466791034\n",
      "  batch 48 loss: 0.00021739141084253788\n",
      "  batch 49 loss: 0.0002794695319607854\n",
      "  batch 50 loss: 0.0002819880028255284\n",
      "  batch 51 loss: 0.0003464345645625144\n",
      "  batch 52 loss: 0.0002488616737537086\n",
      "  batch 53 loss: 0.0002730059204623103\n",
      "  batch 54 loss: 0.00027297387714497745\n",
      "  batch 55 loss: 0.00030654569854959846\n",
      "  batch 56 loss: 0.0002351530420128256\n",
      "  batch 57 loss: 0.00018638462643139064\n",
      "  batch 58 loss: 0.0003274504269938916\n",
      "  batch 59 loss: 0.0005005420534871519\n",
      "  batch 60 loss: 0.0003832072834484279\n",
      "  batch 61 loss: 0.0002544620365370065\n",
      "  batch 62 loss: 0.0002787224657367915\n",
      "  batch 63 loss: 0.0001988630392588675\n",
      "  batch 64 loss: 0.00017464891425333917\n",
      "  batch 65 loss: 0.00021551529061980546\n",
      "  batch 66 loss: 0.00022501913190353662\n",
      "  batch 67 loss: 0.00019783791503868997\n",
      "  batch 68 loss: 0.00024124707852024585\n",
      "  batch 69 loss: 0.00024058707640506327\n",
      "  batch 70 loss: 0.0003192703879904002\n",
      "  batch 71 loss: 0.00021643875516019762\n",
      "  batch 72 loss: 0.0002081879210891202\n",
      "  batch 73 loss: 0.0002073142968583852\n",
      "  batch 74 loss: 0.00026962312404066324\n",
      "  batch 75 loss: 0.0003499434096738696\n",
      "  batch 76 loss: 0.00023457205679733306\n",
      "  batch 77 loss: 0.0002498174726497382\n",
      "  batch 78 loss: 0.00024213998403865844\n",
      "  batch 79 loss: 0.00022985835676081479\n",
      "  batch 80 loss: 0.00025073447613976896\n",
      "  batch 81 loss: 0.0002668360830284655\n",
      "  batch 82 loss: 0.0002825914998538792\n",
      "  batch 83 loss: 0.0002894324716180563\n",
      "  batch 84 loss: 0.00021658799960277975\n",
      "  batch 85 loss: 0.00023196284018922597\n",
      "  batch 86 loss: 0.00026541639817878604\n",
      "  batch 87 loss: 0.00028226140420883894\n",
      "  batch 88 loss: 0.00043186440598219633\n",
      "  batch 89 loss: 0.0003774366632569581\n",
      "  batch 90 loss: 0.0003970992402173579\n",
      "  batch 91 loss: 0.0003178875776939094\n",
      "  batch 92 loss: 0.00029803364304825664\n",
      "  batch 93 loss: 0.0002904800930991769\n",
      "  batch 94 loss: 0.00027249648701399565\n",
      "  batch 95 loss: 0.0004075632896274328\n",
      "LOSS train 0.0004075632896274328 valid 0.0007564619882032275\n",
      "LOSS train 0.0004075632896274328 valid 0.0008601061999797821\n",
      "LOSS train 0.0004075632896274328 valid 0.0007964236428961158\n",
      "LOSS train 0.0004075632896274328 valid 0.0007705105817876756\n",
      "LOSS train 0.0004075632896274328 valid 0.0007575411582365632\n",
      "LOSS train 0.0004075632896274328 valid 0.0007211806951090693\n",
      "LOSS train 0.0004075632896274328 valid 0.0007614557980559766\n",
      "LOSS train 0.0004075632896274328 valid 0.0007853549323044717\n",
      "LOSS train 0.0004075632896274328 valid 0.0007637889939360321\n",
      "LOSS train 0.0004075632896274328 valid 0.0007773137767799199\n",
      "LOSS train 0.0004075632896274328 valid 0.0008231950341723859\n",
      "LOSS train 0.0004075632896274328 valid 0.0008267058874480426\n",
      "LOSS train 0.0004075632896274328 valid 0.0008193746325559914\n",
      "LOSS train 0.0004075632896274328 valid 0.000837079540360719\n",
      "LOSS train 0.0004075632896274328 valid 0.0009133415296673775\n",
      "LOSS train 0.0004075632896274328 valid 0.000945373612921685\n",
      "LOSS train 0.0004075632896274328 valid 0.0009414840606041253\n",
      "LOSS train 0.0004075632896274328 valid 0.0009433545637875795\n",
      "LOSS train 0.0004075632896274328 valid 0.0009396487730555236\n",
      "LOSS train 0.0004075632896274328 valid 0.0009473905083723366\n",
      "LOSS train 0.0004075632896274328 valid 0.0009413887746632099\n",
      "LOSS train 0.0004075632896274328 valid 0.0009511931566521525\n",
      "LOSS train 0.0004075632896274328 valid 0.0009586242958903313\n",
      "LOSS train 0.0004075632896274328 valid 0.0009704236290417612\n",
      "EPOCH 36:\n",
      "  batch 1 loss: 0.00037754204822704196\n",
      "  batch 2 loss: 0.0002503538562450558\n",
      "  batch 3 loss: 0.00035115802893415093\n",
      "  batch 4 loss: 0.00030132391839288175\n",
      "  batch 5 loss: 0.00028548535192385316\n",
      "  batch 6 loss: 0.00030220035114325583\n",
      "  batch 7 loss: 0.00021858734544366598\n",
      "  batch 8 loss: 0.00023544742725789547\n",
      "  batch 9 loss: 0.00034326009335927665\n",
      "  batch 10 loss: 0.0002911225601565093\n",
      "  batch 11 loss: 0.00047749310033395886\n",
      "  batch 12 loss: 0.00039810113958083093\n",
      "  batch 13 loss: 0.0003456264385022223\n",
      "  batch 14 loss: 0.000611051160376519\n",
      "  batch 15 loss: 0.00036567222559824586\n",
      "  batch 16 loss: 0.0002842486137524247\n",
      "  batch 17 loss: 0.00038316749851219356\n",
      "  batch 18 loss: 0.00032026745611801744\n",
      "  batch 19 loss: 0.0006278603686951101\n",
      "  batch 20 loss: 0.00048427883302792907\n",
      "  batch 21 loss: 0.0004951394512318075\n",
      "  batch 22 loss: 0.00041857839096337557\n",
      "  batch 23 loss: 0.0004867604002356529\n",
      "  batch 24 loss: 0.00045885518193244934\n",
      "  batch 25 loss: 0.0005347655387595296\n",
      "  batch 26 loss: 0.0006054382538422942\n",
      "  batch 27 loss: 0.0003167462127748877\n",
      "  batch 28 loss: 0.0005085340817458928\n",
      "  batch 29 loss: 0.00024105126794893295\n",
      "  batch 30 loss: 0.00027000345289707184\n",
      "  batch 31 loss: 0.0002679641474969685\n",
      "  batch 32 loss: 0.0002923909923993051\n",
      "  batch 33 loss: 0.00044110690942034125\n",
      "  batch 34 loss: 0.0004121817764826119\n",
      "  batch 35 loss: 0.00026227475609630346\n",
      "  batch 36 loss: 0.00024211907293647528\n",
      "  batch 37 loss: 0.0002360219950787723\n",
      "  batch 38 loss: 0.00022492290008813143\n",
      "  batch 39 loss: 0.0002950105117633939\n",
      "  batch 40 loss: 0.0004585551214404404\n",
      "  batch 41 loss: 0.00024228269467130303\n",
      "  batch 42 loss: 0.00031497556483373046\n",
      "  batch 43 loss: 0.0002543815935496241\n",
      "  batch 44 loss: 0.00017496742657385767\n",
      "  batch 45 loss: 0.00024113646941259503\n",
      "  batch 46 loss: 0.00025127074331976473\n",
      "  batch 47 loss: 0.00026031406014226377\n",
      "  batch 48 loss: 0.00021592633856926113\n",
      "  batch 49 loss: 0.0002743877121247351\n",
      "  batch 50 loss: 0.00027726375265046954\n",
      "  batch 51 loss: 0.0003405269526410848\n",
      "  batch 52 loss: 0.00024393294006586075\n",
      "  batch 53 loss: 0.0002681072219274938\n",
      "  batch 54 loss: 0.0002725281519815326\n",
      "  batch 55 loss: 0.0003032160166185349\n",
      "  batch 56 loss: 0.00022863384219817817\n",
      "  batch 57 loss: 0.00018294768233317882\n",
      "  batch 58 loss: 0.00032194959931075573\n",
      "  batch 59 loss: 0.0005395497428253293\n",
      "  batch 60 loss: 0.0003610767307691276\n",
      "  batch 61 loss: 0.00023343355860561132\n",
      "  batch 62 loss: 0.00027549214428290725\n",
      "  batch 63 loss: 0.00020193490490783006\n",
      "  batch 64 loss: 0.00017931823094841093\n",
      "  batch 65 loss: 0.00020618890994228423\n",
      "  batch 66 loss: 0.00022921747586224228\n",
      "  batch 67 loss: 0.00020426811533980072\n",
      "  batch 68 loss: 0.000244827417191118\n",
      "  batch 69 loss: 0.0002364877873333171\n",
      "  batch 70 loss: 0.0003302029799669981\n",
      "  batch 71 loss: 0.00021584873320534825\n",
      "  batch 72 loss: 0.0002169649233110249\n",
      "  batch 73 loss: 0.00020652309467550367\n",
      "  batch 74 loss: 0.00025912097771652043\n",
      "  batch 75 loss: 0.00034850294468924403\n",
      "  batch 76 loss: 0.0002490856568329036\n",
      "  batch 77 loss: 0.00024569343077018857\n",
      "  batch 78 loss: 0.00023031997261568904\n",
      "  batch 79 loss: 0.00023765400692354888\n",
      "  batch 80 loss: 0.00025263283168897033\n",
      "  batch 81 loss: 0.00026373908622190356\n",
      "  batch 82 loss: 0.0002800655784085393\n",
      "  batch 83 loss: 0.00029696436831727624\n",
      "  batch 84 loss: 0.0002138050040230155\n",
      "  batch 85 loss: 0.0002332418371224776\n",
      "  batch 86 loss: 0.000269086507614702\n",
      "  batch 87 loss: 0.00028869713423773646\n",
      "  batch 88 loss: 0.00043689191807061434\n",
      "  batch 89 loss: 0.0003854866954497993\n",
      "  batch 90 loss: 0.000397795025492087\n",
      "  batch 91 loss: 0.0003373522777110338\n",
      "  batch 92 loss: 0.0002916775119956583\n",
      "  batch 93 loss: 0.00031084383954294026\n",
      "  batch 94 loss: 0.0002856168139260262\n",
      "  batch 95 loss: 0.00041275896364822984\n",
      "LOSS train 0.00041275896364822984 valid 0.0007857543532736599\n",
      "LOSS train 0.00041275896364822984 valid 0.0008812297601252794\n",
      "LOSS train 0.00041275896364822984 valid 0.0008071847259998322\n",
      "LOSS train 0.00041275896364822984 valid 0.000783905852586031\n",
      "LOSS train 0.00041275896364822984 valid 0.0007662922726012766\n",
      "LOSS train 0.00041275896364822984 valid 0.0007278488483279943\n",
      "LOSS train 0.00041275896364822984 valid 0.0007723522721789777\n",
      "LOSS train 0.00041275896364822984 valid 0.0007958932546898723\n",
      "LOSS train 0.00041275896364822984 valid 0.0007745976326987147\n",
      "LOSS train 0.00041275896364822984 valid 0.0007898341864347458\n",
      "LOSS train 0.00041275896364822984 valid 0.0008355765603482723\n",
      "LOSS train 0.00041275896364822984 valid 0.0008384604007005692\n",
      "LOSS train 0.00041275896364822984 valid 0.0008307225070893764\n",
      "LOSS train 0.00041275896364822984 valid 0.0008436066564172506\n",
      "LOSS train 0.00041275896364822984 valid 0.0009076838032342494\n",
      "LOSS train 0.00041275896364822984 valid 0.0009373820503242314\n",
      "LOSS train 0.00041275896364822984 valid 0.0009335047216154635\n",
      "LOSS train 0.00041275896364822984 valid 0.0009359189425595105\n",
      "LOSS train 0.00041275896364822984 valid 0.0009333299240097404\n",
      "LOSS train 0.00041275896364822984 valid 0.0009387031313963234\n",
      "LOSS train 0.00041275896364822984 valid 0.0009298635413870215\n",
      "LOSS train 0.00041275896364822984 valid 0.0009388895705342293\n",
      "LOSS train 0.00041275896364822984 valid 0.0009421494323760271\n",
      "LOSS train 0.00041275896364822984 valid 0.0009472253150306642\n",
      "EPOCH 37:\n",
      "  batch 1 loss: 0.000374811002984643\n",
      "  batch 2 loss: 0.00024547174689359963\n",
      "  batch 3 loss: 0.0003488711081445217\n",
      "  batch 4 loss: 0.00028386080521158874\n",
      "  batch 5 loss: 0.0002682001213543117\n",
      "  batch 6 loss: 0.0003321321273688227\n",
      "  batch 7 loss: 0.00023931334726512432\n",
      "  batch 8 loss: 0.0002659595338627696\n",
      "  batch 9 loss: 0.000266395480139181\n",
      "  batch 10 loss: 0.00025210590683855116\n",
      "  batch 11 loss: 0.00041120595415122807\n",
      "  batch 12 loss: 0.00037219893420115113\n",
      "  batch 13 loss: 0.0003375130472704768\n",
      "  batch 14 loss: 0.0005924415308982134\n",
      "  batch 15 loss: 0.00036907108733430505\n",
      "  batch 16 loss: 0.00030730903381481767\n",
      "  batch 17 loss: 0.0003513078554533422\n",
      "  batch 18 loss: 0.00029864933458156884\n",
      "  batch 19 loss: 0.0006056819693185389\n",
      "  batch 20 loss: 0.0005149111384525895\n",
      "  batch 21 loss: 0.0005231404211372137\n",
      "  batch 22 loss: 0.00045276086893863976\n",
      "  batch 23 loss: 0.0005534875672310591\n",
      "  batch 24 loss: 0.0004892447032034397\n",
      "  batch 25 loss: 0.0005792301381006837\n",
      "  batch 26 loss: 0.0004790047532878816\n",
      "  batch 27 loss: 0.0002682411577552557\n",
      "  batch 28 loss: 0.0004117086064070463\n",
      "  batch 29 loss: 0.00024160969769582152\n",
      "  batch 30 loss: 0.0002461341500747949\n",
      "  batch 31 loss: 0.0002597603015601635\n",
      "  batch 32 loss: 0.0003039932926185429\n",
      "  batch 33 loss: 0.00040807013283483684\n",
      "  batch 34 loss: 0.0003939814050681889\n",
      "  batch 35 loss: 0.0002430164022371173\n",
      "  batch 36 loss: 0.0002345824905205518\n",
      "  batch 37 loss: 0.00024119819863699377\n",
      "  batch 38 loss: 0.0002553400699980557\n",
      "  batch 39 loss: 0.0002984417660627514\n",
      "  batch 40 loss: 0.0004043489461764693\n",
      "  batch 41 loss: 0.0002484325086697936\n",
      "  batch 42 loss: 0.0003453494282439351\n",
      "  batch 43 loss: 0.000249749340582639\n",
      "  batch 44 loss: 0.00017072560149244964\n",
      "  batch 45 loss: 0.00023787033569533378\n",
      "  batch 46 loss: 0.00023852914455346763\n",
      "  batch 47 loss: 0.0002378544886596501\n",
      "  batch 48 loss: 0.00020843282982241362\n",
      "  batch 49 loss: 0.0002667441440280527\n",
      "  batch 50 loss: 0.0002688470995053649\n",
      "  batch 51 loss: 0.00030648469692096114\n",
      "  batch 52 loss: 0.0002423835830995813\n",
      "  batch 53 loss: 0.00026140816044062376\n",
      "  batch 54 loss: 0.00025601996458135545\n",
      "  batch 55 loss: 0.0003024942707270384\n",
      "  batch 56 loss: 0.0002177556452807039\n",
      "  batch 57 loss: 0.00018007555627264082\n",
      "  batch 58 loss: 0.00030144790071062744\n",
      "  batch 59 loss: 0.0004798252775799483\n",
      "  batch 60 loss: 0.0003582871286198497\n",
      "  batch 61 loss: 0.00023764879733789712\n",
      "  batch 62 loss: 0.00026192073710262775\n",
      "  batch 63 loss: 0.00019906673696823418\n",
      "  batch 64 loss: 0.00017817405750975013\n",
      "  batch 65 loss: 0.00020123730064369738\n",
      "  batch 66 loss: 0.0002184949116781354\n",
      "  batch 67 loss: 0.00020372965082060546\n",
      "  batch 68 loss: 0.0002396490308456123\n",
      "  batch 69 loss: 0.00023724447237327695\n",
      "  batch 70 loss: 0.00032341189216822386\n",
      "  batch 71 loss: 0.00021112526883371174\n",
      "  batch 72 loss: 0.00020449470321182162\n",
      "  batch 73 loss: 0.00019996096671093255\n",
      "  batch 74 loss: 0.0002547136973589659\n",
      "  batch 75 loss: 0.00033563445322215557\n",
      "  batch 76 loss: 0.00023338315077126026\n",
      "  batch 77 loss: 0.00024542163009755313\n",
      "  batch 78 loss: 0.00023437409254256636\n",
      "  batch 79 loss: 0.00023058181977830827\n",
      "  batch 80 loss: 0.0002521317219361663\n",
      "  batch 81 loss: 0.00025949208065867424\n",
      "  batch 82 loss: 0.0002753071894403547\n",
      "  batch 83 loss: 0.00029061920940876007\n",
      "  batch 84 loss: 0.00021072982053738087\n",
      "  batch 85 loss: 0.0002253318962175399\n",
      "  batch 86 loss: 0.0002603127504698932\n",
      "  batch 87 loss: 0.0002792098093777895\n",
      "  batch 88 loss: 0.0004058429622091353\n",
      "  batch 89 loss: 0.00037181563675403595\n",
      "  batch 90 loss: 0.0003822542494162917\n",
      "  batch 91 loss: 0.0003381987917236984\n",
      "  batch 92 loss: 0.00029030806035734713\n",
      "  batch 93 loss: 0.0002888269955292344\n",
      "  batch 94 loss: 0.0002688855165615678\n",
      "  batch 95 loss: 0.0003879769647028297\n",
      "LOSS train 0.0003879769647028297 valid 0.0007325168699026108\n",
      "LOSS train 0.0003879769647028297 valid 0.0008268804522231221\n",
      "LOSS train 0.0003879769647028297 valid 0.0007854332216084003\n",
      "LOSS train 0.0003879769647028297 valid 0.0007639562245458364\n",
      "LOSS train 0.0003879769647028297 valid 0.000750549603253603\n",
      "LOSS train 0.0003879769647028297 valid 0.0007085714023560286\n",
      "LOSS train 0.0003879769647028297 valid 0.0007473218138329685\n",
      "LOSS train 0.0003879769647028297 valid 0.0007725865580141544\n",
      "LOSS train 0.0003879769647028297 valid 0.0007515743491239846\n",
      "LOSS train 0.0003879769647028297 valid 0.0007643271237611771\n",
      "LOSS train 0.0003879769647028297 valid 0.0008010264136828482\n",
      "LOSS train 0.0003879769647028297 valid 0.0008034982602111995\n",
      "LOSS train 0.0003879769647028297 valid 0.000798279361333698\n",
      "LOSS train 0.0003879769647028297 valid 0.0008176971459761262\n",
      "LOSS train 0.0003879769647028297 valid 0.0008800465730018914\n",
      "LOSS train 0.0003879769647028297 valid 0.0009074856643564999\n",
      "LOSS train 0.0003879769647028297 valid 0.0009071824606508017\n",
      "LOSS train 0.0003879769647028297 valid 0.000909306516405195\n",
      "LOSS train 0.0003879769647028297 valid 0.0009072390967048705\n",
      "LOSS train 0.0003879769647028297 valid 0.0009113278356380761\n",
      "LOSS train 0.0003879769647028297 valid 0.0009037088020704687\n",
      "LOSS train 0.0003879769647028297 valid 0.0009130946127697825\n",
      "LOSS train 0.0003879769647028297 valid 0.0009155074367299676\n",
      "LOSS train 0.0003879769647028297 valid 0.0009226751863025129\n",
      "EPOCH 38:\n",
      "  batch 1 loss: 0.0003677578060887754\n",
      "  batch 2 loss: 0.00023312326811719686\n",
      "  batch 3 loss: 0.0003379621193744242\n",
      "  batch 4 loss: 0.0002753680164460093\n",
      "  batch 5 loss: 0.0002725799276959151\n",
      "  batch 6 loss: 0.00033376223291270435\n",
      "  batch 7 loss: 0.00024039525305852294\n",
      "  batch 8 loss: 0.00026012901798821986\n",
      "  batch 9 loss: 0.0002584975736681372\n",
      "  batch 10 loss: 0.00023006908304523677\n",
      "  batch 11 loss: 0.00039055419620126486\n",
      "  batch 12 loss: 0.00035737233702093363\n",
      "  batch 13 loss: 0.0003295923233963549\n",
      "  batch 14 loss: 0.0005478367675095797\n",
      "  batch 15 loss: 0.00035354442661628127\n",
      "  batch 16 loss: 0.00030179531313478947\n",
      "  batch 17 loss: 0.0003617404727265239\n",
      "  batch 18 loss: 0.00030520762084051967\n",
      "  batch 19 loss: 0.0005760680651292205\n",
      "  batch 20 loss: 0.0005085247685201466\n",
      "  batch 21 loss: 0.0005169076612219214\n",
      "  batch 22 loss: 0.0004316136473789811\n",
      "  batch 23 loss: 0.00048686773516237736\n",
      "  batch 24 loss: 0.0004652100906241685\n",
      "  batch 25 loss: 0.0005725591909140348\n",
      "  batch 26 loss: 0.00045385974226519465\n",
      "  batch 27 loss: 0.00026999443070963025\n",
      "  batch 28 loss: 0.0004212656931485981\n",
      "  batch 29 loss: 0.00022719454136677086\n",
      "  batch 30 loss: 0.00023846313706599176\n",
      "  batch 31 loss: 0.0002400005905656144\n",
      "  batch 32 loss: 0.0002932994975708425\n",
      "  batch 33 loss: 0.00039139256114140153\n",
      "  batch 34 loss: 0.0003802906139753759\n",
      "  batch 35 loss: 0.00022737566905561835\n",
      "  batch 36 loss: 0.00022220640676096082\n",
      "  batch 37 loss: 0.00023423877428285778\n",
      "  batch 38 loss: 0.0002348213893128559\n",
      "  batch 39 loss: 0.0002914231736212969\n",
      "  batch 40 loss: 0.0004162983677815646\n",
      "  batch 41 loss: 0.00023565170704387128\n",
      "  batch 42 loss: 0.0003233435854781419\n",
      "  batch 43 loss: 0.00023503159172832966\n",
      "  batch 44 loss: 0.0001648254692554474\n",
      "  batch 45 loss: 0.00022541539510712028\n",
      "  batch 46 loss: 0.00023955934739205986\n",
      "  batch 47 loss: 0.0002387679705861956\n",
      "  batch 48 loss: 0.00020487199071794748\n",
      "  batch 49 loss: 0.00026088504819199443\n",
      "  batch 50 loss: 0.00026303541380912066\n",
      "  batch 51 loss: 0.0003024964826181531\n",
      "  batch 52 loss: 0.00023491364845540375\n",
      "  batch 53 loss: 0.00025447376538068056\n",
      "  batch 54 loss: 0.0002559794520493597\n",
      "  batch 55 loss: 0.0002924477157648653\n",
      "  batch 56 loss: 0.00021115201525390148\n",
      "  batch 57 loss: 0.00017743854550644755\n",
      "  batch 58 loss: 0.00028954268782399595\n",
      "  batch 59 loss: 0.0004537436179816723\n",
      "  batch 60 loss: 0.00036040658596903086\n",
      "  batch 61 loss: 0.0002337230835109949\n",
      "  batch 62 loss: 0.0002497121458873153\n",
      "  batch 63 loss: 0.00019974664610344917\n",
      "  batch 64 loss: 0.00017736178415361792\n",
      "  batch 65 loss: 0.0002018413506448269\n",
      "  batch 66 loss: 0.0002089377521770075\n",
      "  batch 67 loss: 0.00017369151464663446\n",
      "  batch 68 loss: 0.00022629550949204713\n",
      "  batch 69 loss: 0.00022477470338344574\n",
      "  batch 70 loss: 0.00031319702975451946\n",
      "  batch 71 loss: 0.0002008480078075081\n",
      "  batch 72 loss: 0.00020245660562068224\n",
      "  batch 73 loss: 0.00019555144535843283\n",
      "  batch 74 loss: 0.0002667078224476427\n",
      "  batch 75 loss: 0.00033919484121724963\n",
      "  batch 76 loss: 0.000228648423217237\n",
      "  batch 77 loss: 0.00023731162946205586\n",
      "  batch 78 loss: 0.00022712630743626505\n",
      "  batch 79 loss: 0.0002158270071959123\n",
      "  batch 80 loss: 0.00023896859784144908\n",
      "  batch 81 loss: 0.00024630408734083176\n",
      "  batch 82 loss: 0.00027259462513029575\n",
      "  batch 83 loss: 0.00028291522176004946\n",
      "  batch 84 loss: 0.00020566547755151987\n",
      "  batch 85 loss: 0.00021957390708848834\n",
      "  batch 86 loss: 0.00026143924333155155\n",
      "  batch 87 loss: 0.0002684939536266029\n",
      "  batch 88 loss: 0.0003799989935941994\n",
      "  batch 89 loss: 0.00034739807597361505\n",
      "  batch 90 loss: 0.0003757494851015508\n",
      "  batch 91 loss: 0.0002945312298834324\n",
      "  batch 92 loss: 0.00027042970759794116\n",
      "  batch 93 loss: 0.00027138355653733015\n",
      "  batch 94 loss: 0.00026703515322878957\n",
      "  batch 95 loss: 0.00037842587335035205\n",
      "LOSS train 0.00037842587335035205 valid 0.0007425171788781881\n",
      "LOSS train 0.00037842587335035205 valid 0.0008499915711581707\n",
      "LOSS train 0.00037842587335035205 valid 0.0008141828584484756\n",
      "LOSS train 0.00037842587335035205 valid 0.0007869477849453688\n",
      "LOSS train 0.00037842587335035205 valid 0.0007735923863947392\n",
      "LOSS train 0.00037842587335035205 valid 0.0007325428887270391\n",
      "LOSS train 0.00037842587335035205 valid 0.0007720637950114906\n",
      "LOSS train 0.00037842587335035205 valid 0.0007919652271084487\n",
      "LOSS train 0.00037842587335035205 valid 0.0007684820448048413\n",
      "LOSS train 0.00037842587335035205 valid 0.0007836957811377943\n",
      "LOSS train 0.00037842587335035205 valid 0.0008326364331878722\n",
      "LOSS train 0.00037842587335035205 valid 0.0008342451765201986\n",
      "LOSS train 0.00037842587335035205 valid 0.0008278277236968279\n",
      "LOSS train 0.00037842587335035205 valid 0.0008469371241517365\n",
      "LOSS train 0.00037842587335035205 valid 0.0009114632848650217\n",
      "LOSS train 0.00037842587335035205 valid 0.0009450659272260964\n",
      "LOSS train 0.00037842587335035205 valid 0.0009428318589925766\n",
      "LOSS train 0.00037842587335035205 valid 0.0009437539847567677\n",
      "LOSS train 0.00037842587335035205 valid 0.0009389850310981274\n",
      "LOSS train 0.00037842587335035205 valid 0.0009463513270020485\n",
      "LOSS train 0.00037842587335035205 valid 0.0009394125081598759\n",
      "LOSS train 0.00037842587335035205 valid 0.0009498102008365095\n",
      "LOSS train 0.00037842587335035205 valid 0.000956484756898135\n",
      "LOSS train 0.00037842587335035205 valid 0.0009669003775343299\n",
      "EPOCH 39:\n",
      "  batch 1 loss: 0.0003664215619210154\n",
      "  batch 2 loss: 0.00022701497073285282\n",
      "  batch 3 loss: 0.00032187163014896214\n",
      "  batch 4 loss: 0.00027369539020583034\n",
      "  batch 5 loss: 0.00026448286371305585\n",
      "  batch 6 loss: 0.00031504244543612003\n",
      "  batch 7 loss: 0.0002242939663119614\n",
      "  batch 8 loss: 0.00023630815849173814\n",
      "  batch 9 loss: 0.0002526301541365683\n",
      "  batch 10 loss: 0.00021471947547979653\n",
      "  batch 11 loss: 0.0003936399007216096\n",
      "  batch 12 loss: 0.0003530314425006509\n",
      "  batch 13 loss: 0.00032924028346315026\n",
      "  batch 14 loss: 0.0005488833412528038\n",
      "  batch 15 loss: 0.00035593134816735983\n",
      "  batch 16 loss: 0.00029368529794737697\n",
      "  batch 17 loss: 0.00034204477560706437\n",
      "  batch 18 loss: 0.0002825792762450874\n",
      "  batch 19 loss: 0.0005518800462596118\n",
      "  batch 20 loss: 0.0004909103736281395\n",
      "  batch 21 loss: 0.000495104119181633\n",
      "  batch 22 loss: 0.0004102463135495782\n",
      "  batch 23 loss: 0.00046810891944915056\n",
      "  batch 24 loss: 0.0004665492451749742\n",
      "  batch 25 loss: 0.000536048726644367\n",
      "  batch 26 loss: 0.0004683823208324611\n",
      "  batch 27 loss: 0.00027524595498107374\n",
      "  batch 28 loss: 0.0004282857698854059\n",
      "  batch 29 loss: 0.00021924344764556736\n",
      "  batch 30 loss: 0.00021315697813406587\n",
      "  batch 31 loss: 0.0002123753074556589\n",
      "  batch 32 loss: 0.0002788224956020713\n",
      "  batch 33 loss: 0.0003759060346055776\n",
      "  batch 34 loss: 0.00037134543526917696\n",
      "  batch 35 loss: 0.0002224238560302183\n",
      "  batch 36 loss: 0.00020487510482780635\n",
      "  batch 37 loss: 0.0002194786793552339\n",
      "  batch 38 loss: 0.00022948648256715387\n",
      "  batch 39 loss: 0.0002804213436320424\n",
      "  batch 40 loss: 0.0004271367797628045\n",
      "  batch 41 loss: 0.0002073805662803352\n",
      "  batch 42 loss: 0.00030073244124650955\n",
      "  batch 43 loss: 0.0002252314006909728\n",
      "  batch 44 loss: 0.000165655292221345\n",
      "  batch 45 loss: 0.00019964667444583029\n",
      "  batch 46 loss: 0.00023708930530119687\n",
      "  batch 47 loss: 0.00024117971770465374\n",
      "  batch 48 loss: 0.00020778781617991626\n",
      "  batch 49 loss: 0.0002609596122056246\n",
      "  batch 50 loss: 0.00026436540065333247\n",
      "  batch 51 loss: 0.00029936135979369283\n",
      "  batch 52 loss: 0.00023689567751716822\n",
      "  batch 53 loss: 0.00023640230938326567\n",
      "  batch 54 loss: 0.000239856744883582\n",
      "  batch 55 loss: 0.00029874901520088315\n",
      "  batch 56 loss: 0.0002154692483600229\n",
      "  batch 57 loss: 0.0001739600847940892\n",
      "  batch 58 loss: 0.00027514330577105284\n",
      "  batch 59 loss: 0.00044655357487499714\n",
      "  batch 60 loss: 0.00034679233795031905\n",
      "  batch 61 loss: 0.00022747638286091387\n",
      "  batch 62 loss: 0.00024699309142306447\n",
      "  batch 63 loss: 0.00019260472618043423\n",
      "  batch 64 loss: 0.00016677561507094651\n",
      "  batch 65 loss: 0.00019138478091917932\n",
      "  batch 66 loss: 0.0002060492552118376\n",
      "  batch 67 loss: 0.00017515606305096298\n",
      "  batch 68 loss: 0.00022678730601910502\n",
      "  batch 69 loss: 0.00022016357979737222\n",
      "  batch 70 loss: 0.00030147156212478876\n",
      "  batch 71 loss: 0.00019303509907331318\n",
      "  batch 72 loss: 0.00020064503769390285\n",
      "  batch 73 loss: 0.00019255588995292783\n",
      "  batch 74 loss: 0.0002560161519795656\n",
      "  batch 75 loss: 0.00032848166301846504\n",
      "  batch 76 loss: 0.0002216744760517031\n",
      "  batch 77 loss: 0.00023142337158787996\n",
      "  batch 78 loss: 0.00022485246881842613\n",
      "  batch 79 loss: 0.0002099992852890864\n",
      "  batch 80 loss: 0.00023097392113413662\n",
      "  batch 81 loss: 0.00023674855765420943\n",
      "  batch 82 loss: 0.0002636111166793853\n",
      "  batch 83 loss: 0.00027495616814121604\n",
      "  batch 84 loss: 0.00020054352353326976\n",
      "  batch 85 loss: 0.00021503666357602924\n",
      "  batch 86 loss: 0.00025052379351109266\n",
      "  batch 87 loss: 0.0002649218076840043\n",
      "  batch 88 loss: 0.0003797209355980158\n",
      "  batch 89 loss: 0.0003472360549494624\n",
      "  batch 90 loss: 0.00036578677827492356\n",
      "  batch 91 loss: 0.00030388616141863167\n",
      "  batch 92 loss: 0.00026214029639959335\n",
      "  batch 93 loss: 0.00027101521845906973\n",
      "  batch 94 loss: 0.00025745356106199324\n",
      "  batch 95 loss: 0.0003833214577753097\n",
      "LOSS train 0.0003833214577753097 valid 0.0007472657598555088\n",
      "LOSS train 0.0003833214577753097 valid 0.0008502913406118751\n",
      "LOSS train 0.0003833214577753097 valid 0.0008215142297558486\n",
      "LOSS train 0.0003833214577753097 valid 0.0008017884101718664\n",
      "LOSS train 0.0003833214577753097 valid 0.0007943473756313324\n",
      "LOSS train 0.0003833214577753097 valid 0.0007491132128052413\n",
      "LOSS train 0.0003833214577753097 valid 0.0007857151213102043\n",
      "LOSS train 0.0003833214577753097 valid 0.0008060874533839524\n",
      "LOSS train 0.0003833214577753097 valid 0.0007857719901949167\n",
      "LOSS train 0.0003833214577753097 valid 0.0008009853190742433\n",
      "LOSS train 0.0003833214577753097 valid 0.0008519042166881263\n",
      "LOSS train 0.0003833214577753097 valid 0.0008523546857759356\n",
      "LOSS train 0.0003833214577753097 valid 0.0008490115869790316\n",
      "LOSS train 0.0003833214577753097 valid 0.0008709505782462656\n",
      "LOSS train 0.0003833214577753097 valid 0.0009340222459286451\n",
      "LOSS train 0.0003833214577753097 valid 0.0009682325180619955\n",
      "LOSS train 0.0003833214577753097 valid 0.0009699944639578462\n",
      "LOSS train 0.0003833214577753097 valid 0.0009760877583175898\n",
      "LOSS train 0.0003833214577753097 valid 0.000971218862105161\n",
      "LOSS train 0.0003833214577753097 valid 0.0009791770717129111\n",
      "LOSS train 0.0003833214577753097 valid 0.0009712630999274552\n",
      "LOSS train 0.0003833214577753097 valid 0.0009814193472266197\n",
      "LOSS train 0.0003833214577753097 valid 0.0009886958869174123\n",
      "LOSS train 0.0003833214577753097 valid 0.000993572291918099\n",
      "EPOCH 40:\n",
      "  batch 1 loss: 0.000352697737980634\n",
      "  batch 2 loss: 0.0002187303762184456\n",
      "  batch 3 loss: 0.00032628545886836946\n",
      "  batch 4 loss: 0.0002599461586214602\n",
      "  batch 5 loss: 0.0002532781509216875\n",
      "  batch 6 loss: 0.0002964173909276724\n",
      "  batch 7 loss: 0.00020117004169151187\n",
      "  batch 8 loss: 0.0002171018859371543\n",
      "  batch 9 loss: 0.00024345719430129975\n",
      "  batch 10 loss: 0.00020817761833313853\n",
      "  batch 11 loss: 0.00036826712312176824\n",
      "  batch 12 loss: 0.00034513819264248013\n",
      "  batch 13 loss: 0.00031770841451361775\n",
      "  batch 14 loss: 0.000553973950445652\n",
      "  batch 15 loss: 0.0003937517758458853\n",
      "  batch 16 loss: 0.00029309658566489816\n",
      "  batch 17 loss: 0.00032288741203956306\n",
      "  batch 18 loss: 0.0002936088712885976\n",
      "  batch 19 loss: 0.0005677816225215793\n",
      "  batch 20 loss: 0.00046412908704951406\n",
      "  batch 21 loss: 0.0004652267089113593\n",
      "  batch 22 loss: 0.0003912450629286468\n",
      "  batch 23 loss: 0.0004386762739159167\n",
      "  batch 24 loss: 0.0004507880948949605\n",
      "  batch 25 loss: 0.0005461452528834343\n",
      "  batch 26 loss: 0.0005074454820714891\n",
      "  batch 27 loss: 0.0002617455320432782\n",
      "  batch 28 loss: 0.0004259701236151159\n",
      "  batch 29 loss: 0.00021203453070484102\n",
      "  batch 30 loss: 0.00021498624118976295\n",
      "  batch 31 loss: 0.00022022865596227348\n",
      "  batch 32 loss: 0.0002803457318805158\n",
      "  batch 33 loss: 0.00035265146289020777\n",
      "  batch 34 loss: 0.00036222056951373816\n",
      "  batch 35 loss: 0.00020626821788027883\n",
      "  batch 36 loss: 0.000203628049348481\n",
      "  batch 37 loss: 0.0002153339155483991\n",
      "  batch 38 loss: 0.00020769746333826333\n",
      "  batch 39 loss: 0.00027480468270368874\n",
      "  batch 40 loss: 0.0004004311340395361\n",
      "  batch 41 loss: 0.00019191528554074466\n",
      "  batch 42 loss: 0.00028622179524973035\n",
      "  batch 43 loss: 0.00021540513262152672\n",
      "  batch 44 loss: 0.000155014538904652\n",
      "  batch 45 loss: 0.00020198691345285624\n",
      "  batch 46 loss: 0.0002390633017057553\n",
      "  batch 47 loss: 0.00023414535098709166\n",
      "  batch 48 loss: 0.00020126751041971147\n",
      "  batch 49 loss: 0.0002628376241773367\n",
      "  batch 50 loss: 0.00025999158970080316\n",
      "  batch 51 loss: 0.0003270618326496333\n",
      "  batch 52 loss: 0.00023600473650731146\n",
      "  batch 53 loss: 0.00024427042808383703\n",
      "  batch 54 loss: 0.00024440657580271363\n",
      "  batch 55 loss: 0.0002936985401902348\n",
      "  batch 56 loss: 0.00021711803856305778\n",
      "  batch 57 loss: 0.00017723679775372148\n",
      "  batch 58 loss: 0.0002682864142116159\n",
      "  batch 59 loss: 0.0004655752854887396\n",
      "  batch 60 loss: 0.0003411845536902547\n",
      "  batch 61 loss: 0.0002164888137485832\n",
      "  batch 62 loss: 0.000257530395174399\n",
      "  batch 63 loss: 0.00018902579904533923\n",
      "  batch 64 loss: 0.00016098319611046463\n",
      "  batch 65 loss: 0.00018204122898168862\n",
      "  batch 66 loss: 0.00022170267766341567\n",
      "  batch 67 loss: 0.00017912074690684676\n",
      "  batch 68 loss: 0.00024718965869396925\n",
      "  batch 69 loss: 0.00022956450993660837\n",
      "  batch 70 loss: 0.00031923112692311406\n",
      "  batch 71 loss: 0.00019695537048391998\n",
      "  batch 72 loss: 0.00019382787286303937\n",
      "  batch 73 loss: 0.00019257293024566025\n",
      "  batch 74 loss: 0.00025222758995369077\n",
      "  batch 75 loss: 0.00033801980316638947\n",
      "  batch 76 loss: 0.0002214077685493976\n",
      "  batch 77 loss: 0.00023961017723195255\n",
      "  batch 78 loss: 0.0002292734570801258\n",
      "  batch 79 loss: 0.00022481364430859685\n",
      "  batch 80 loss: 0.0002424824924673885\n",
      "  batch 81 loss: 0.00023871456505730748\n",
      "  batch 82 loss: 0.00026697939028963447\n",
      "  batch 83 loss: 0.000275878090178594\n",
      "  batch 84 loss: 0.00019763690943364054\n",
      "  batch 85 loss: 0.00021751683379989117\n",
      "  batch 86 loss: 0.00025253926287405193\n",
      "  batch 87 loss: 0.0002652617695275694\n",
      "  batch 88 loss: 0.0003858168493025005\n",
      "  batch 89 loss: 0.00035310996463522315\n",
      "  batch 90 loss: 0.00037632626481354237\n",
      "  batch 91 loss: 0.0002856362843886018\n",
      "  batch 92 loss: 0.0002697558666113764\n",
      "  batch 93 loss: 0.00026935129426419735\n",
      "  batch 94 loss: 0.0002575150574557483\n",
      "  batch 95 loss: 0.0003745622525457293\n",
      "LOSS train 0.0003745622525457293 valid 0.0008232326945289969\n",
      "LOSS train 0.0003745622525457293 valid 0.0009369041654281318\n",
      "LOSS train 0.0003745622525457293 valid 0.0008939406834542751\n",
      "LOSS train 0.0003745622525457293 valid 0.000853866571560502\n",
      "LOSS train 0.0003745622525457293 valid 0.0008425111300311983\n",
      "LOSS train 0.0003745622525457293 valid 0.0007975276093930006\n",
      "LOSS train 0.0003745622525457293 valid 0.0008406366687268019\n",
      "LOSS train 0.0003745622525457293 valid 0.0008705584914423525\n",
      "LOSS train 0.0003745622525457293 valid 0.0008484576246701181\n",
      "LOSS train 0.0003745622525457293 valid 0.0008649090304970741\n",
      "LOSS train 0.0003745622525457293 valid 0.0009111264371313155\n",
      "LOSS train 0.0003745622525457293 valid 0.0009097259026020765\n",
      "LOSS train 0.0003745622525457293 valid 0.0009057855932042003\n",
      "LOSS train 0.0003745622525457293 valid 0.0009265576372854412\n",
      "LOSS train 0.0003745622525457293 valid 0.0009974341373890638\n",
      "LOSS train 0.0003745622525457293 valid 0.0010295750107616186\n",
      "LOSS train 0.0003745622525457293 valid 0.0010263260919600725\n",
      "LOSS train 0.0003745622525457293 valid 0.0010304625611752272\n",
      "LOSS train 0.0003745622525457293 valid 0.0010269138729199767\n",
      "LOSS train 0.0003745622525457293 valid 0.0010364903137087822\n",
      "LOSS train 0.0003745622525457293 valid 0.00102754938416183\n",
      "LOSS train 0.0003745622525457293 valid 0.0010388449300080538\n",
      "LOSS train 0.0003745622525457293 valid 0.0010499943746253848\n",
      "LOSS train 0.0003745622525457293 valid 0.0010653671342879534\n",
      "EPOCH 41:\n",
      "  batch 1 loss: 0.00036079162964597344\n",
      "  batch 2 loss: 0.00022680147958453745\n",
      "  batch 3 loss: 0.00032739934977144003\n",
      "  batch 4 loss: 0.0002623327891342342\n",
      "  batch 5 loss: 0.00023897786741144955\n",
      "  batch 6 loss: 0.0002904962166212499\n",
      "  batch 7 loss: 0.00019113154849037528\n",
      "  batch 8 loss: 0.0002259320463053882\n",
      "  batch 9 loss: 0.0002443691191729158\n",
      "  batch 10 loss: 0.00022015154536347836\n",
      "  batch 11 loss: 0.00042877410305663943\n",
      "  batch 12 loss: 0.0003451418597251177\n",
      "  batch 13 loss: 0.0003234993200749159\n",
      "  batch 14 loss: 0.000528506119735539\n",
      "  batch 15 loss: 0.0003375949163455516\n",
      "  batch 16 loss: 0.0003089840174652636\n",
      "  batch 17 loss: 0.0003384361043572426\n",
      "  batch 18 loss: 0.0003102547489106655\n",
      "  batch 19 loss: 0.000534268154297024\n",
      "  batch 20 loss: 0.00048271435662172735\n",
      "  batch 21 loss: 0.0005244119674898684\n",
      "  batch 22 loss: 0.000407253741286695\n",
      "  batch 23 loss: 0.0004405650543048978\n",
      "  batch 24 loss: 0.00044931474258191884\n",
      "  batch 25 loss: 0.0005492879427038133\n",
      "  batch 26 loss: 0.00048006518045440316\n",
      "  batch 27 loss: 0.00028985782410018146\n",
      "  batch 28 loss: 0.00045735161984339356\n",
      "  batch 29 loss: 0.00024277178454212844\n",
      "  batch 30 loss: 0.0002166102931369096\n",
      "  batch 31 loss: 0.00021372869377955794\n",
      "  batch 32 loss: 0.0002823732211254537\n",
      "  batch 33 loss: 0.0003480413870420307\n",
      "  batch 34 loss: 0.00036144809564575553\n",
      "  batch 35 loss: 0.00021505632321350276\n",
      "  batch 36 loss: 0.00022413115948438644\n",
      "  batch 37 loss: 0.00021748922881670296\n",
      "  batch 38 loss: 0.00022097807959653437\n",
      "  batch 39 loss: 0.0002931327326223254\n",
      "  batch 40 loss: 0.00041938835056498647\n",
      "  batch 41 loss: 0.0002159044670406729\n",
      "  batch 42 loss: 0.0002982825681101531\n",
      "  batch 43 loss: 0.00023131065245252103\n",
      "  batch 44 loss: 0.0001572337350808084\n",
      "  batch 45 loss: 0.00020228537323419005\n",
      "  batch 46 loss: 0.00023052707547321916\n",
      "  batch 47 loss: 0.00023441169469151646\n",
      "  batch 48 loss: 0.0002141822624253109\n",
      "  batch 49 loss: 0.00027564281481318176\n",
      "  batch 50 loss: 0.00027508390485309064\n",
      "  batch 51 loss: 0.00030207238160073757\n",
      "  batch 52 loss: 0.0002322531072422862\n",
      "  batch 53 loss: 0.00024717272026464343\n",
      "  batch 54 loss: 0.0002486731973476708\n",
      "  batch 55 loss: 0.0002969153574667871\n",
      "  batch 56 loss: 0.0002276193699799478\n",
      "  batch 57 loss: 0.00017686681530904025\n",
      "  batch 58 loss: 0.00028878863668069243\n",
      "  batch 59 loss: 0.0004577547078952193\n",
      "  batch 60 loss: 0.00035419673076830804\n",
      "  batch 61 loss: 0.00021782326803077012\n",
      "  batch 62 loss: 0.0002521669666748494\n",
      "  batch 63 loss: 0.00019938650075346231\n",
      "  batch 64 loss: 0.00016379573207814246\n",
      "  batch 65 loss: 0.0001884298981167376\n",
      "  batch 66 loss: 0.00021439266856759787\n",
      "  batch 67 loss: 0.00019044274813495576\n",
      "  batch 68 loss: 0.0002335831813979894\n",
      "  batch 69 loss: 0.00022708033793605864\n",
      "  batch 70 loss: 0.00031100164051167667\n",
      "  batch 71 loss: 0.00019012166012544185\n",
      "  batch 72 loss: 0.0002019636012846604\n",
      "  batch 73 loss: 0.00018778364756144583\n",
      "  batch 74 loss: 0.00025753770023584366\n",
      "  batch 75 loss: 0.00034446889185346663\n",
      "  batch 76 loss: 0.00023202860029414296\n",
      "  batch 77 loss: 0.00023342868371400982\n",
      "  batch 78 loss: 0.000221753740333952\n",
      "  batch 79 loss: 0.00021705799736082554\n",
      "  batch 80 loss: 0.00023917373619042337\n",
      "  batch 81 loss: 0.0002385605184827\n",
      "  batch 82 loss: 0.00025972671573981643\n",
      "  batch 83 loss: 0.0002755702589638531\n",
      "  batch 84 loss: 0.00019932814757339656\n",
      "  batch 85 loss: 0.00021320849191397429\n",
      "  batch 86 loss: 0.00024614445283077657\n",
      "  batch 87 loss: 0.00025866145733743906\n",
      "  batch 88 loss: 0.0003897223505191505\n",
      "  batch 89 loss: 0.00035712766111828387\n",
      "  batch 90 loss: 0.0003691231249831617\n",
      "  batch 91 loss: 0.0003202070074621588\n",
      "  batch 92 loss: 0.0002690131077542901\n",
      "  batch 93 loss: 0.0002713677240535617\n",
      "  batch 94 loss: 0.0002626226923894137\n",
      "  batch 95 loss: 0.00037233324837870896\n",
      "LOSS train 0.00037233324837870896 valid 0.0008126838365569711\n",
      "LOSS train 0.00037233324837870896 valid 0.0009362414130009711\n",
      "LOSS train 0.00037233324837870896 valid 0.0008562867296859622\n",
      "LOSS train 0.00037233324837870896 valid 0.0008172341622412205\n",
      "LOSS train 0.00037233324837870896 valid 0.0008000637753866613\n",
      "LOSS train 0.00037233324837870896 valid 0.0007605489809066057\n",
      "LOSS train 0.00037233324837870896 valid 0.0008122601429931819\n",
      "LOSS train 0.00037233324837870896 valid 0.0008417484350502491\n",
      "LOSS train 0.00037233324837870896 valid 0.0008145237225107849\n",
      "LOSS train 0.00037233324837870896 valid 0.0008298473549075425\n",
      "LOSS train 0.00037233324837870896 valid 0.0008847138960845768\n",
      "LOSS train 0.00037233324837870896 valid 0.0008862774702720344\n",
      "LOSS train 0.00037233324837870896 valid 0.0008729179971851408\n",
      "LOSS train 0.00037233324837870896 valid 0.0008939496474340558\n",
      "LOSS train 0.00037233324837870896 valid 0.0009831998031586409\n",
      "LOSS train 0.00037233324837870896 valid 0.0010150509187951684\n",
      "LOSS train 0.00037233324837870896 valid 0.0010124160908162594\n",
      "LOSS train 0.00037233324837870896 valid 0.0010174111230298877\n",
      "LOSS train 0.00037233324837870896 valid 0.0010148737346753478\n",
      "LOSS train 0.00037233324837870896 valid 0.0010175475617870688\n",
      "LOSS train 0.00037233324837870896 valid 0.0010088690323755145\n",
      "LOSS train 0.00037233324837870896 valid 0.0010185237042605877\n",
      "LOSS train 0.00037233324837870896 valid 0.00102427345700562\n",
      "LOSS train 0.00037233324837870896 valid 0.0010455797892063856\n",
      "EPOCH 42:\n",
      "  batch 1 loss: 0.0003425746981520206\n",
      "  batch 2 loss: 0.00022047940001357347\n",
      "  batch 3 loss: 0.00032679171999916434\n",
      "  batch 4 loss: 0.00026148848701268435\n",
      "  batch 5 loss: 0.0002431115717627108\n",
      "  batch 6 loss: 0.00031035050051286817\n",
      "  batch 7 loss: 0.00018686258408706635\n",
      "  batch 8 loss: 0.00021417290554381907\n",
      "  batch 9 loss: 0.00025728257605805993\n",
      "  batch 10 loss: 0.00021669745910912752\n",
      "  batch 11 loss: 0.00040848698699846864\n",
      "  batch 12 loss: 0.0003556399024091661\n",
      "  batch 13 loss: 0.0003193437005393207\n",
      "  batch 14 loss: 0.0005534685915336013\n",
      "  batch 15 loss: 0.0003261172096244991\n",
      "  batch 16 loss: 0.0002826356212608516\n",
      "  batch 17 loss: 0.0003062209871131927\n",
      "  batch 18 loss: 0.00031070265686139464\n",
      "  batch 19 loss: 0.000501043105032295\n",
      "  batch 20 loss: 0.0004439178737811744\n",
      "  batch 21 loss: 0.0004401860642246902\n",
      "  batch 22 loss: 0.00039709429256618023\n",
      "  batch 23 loss: 0.00046985832159407437\n",
      "  batch 24 loss: 0.00043841078877449036\n",
      "  batch 25 loss: 0.0005274813156574965\n",
      "  batch 26 loss: 0.00046515086432918906\n",
      "  batch 27 loss: 0.00024505265173502266\n",
      "  batch 28 loss: 0.00040075145079754293\n",
      "  batch 29 loss: 0.00020541544654406607\n",
      "  batch 30 loss: 0.00019895468722097576\n",
      "  batch 31 loss: 0.00020911797764711082\n",
      "  batch 32 loss: 0.0002640130987856537\n",
      "  batch 33 loss: 0.0003184826928190887\n",
      "  batch 34 loss: 0.0003566373488865793\n",
      "  batch 35 loss: 0.00019523339869920164\n",
      "  batch 36 loss: 0.00019037933088839054\n",
      "  batch 37 loss: 0.00020359971676953137\n",
      "  batch 38 loss: 0.00019363249884918332\n",
      "  batch 39 loss: 0.00028495703008957207\n",
      "  batch 40 loss: 0.0004196404479444027\n",
      "  batch 41 loss: 0.00019182628602720797\n",
      "  batch 42 loss: 0.0002658087760210037\n",
      "  batch 43 loss: 0.00020907027646899223\n",
      "  batch 44 loss: 0.0001446442329324782\n",
      "  batch 45 loss: 0.0001798933808458969\n",
      "  batch 46 loss: 0.00021095536067150533\n",
      "  batch 47 loss: 0.00022485703811980784\n",
      "  batch 48 loss: 0.0001975860504899174\n",
      "  batch 49 loss: 0.0002622960601001978\n",
      "  batch 50 loss: 0.00025544787058606744\n",
      "  batch 51 loss: 0.0002855025522876531\n",
      "  batch 52 loss: 0.00022068315593060106\n",
      "  batch 53 loss: 0.00022745499154552817\n",
      "  batch 54 loss: 0.00023227528436109424\n",
      "  batch 55 loss: 0.0002732821158133447\n",
      "  batch 56 loss: 0.00019909124239347875\n",
      "  batch 57 loss: 0.00016557522758375853\n",
      "  batch 58 loss: 0.0002638429868966341\n",
      "  batch 59 loss: 0.0003650275757536292\n",
      "  batch 60 loss: 0.0003432107041589916\n",
      "  batch 61 loss: 0.0002110484056174755\n",
      "  batch 62 loss: 0.00024770578602328897\n",
      "  batch 63 loss: 0.0001949671859620139\n",
      "  batch 64 loss: 0.00015465902106370777\n",
      "  batch 65 loss: 0.00017532649508211762\n",
      "  batch 66 loss: 0.00018959194130729884\n",
      "  batch 67 loss: 0.00015767633158247918\n",
      "  batch 68 loss: 0.00021775455388706177\n",
      "  batch 69 loss: 0.0002195443958044052\n",
      "  batch 70 loss: 0.00029993191128596663\n",
      "  batch 71 loss: 0.0001869358675321564\n",
      "  batch 72 loss: 0.00019364990293979645\n",
      "  batch 73 loss: 0.00018140289466828108\n",
      "  batch 74 loss: 0.0002419140364509076\n",
      "  batch 75 loss: 0.0003120190813206136\n",
      "  batch 76 loss: 0.00020528647291939706\n",
      "  batch 77 loss: 0.0002223947667516768\n",
      "  batch 78 loss: 0.0002117954718414694\n",
      "  batch 79 loss: 0.00020019171643070877\n",
      "  batch 80 loss: 0.00022474786965176463\n",
      "  batch 81 loss: 0.00022984683164395392\n",
      "  batch 82 loss: 0.00025971990544348955\n",
      "  batch 83 loss: 0.0002627926878631115\n",
      "  batch 84 loss: 0.00019167375285178423\n",
      "  batch 85 loss: 0.0002066374581772834\n",
      "  batch 86 loss: 0.00024149962700903416\n",
      "  batch 87 loss: 0.0002429783926345408\n",
      "  batch 88 loss: 0.00035579100949689746\n",
      "  batch 89 loss: 0.00033611702383495867\n",
      "  batch 90 loss: 0.00035507167922332883\n",
      "  batch 91 loss: 0.00027985323686152697\n",
      "  batch 92 loss: 0.000240858003962785\n",
      "  batch 93 loss: 0.00026105044526048005\n",
      "  batch 94 loss: 0.00024225079687312245\n",
      "  batch 95 loss: 0.00035107415169477463\n",
      "LOSS train 0.00035107415169477463 valid 0.0007966991979628801\n",
      "LOSS train 0.00035107415169477463 valid 0.0009147058008238673\n",
      "LOSS train 0.00035107415169477463 valid 0.0008660645107738674\n",
      "LOSS train 0.00035107415169477463 valid 0.0008326447568833828\n",
      "LOSS train 0.00035107415169477463 valid 0.0008197324350476265\n",
      "LOSS train 0.00035107415169477463 valid 0.0007706191390752792\n",
      "LOSS train 0.00035107415169477463 valid 0.0008079091785475612\n",
      "LOSS train 0.00035107415169477463 valid 0.0008401271188631654\n",
      "LOSS train 0.00035107415169477463 valid 0.0008174960385076702\n",
      "LOSS train 0.00035107415169477463 valid 0.0008308372343890369\n",
      "LOSS train 0.00035107415169477463 valid 0.0008852812461555004\n",
      "LOSS train 0.00035107415169477463 valid 0.0008843468967825174\n",
      "LOSS train 0.00035107415169477463 valid 0.0008845349075272679\n",
      "LOSS train 0.00035107415169477463 valid 0.0009013420203700662\n",
      "LOSS train 0.00035107415169477463 valid 0.0009765604045242071\n",
      "LOSS train 0.00035107415169477463 valid 0.001007952494546771\n",
      "LOSS train 0.00035107415169477463 valid 0.0010052613215520978\n",
      "LOSS train 0.00035107415169477463 valid 0.00100805819965899\n",
      "LOSS train 0.00035107415169477463 valid 0.0010053772712126374\n",
      "LOSS train 0.00035107415169477463 valid 0.001008508144877851\n",
      "LOSS train 0.00035107415169477463 valid 0.0009982489282265306\n",
      "LOSS train 0.00035107415169477463 valid 0.0010091204894706607\n",
      "LOSS train 0.00035107415169477463 valid 0.0010117948986589909\n",
      "LOSS train 0.00035107415169477463 valid 0.001019086455926299\n",
      "EPOCH 43:\n",
      "  batch 1 loss: 0.0003368621692061424\n",
      "  batch 2 loss: 0.0002018145751208067\n",
      "  batch 3 loss: 0.000300877436529845\n",
      "  batch 4 loss: 0.0002481518604326993\n",
      "  batch 5 loss: 0.000226553893298842\n",
      "  batch 6 loss: 0.0002816173364408314\n",
      "  batch 7 loss: 0.00017683301120996475\n",
      "  batch 8 loss: 0.00020014290930703282\n",
      "  batch 9 loss: 0.00023316859733313322\n",
      "  batch 10 loss: 0.00020196163677610457\n",
      "  batch 11 loss: 0.0003552435082383454\n",
      "  batch 12 loss: 0.00032824522349983454\n",
      "  batch 13 loss: 0.00030757905915379524\n",
      "  batch 14 loss: 0.0004969136789441109\n",
      "  batch 15 loss: 0.0003268808941356838\n",
      "  batch 16 loss: 0.0002700650948099792\n",
      "  batch 17 loss: 0.0002900361141655594\n",
      "  batch 18 loss: 0.0002685419749468565\n",
      "  batch 19 loss: 0.000470860511995852\n",
      "  batch 20 loss: 0.0004058826598338783\n",
      "  batch 21 loss: 0.00040513917338103056\n",
      "  batch 22 loss: 0.00035544484853744507\n",
      "  batch 23 loss: 0.0004198284004814923\n",
      "  batch 24 loss: 0.0004108969878870994\n",
      "  batch 25 loss: 0.00047727039782330394\n",
      "  batch 26 loss: 0.0004674171796068549\n",
      "  batch 27 loss: 0.00024472890072502196\n",
      "  batch 28 loss: 0.0003755174984689802\n",
      "  batch 29 loss: 0.00019088215776719153\n",
      "  batch 30 loss: 0.00017427488637622446\n",
      "  batch 31 loss: 0.00017584848683327436\n",
      "  batch 32 loss: 0.00024733226746320724\n",
      "  batch 33 loss: 0.00031553569715470076\n",
      "  batch 34 loss: 0.0003448885981924832\n",
      "  batch 35 loss: 0.00018800920224748552\n",
      "  batch 36 loss: 0.00018277282651979476\n",
      "  batch 37 loss: 0.00020156637765467167\n",
      "  batch 38 loss: 0.00019412467372603714\n",
      "  batch 39 loss: 0.0002553932718001306\n",
      "  batch 40 loss: 0.000413518282584846\n",
      "  batch 41 loss: 0.00018425263988319784\n",
      "  batch 42 loss: 0.00026854657335206866\n",
      "  batch 43 loss: 0.00021084805484861135\n",
      "  batch 44 loss: 0.00014744442887604237\n",
      "  batch 45 loss: 0.00016982443048618734\n",
      "  batch 46 loss: 0.0002154709363821894\n",
      "  batch 47 loss: 0.0002182789903599769\n",
      "  batch 48 loss: 0.00019138050265610218\n",
      "  batch 49 loss: 0.00024546319036744535\n",
      "  batch 50 loss: 0.0002554169041104615\n",
      "  batch 51 loss: 0.00028115377062931657\n",
      "  batch 52 loss: 0.00021536677377298474\n",
      "  batch 53 loss: 0.00022771995281800628\n",
      "  batch 54 loss: 0.00022166376584209502\n",
      "  batch 55 loss: 0.0002627238864079118\n",
      "  batch 56 loss: 0.00019529458950273693\n",
      "  batch 57 loss: 0.00016375472478102893\n",
      "  batch 58 loss: 0.0002570963406469673\n",
      "  batch 59 loss: 0.0003531137481331825\n",
      "  batch 60 loss: 0.0003315393696539104\n",
      "  batch 61 loss: 0.00020833595772273839\n",
      "  batch 62 loss: 0.00023966017761267722\n",
      "  batch 63 loss: 0.00018168994574807584\n",
      "  batch 64 loss: 0.0001573926128912717\n",
      "  batch 65 loss: 0.0001822460035327822\n",
      "  batch 66 loss: 0.00017320411279797554\n",
      "  batch 67 loss: 0.00015453917148988694\n",
      "  batch 68 loss: 0.00020542398851830512\n",
      "  batch 69 loss: 0.00021423742873594165\n",
      "  batch 70 loss: 0.0002932931238319725\n",
      "  batch 71 loss: 0.0001711516233626753\n",
      "  batch 72 loss: 0.00018889951752498746\n",
      "  batch 73 loss: 0.0001761720923241228\n",
      "  batch 74 loss: 0.0002432892651995644\n",
      "  batch 75 loss: 0.00030963175231590867\n",
      "  batch 76 loss: 0.0002033098426181823\n",
      "  batch 77 loss: 0.00021552073303610086\n",
      "  batch 78 loss: 0.00020661044982261956\n",
      "  batch 79 loss: 0.0001928289420902729\n",
      "  batch 80 loss: 0.00022382814495358616\n",
      "  batch 81 loss: 0.00022415388957597315\n",
      "  batch 82 loss: 0.0002482084964867681\n",
      "  batch 83 loss: 0.00025224825367331505\n",
      "  batch 84 loss: 0.00018848312902264297\n",
      "  batch 85 loss: 0.00019990120199508965\n",
      "  batch 86 loss: 0.00023070770839694887\n",
      "  batch 87 loss: 0.00023733204579912126\n",
      "  batch 88 loss: 0.0003308961750008166\n",
      "  batch 89 loss: 0.0003148170071654022\n",
      "  batch 90 loss: 0.0003474197583273053\n",
      "  batch 91 loss: 0.0002729271072894335\n",
      "  batch 92 loss: 0.0002283482754137367\n",
      "  batch 93 loss: 0.0002450965694151819\n",
      "  batch 94 loss: 0.00024204263172578067\n",
      "  batch 95 loss: 0.0003341791161801666\n",
      "LOSS train 0.0003341791161801666 valid 0.0008322139037773013\n",
      "LOSS train 0.0003341791161801666 valid 0.0009131088736467063\n",
      "LOSS train 0.0003341791161801666 valid 0.0008572045480832458\n",
      "LOSS train 0.0003341791161801666 valid 0.0008254545973613858\n",
      "LOSS train 0.0003341791161801666 valid 0.0008174548274837434\n",
      "LOSS train 0.0003341791161801666 valid 0.0007736962288618088\n",
      "LOSS train 0.0003341791161801666 valid 0.0008183174068108201\n",
      "LOSS train 0.0003341791161801666 valid 0.0008588419295847416\n",
      "LOSS train 0.0003341791161801666 valid 0.0008315446902997792\n",
      "LOSS train 0.0003341791161801666 valid 0.0008468293817713857\n",
      "LOSS train 0.0003341791161801666 valid 0.0008955483208410442\n",
      "LOSS train 0.0003341791161801666 valid 0.0008908548625186086\n",
      "LOSS train 0.0003341791161801666 valid 0.0008850003941915929\n",
      "LOSS train 0.0003341791161801666 valid 0.0009104694472625852\n",
      "LOSS train 0.0003341791161801666 valid 0.0010022080969065428\n",
      "LOSS train 0.0003341791161801666 valid 0.0010368344374001026\n",
      "LOSS train 0.0003341791161801666 valid 0.0010348905343562365\n",
      "LOSS train 0.0003341791161801666 valid 0.001041568350046873\n",
      "LOSS train 0.0003341791161801666 valid 0.0010371652897447348\n",
      "LOSS train 0.0003341791161801666 valid 0.0010365067282691598\n",
      "LOSS train 0.0003341791161801666 valid 0.0010246910387650132\n",
      "LOSS train 0.0003341791161801666 valid 0.0010346528142690659\n",
      "LOSS train 0.0003341791161801666 valid 0.0010391981340944767\n",
      "LOSS train 0.0003341791161801666 valid 0.00106417341157794\n",
      "EPOCH 44:\n",
      "  batch 1 loss: 0.00033242901554331183\n",
      "  batch 2 loss: 0.0001941885711858049\n",
      "  batch 3 loss: 0.00028688987367786467\n",
      "  batch 4 loss: 0.0002431813336443156\n",
      "  batch 5 loss: 0.00021982146427035332\n",
      "  batch 6 loss: 0.00026947190053761005\n",
      "  batch 7 loss: 0.0001690767821855843\n",
      "  batch 8 loss: 0.000184146425453946\n",
      "  batch 9 loss: 0.00023368409893009812\n",
      "  batch 10 loss: 0.00019587452698033303\n",
      "  batch 11 loss: 0.00033981807064265013\n",
      "  batch 12 loss: 0.00031872245017439127\n",
      "  batch 13 loss: 0.0003047215286642313\n",
      "  batch 14 loss: 0.00047995083150453866\n",
      "  batch 15 loss: 0.00030444387812167406\n",
      "  batch 16 loss: 0.00024866056628525257\n",
      "  batch 17 loss: 0.0002933828509412706\n",
      "  batch 18 loss: 0.000270330929197371\n",
      "  batch 19 loss: 0.00047739926958456635\n",
      "  batch 20 loss: 0.0003869483189191669\n",
      "  batch 21 loss: 0.00039486956666223705\n",
      "  batch 22 loss: 0.00033388566225767136\n",
      "  batch 23 loss: 0.00041550048626959324\n",
      "  batch 24 loss: 0.0003856595139950514\n",
      "  batch 25 loss: 0.00046344177098944783\n",
      "  batch 26 loss: 0.0004336964921094477\n",
      "  batch 27 loss: 0.00022261378762777895\n",
      "  batch 28 loss: 0.0003732347977347672\n",
      "  batch 29 loss: 0.00019915033772122115\n",
      "  batch 30 loss: 0.0001833436544984579\n",
      "  batch 31 loss: 0.0001791561080608517\n",
      "  batch 32 loss: 0.00024169313837774098\n",
      "  batch 33 loss: 0.00029749603709205985\n",
      "  batch 34 loss: 0.0003380568523425609\n",
      "  batch 35 loss: 0.00019652806804515421\n",
      "  batch 36 loss: 0.00017652723181527108\n",
      "  batch 37 loss: 0.00019078401965089142\n",
      "  batch 38 loss: 0.00017595042299944907\n",
      "  batch 39 loss: 0.00023662534658797085\n",
      "  batch 40 loss: 0.00039771522278897464\n",
      "  batch 41 loss: 0.00016986500122584403\n",
      "  batch 42 loss: 0.0002430019958410412\n",
      "  batch 43 loss: 0.00018652953440323472\n",
      "  batch 44 loss: 0.00012756932119373232\n",
      "  batch 45 loss: 0.00015457638073712587\n",
      "  batch 46 loss: 0.00019888994575012475\n",
      "  batch 47 loss: 0.0002222667826572433\n",
      "  batch 48 loss: 0.00019375747069716454\n",
      "  batch 49 loss: 0.00024225912056863308\n",
      "  batch 50 loss: 0.0002451502368785441\n",
      "  batch 51 loss: 0.00028688108432106674\n",
      "  batch 52 loss: 0.00021199509501457214\n",
      "  batch 53 loss: 0.00021969847148284316\n",
      "  batch 54 loss: 0.00022151469602249563\n",
      "  batch 55 loss: 0.0002591958618722856\n",
      "  batch 56 loss: 0.0001840003242250532\n",
      "  batch 57 loss: 0.00015840152627788484\n",
      "  batch 58 loss: 0.00024901668075472116\n",
      "  batch 59 loss: 0.0003335655201226473\n",
      "  batch 60 loss: 0.0003231096488889307\n",
      "  batch 61 loss: 0.0002022281987592578\n",
      "  batch 62 loss: 0.0002369362919125706\n",
      "  batch 63 loss: 0.00018198826001025736\n",
      "  batch 64 loss: 0.00015186809469014406\n",
      "  batch 65 loss: 0.00017038692021742463\n",
      "  batch 66 loss: 0.00018374074716120958\n",
      "  batch 67 loss: 0.00015155371511355042\n",
      "  batch 68 loss: 0.0001971775636775419\n",
      "  batch 69 loss: 0.00020313987624831498\n",
      "  batch 70 loss: 0.00027611717814579606\n",
      "  batch 71 loss: 0.0001702414738247171\n",
      "  batch 72 loss: 0.00018481785082258284\n",
      "  batch 73 loss: 0.00016753787349443883\n",
      "  batch 74 loss: 0.0002360200451221317\n",
      "  batch 75 loss: 0.00030986304045654833\n",
      "  batch 76 loss: 0.00020076968939974904\n",
      "  batch 77 loss: 0.00020081277762074023\n",
      "  batch 78 loss: 0.00019997364142909646\n",
      "  batch 79 loss: 0.00018311722669750452\n",
      "  batch 80 loss: 0.00021237922192085534\n",
      "  batch 81 loss: 0.0002130911743734032\n",
      "  batch 82 loss: 0.00024102794122882187\n",
      "  batch 83 loss: 0.0002451685140840709\n",
      "  batch 84 loss: 0.000182358140591532\n",
      "  batch 85 loss: 0.00020138801482971758\n",
      "  batch 86 loss: 0.0002271478733746335\n",
      "  batch 87 loss: 0.00023553299251943827\n",
      "  batch 88 loss: 0.0003174514858983457\n",
      "  batch 89 loss: 0.0003034165711142123\n",
      "  batch 90 loss: 0.0003429593634791672\n",
      "  batch 91 loss: 0.0002601954329293221\n",
      "  batch 92 loss: 0.00022425709175877273\n",
      "  batch 93 loss: 0.00024237243633251637\n",
      "  batch 94 loss: 0.000238450025790371\n",
      "  batch 95 loss: 0.00032428724807687104\n",
      "LOSS train 0.00032428724807687104 valid 0.0008914513746276498\n",
      "LOSS train 0.00032428724807687104 valid 0.00099321105517447\n",
      "LOSS train 0.00032428724807687104 valid 0.0008952554198913276\n",
      "LOSS train 0.00032428724807687104 valid 0.000851642747875303\n",
      "LOSS train 0.00032428724807687104 valid 0.0008426244603469968\n",
      "LOSS train 0.00032428724807687104 valid 0.0007999724475666881\n",
      "LOSS train 0.00032428724807687104 valid 0.0008492112392559648\n",
      "LOSS train 0.00032428724807687104 valid 0.0008905779686756432\n",
      "LOSS train 0.00032428724807687104 valid 0.0008604711620137095\n",
      "LOSS train 0.00032428724807687104 valid 0.0008784212404862046\n",
      "LOSS train 0.00032428724807687104 valid 0.0009394704247824848\n",
      "LOSS train 0.00032428724807687104 valid 0.0009386864840053022\n",
      "LOSS train 0.00032428724807687104 valid 0.0009255622862838209\n",
      "LOSS train 0.00032428724807687104 valid 0.0009475450497120619\n",
      "LOSS train 0.00032428724807687104 valid 0.0010303074959665537\n",
      "LOSS train 0.00032428724807687104 valid 0.001065372838638723\n",
      "LOSS train 0.00032428724807687104 valid 0.001064933487214148\n",
      "LOSS train 0.00032428724807687104 valid 0.0010727308690547943\n",
      "LOSS train 0.00032428724807687104 valid 0.0010691371280699968\n",
      "LOSS train 0.00032428724807687104 valid 0.0010693377116695046\n",
      "LOSS train 0.00032428724807687104 valid 0.0010563336545601487\n",
      "LOSS train 0.00032428724807687104 valid 0.0010641493136063218\n",
      "LOSS train 0.00032428724807687104 valid 0.0010710533242672682\n",
      "LOSS train 0.00032428724807687104 valid 0.0010954466415569186\n",
      "EPOCH 45:\n",
      "  batch 1 loss: 0.00031525857048109174\n",
      "  batch 2 loss: 0.00018162644119001925\n",
      "  batch 3 loss: 0.00028373292298056185\n",
      "  batch 4 loss: 0.00023806007811799645\n",
      "  batch 5 loss: 0.00020808535919059068\n",
      "  batch 6 loss: 0.00025872024707496166\n",
      "  batch 7 loss: 0.00015935522969812155\n",
      "  batch 8 loss: 0.00017694370762910694\n",
      "  batch 9 loss: 0.0002115431852871552\n",
      "  batch 10 loss: 0.00019491580314934254\n",
      "  batch 11 loss: 0.0003276019706390798\n",
      "  batch 12 loss: 0.00029882468516007066\n",
      "  batch 13 loss: 0.0002998734707944095\n",
      "  batch 14 loss: 0.000473168445751071\n",
      "  batch 15 loss: 0.00030178698943927884\n",
      "  batch 16 loss: 0.00024152857076842338\n",
      "  batch 17 loss: 0.0002704017097130418\n",
      "  batch 18 loss: 0.0002587873605079949\n",
      "  batch 19 loss: 0.0004110124136786908\n",
      "  batch 20 loss: 0.0003658894856926054\n",
      "  batch 21 loss: 0.00037788410554639995\n",
      "  batch 22 loss: 0.0003131548292003572\n",
      "  batch 23 loss: 0.0003997838939540088\n",
      "  batch 24 loss: 0.0003710508462972939\n",
      "  batch 25 loss: 0.0004431502893567085\n",
      "  batch 26 loss: 0.0004019892367068678\n",
      "  batch 27 loss: 0.00020235202100593597\n",
      "  batch 28 loss: 0.00034904852509498596\n",
      "  batch 29 loss: 0.00018699245993047953\n",
      "  batch 30 loss: 0.00017352597205899656\n",
      "  batch 31 loss: 0.000159626011736691\n",
      "  batch 32 loss: 0.000233195154578425\n",
      "  batch 33 loss: 0.00026226797490380704\n",
      "  batch 34 loss: 0.00032553263008594513\n",
      "  batch 35 loss: 0.000157199363457039\n",
      "  batch 36 loss: 0.00016534602036699653\n",
      "  batch 37 loss: 0.0001766590285114944\n",
      "  batch 38 loss: 0.00017048361769411713\n",
      "  batch 39 loss: 0.00022918838658370078\n",
      "  batch 40 loss: 0.00037529185647144914\n",
      "  batch 41 loss: 0.00015133192937355489\n",
      "  batch 42 loss: 0.00022642781550530344\n",
      "  batch 43 loss: 0.00017330398259218782\n",
      "  batch 44 loss: 0.00012118315498810261\n",
      "  batch 45 loss: 0.00014525922597385943\n",
      "  batch 46 loss: 0.00017456631758250296\n",
      "  batch 47 loss: 0.00020429158757906407\n",
      "  batch 48 loss: 0.00018811834161169827\n",
      "  batch 49 loss: 0.00022746296599507332\n",
      "  batch 50 loss: 0.00023831285943742841\n",
      "  batch 51 loss: 0.0002822177775669843\n",
      "  batch 52 loss: 0.0002045638975687325\n",
      "  batch 53 loss: 0.00021969160297885537\n",
      "  batch 54 loss: 0.00020547170424833894\n",
      "  batch 55 loss: 0.00025111588183790445\n",
      "  batch 56 loss: 0.00018212271970696747\n",
      "  batch 57 loss: 0.00014770912821404636\n",
      "  batch 58 loss: 0.00022260108380578458\n",
      "  batch 59 loss: 0.00030040403362363577\n",
      "  batch 60 loss: 0.0003172546857967973\n",
      "  batch 61 loss: 0.00019789839279837906\n",
      "  batch 62 loss: 0.00023373916337732226\n",
      "  batch 63 loss: 0.00018520983576308936\n",
      "  batch 64 loss: 0.0001451822172384709\n",
      "  batch 65 loss: 0.00016643141862004995\n",
      "  batch 66 loss: 0.000180871196789667\n",
      "  batch 67 loss: 0.00014052286860533059\n",
      "  batch 68 loss: 0.00018606125377118587\n",
      "  batch 69 loss: 0.00019320414867252111\n",
      "  batch 70 loss: 0.0002777968184091151\n",
      "  batch 71 loss: 0.0001617036759853363\n",
      "  batch 72 loss: 0.0001763813488651067\n",
      "  batch 73 loss: 0.00016326815239153802\n",
      "  batch 74 loss: 0.00022933512809686363\n",
      "  batch 75 loss: 0.00029064895352348685\n",
      "  batch 76 loss: 0.00019320283900015056\n",
      "  batch 77 loss: 0.0001940357033163309\n",
      "  batch 78 loss: 0.00019436990260146558\n",
      "  batch 79 loss: 0.00017981635755859315\n",
      "  batch 80 loss: 0.00020578131079673767\n",
      "  batch 81 loss: 0.00020908612350467592\n",
      "  batch 82 loss: 0.00023407701519317925\n",
      "  batch 83 loss: 0.00023176774266175926\n",
      "  batch 84 loss: 0.00017598508566152304\n",
      "  batch 85 loss: 0.0001919685018947348\n",
      "  batch 86 loss: 0.00022091620485298336\n",
      "  batch 87 loss: 0.00022027049271855503\n",
      "  batch 88 loss: 0.0003034965484403074\n",
      "  batch 89 loss: 0.00029015468317084014\n",
      "  batch 90 loss: 0.00032583228312432766\n",
      "  batch 91 loss: 0.00025431386893615127\n",
      "  batch 92 loss: 0.00021637762256432325\n",
      "  batch 93 loss: 0.0002340590872336179\n",
      "  batch 94 loss: 0.0002301861677551642\n",
      "  batch 95 loss: 0.0003154811856802553\n",
      "LOSS train 0.0003154811856802553 valid 0.0008105995948426425\n",
      "LOSS train 0.0003154811856802553 valid 0.0009439531713724136\n",
      "LOSS train 0.0003154811856802553 valid 0.0009233194286935031\n",
      "LOSS train 0.0003154811856802553 valid 0.0009151996928267181\n",
      "LOSS train 0.0003154811856802553 valid 0.0009004940511658788\n",
      "LOSS train 0.0003154811856802553 valid 0.0008441065438091755\n",
      "LOSS train 0.0003154811856802553 valid 0.0008797344053164124\n",
      "LOSS train 0.0003154811856802553 valid 0.0009185641538351774\n",
      "LOSS train 0.0003154811856802553 valid 0.0008918470120988786\n",
      "LOSS train 0.0003154811856802553 valid 0.0009016040130518377\n",
      "LOSS train 0.0003154811856802553 valid 0.0009404150187037885\n",
      "LOSS train 0.0003154811856802553 valid 0.0009347909362986684\n",
      "LOSS train 0.0003154811856802553 valid 0.0009350049658678472\n",
      "LOSS train 0.0003154811856802553 valid 0.0009509313385933638\n",
      "LOSS train 0.0003154811856802553 valid 0.001012721797451377\n",
      "LOSS train 0.0003154811856802553 valid 0.0010444314684718847\n",
      "LOSS train 0.0003154811856802553 valid 0.0010482415091246367\n",
      "LOSS train 0.0003154811856802553 valid 0.0010503333760425448\n",
      "LOSS train 0.0003154811856802553 valid 0.0010461065685376525\n",
      "LOSS train 0.0003154811856802553 valid 0.0010475156595930457\n",
      "LOSS train 0.0003154811856802553 valid 0.001038314658217132\n",
      "LOSS train 0.0003154811856802553 valid 0.0010508006671443582\n",
      "LOSS train 0.0003154811856802553 valid 0.0010538441129028797\n",
      "LOSS train 0.0003154811856802553 valid 0.0010557849891483784\n",
      "EPOCH 46:\n",
      "  batch 1 loss: 0.0003075585118494928\n",
      "  batch 2 loss: 0.0001731956726871431\n",
      "  batch 3 loss: 0.00026904718833975494\n",
      "  batch 4 loss: 0.00022439241001848131\n",
      "  batch 5 loss: 0.00019807071657851338\n",
      "  batch 6 loss: 0.0002493513748049736\n",
      "  batch 7 loss: 0.00014645239571109414\n",
      "  batch 8 loss: 0.00016998873616103083\n",
      "  batch 9 loss: 0.0002086524327751249\n",
      "  batch 10 loss: 0.00017931402544490993\n",
      "  batch 11 loss: 0.0003208559937775135\n",
      "  batch 12 loss: 0.0002888190501835197\n",
      "  batch 13 loss: 0.00029017325141467154\n",
      "  batch 14 loss: 0.0004575408238451928\n",
      "  batch 15 loss: 0.0002972688525915146\n",
      "  batch 16 loss: 0.0002317185135325417\n",
      "  batch 17 loss: 0.0002589359355624765\n",
      "  batch 18 loss: 0.0002378062781644985\n",
      "  batch 19 loss: 0.0004065711982548237\n",
      "  batch 20 loss: 0.00035109888995066285\n",
      "  batch 21 loss: 0.0003668004064820707\n",
      "  batch 22 loss: 0.0003167491522617638\n",
      "  batch 23 loss: 0.0003660284273792058\n",
      "  batch 24 loss: 0.00036975854891352355\n",
      "  batch 25 loss: 0.00044013181468471885\n",
      "  batch 26 loss: 0.0004116493510082364\n",
      "  batch 27 loss: 0.000210385158425197\n",
      "  batch 28 loss: 0.0003404317540116608\n",
      "  batch 29 loss: 0.00018430163618177176\n",
      "  batch 30 loss: 0.00017377783660776913\n",
      "  batch 31 loss: 0.00016049950500018895\n",
      "  batch 32 loss: 0.00023405638057738543\n",
      "  batch 33 loss: 0.000250059412792325\n",
      "  batch 34 loss: 0.0003250315203331411\n",
      "  batch 35 loss: 0.00015282523236237466\n",
      "  batch 36 loss: 0.00016955465252976865\n",
      "  batch 37 loss: 0.00017886202840600163\n",
      "  batch 38 loss: 0.0001775768760126084\n",
      "  batch 39 loss: 0.0002306250826222822\n",
      "  batch 40 loss: 0.0003673194441944361\n",
      "  batch 41 loss: 0.00013884346117265522\n",
      "  batch 42 loss: 0.00019318159320391715\n",
      "  batch 43 loss: 0.00016652361955493689\n",
      "  batch 44 loss: 0.000113819376565516\n",
      "  batch 45 loss: 0.00014871162420604378\n",
      "  batch 46 loss: 0.00018953172548208386\n",
      "  batch 47 loss: 0.00020043626136612147\n",
      "  batch 48 loss: 0.00017965343431569636\n",
      "  batch 49 loss: 0.00022404467745218426\n",
      "  batch 50 loss: 0.00023147559841163456\n",
      "  batch 51 loss: 0.00027524010511115193\n",
      "  batch 52 loss: 0.00019477277237456292\n",
      "  batch 53 loss: 0.000213202292798087\n",
      "  batch 54 loss: 0.00019940672791562974\n",
      "  batch 55 loss: 0.0002469887549523264\n",
      "  batch 56 loss: 0.0001772854448063299\n",
      "  batch 57 loss: 0.00014257035218179226\n",
      "  batch 58 loss: 0.00022193838958628476\n",
      "  batch 59 loss: 0.00031519500771537423\n",
      "  batch 60 loss: 0.0003015071852132678\n",
      "  batch 61 loss: 0.00018847410683520138\n",
      "  batch 62 loss: 0.00023719544697087258\n",
      "  batch 63 loss: 0.0001789802045095712\n",
      "  batch 64 loss: 0.00013823967310599983\n",
      "  batch 65 loss: 0.0001663283328525722\n",
      "  batch 66 loss: 0.00016665883595123887\n",
      "  batch 67 loss: 0.00015336510841734707\n",
      "  batch 68 loss: 0.00018817084492184222\n",
      "  batch 69 loss: 0.00018724030815064907\n",
      "  batch 70 loss: 0.00027243365184403956\n",
      "  batch 71 loss: 0.0001585275458637625\n",
      "  batch 72 loss: 0.0001756496640155092\n",
      "  batch 73 loss: 0.00016213409253396094\n",
      "  batch 74 loss: 0.00023191185027826577\n",
      "  batch 75 loss: 0.00029112413176335394\n",
      "  batch 76 loss: 0.00019409757805988193\n",
      "  batch 77 loss: 0.00019689358305186033\n",
      "  batch 78 loss: 0.00018828794418368489\n",
      "  batch 79 loss: 0.00017383186786901206\n",
      "  batch 80 loss: 0.00020374447922222316\n",
      "  batch 81 loss: 0.0002040154067799449\n",
      "  batch 82 loss: 0.00022826345229987055\n",
      "  batch 83 loss: 0.00023074928321875632\n",
      "  batch 84 loss: 0.0001710176293272525\n",
      "  batch 85 loss: 0.00018555816495791078\n",
      "  batch 86 loss: 0.00021376044605858624\n",
      "  batch 87 loss: 0.00021290811127983034\n",
      "  batch 88 loss: 0.00029121461557224393\n",
      "  batch 89 loss: 0.000292793323751539\n",
      "  batch 90 loss: 0.00032825605012476444\n",
      "  batch 91 loss: 0.00024485221365466714\n",
      "  batch 92 loss: 0.00020677766588050872\n",
      "  batch 93 loss: 0.0002203644107794389\n",
      "  batch 94 loss: 0.0002293813886353746\n",
      "  batch 95 loss: 0.000316649122396484\n",
      "LOSS train 0.000316649122396484 valid 0.0009944887133315206\n",
      "LOSS train 0.000316649122396484 valid 0.0010978365316987038\n",
      "LOSS train 0.000316649122396484 valid 0.0009769394528120756\n",
      "LOSS train 0.000316649122396484 valid 0.0009399723494425416\n",
      "LOSS train 0.000316649122396484 valid 0.0009276402415707707\n",
      "LOSS train 0.000316649122396484 valid 0.0008892840705811977\n",
      "LOSS train 0.000316649122396484 valid 0.0009468835778534412\n",
      "LOSS train 0.000316649122396484 valid 0.000996441813185811\n",
      "LOSS train 0.000316649122396484 valid 0.0009588425164110959\n",
      "LOSS train 0.000316649122396484 valid 0.0009764489950612187\n",
      "LOSS train 0.000316649122396484 valid 0.0010390038369223475\n",
      "LOSS train 0.000316649122396484 valid 0.0010324541945010424\n",
      "LOSS train 0.000316649122396484 valid 0.001016085036098957\n",
      "LOSS train 0.000316649122396484 valid 0.0010439687175676227\n",
      "LOSS train 0.000316649122396484 valid 0.0011453115148469806\n",
      "LOSS train 0.000316649122396484 valid 0.001183298067189753\n",
      "LOSS train 0.000316649122396484 valid 0.001181330531835556\n",
      "LOSS train 0.000316649122396484 valid 0.0011937827803194523\n",
      "LOSS train 0.000316649122396484 valid 0.0011949008330702782\n",
      "LOSS train 0.000316649122396484 valid 0.001201306818984449\n",
      "LOSS train 0.000316649122396484 valid 0.001188597991131246\n",
      "LOSS train 0.000316649122396484 valid 0.0011990985367447138\n",
      "LOSS train 0.000316649122396484 valid 0.0012098881416022778\n",
      "LOSS train 0.000316649122396484 valid 0.001229599816724658\n",
      "EPOCH 47:\n",
      "  batch 1 loss: 0.00029961118707433343\n",
      "  batch 2 loss: 0.0001683012960711494\n",
      "  batch 3 loss: 0.00027985754422843456\n",
      "  batch 4 loss: 0.00022530191927216947\n",
      "  batch 5 loss: 0.00019254496146459132\n",
      "  batch 6 loss: 0.00022181356325745583\n",
      "  batch 7 loss: 0.00014213609392754734\n",
      "  batch 8 loss: 0.00016143464017659426\n",
      "  batch 9 loss: 0.0002046025765594095\n",
      "  batch 10 loss: 0.00016605755081400275\n",
      "  batch 11 loss: 0.00032669524080120027\n",
      "  batch 12 loss: 0.0002966962638311088\n",
      "  batch 13 loss: 0.00028714517247863114\n",
      "  batch 14 loss: 0.0004472667060326785\n",
      "  batch 15 loss: 0.0002968159387819469\n",
      "  batch 16 loss: 0.0002180141455028206\n",
      "  batch 17 loss: 0.0002519541885703802\n",
      "  batch 18 loss: 0.00022667937446385622\n",
      "  batch 19 loss: 0.0003961539769079536\n",
      "  batch 20 loss: 0.0003527499793563038\n",
      "  batch 21 loss: 0.0003625006356742233\n",
      "  batch 22 loss: 0.00028211844619363546\n",
      "  batch 23 loss: 0.00034855733974836767\n",
      "  batch 24 loss: 0.00034980836790055037\n",
      "  batch 25 loss: 0.0004038907936774194\n",
      "  batch 26 loss: 0.00041154224891215563\n",
      "  batch 27 loss: 0.00020501908147707582\n",
      "  batch 28 loss: 0.00032198053668253124\n",
      "  batch 29 loss: 0.00017890764866024256\n",
      "  batch 30 loss: 0.00016989110736176372\n",
      "  batch 31 loss: 0.0001509353460278362\n",
      "  batch 32 loss: 0.00022031538537703454\n",
      "  batch 33 loss: 0.0002565147588029504\n",
      "  batch 34 loss: 0.00030384049750864506\n",
      "  batch 35 loss: 0.0001497956400271505\n",
      "  batch 36 loss: 0.00016270596825052053\n",
      "  batch 37 loss: 0.0001739857834763825\n",
      "  batch 38 loss: 0.0001572364562889561\n",
      "  batch 39 loss: 0.0002316333557246253\n",
      "  batch 40 loss: 0.0003722913679666817\n",
      "  batch 41 loss: 0.0001311353116761893\n",
      "  batch 42 loss: 0.00017610922805033624\n",
      "  batch 43 loss: 0.00016762664017733186\n",
      "  batch 44 loss: 0.00010812614345923066\n",
      "  batch 45 loss: 0.00014536388334818184\n",
      "  batch 46 loss: 0.00017420659423805773\n",
      "  batch 47 loss: 0.0001901896030176431\n",
      "  batch 48 loss: 0.0001860542397480458\n",
      "  batch 49 loss: 0.00022346193145494908\n",
      "  batch 50 loss: 0.0002332210133317858\n",
      "  batch 51 loss: 0.00026163479196839035\n",
      "  batch 52 loss: 0.0001905422395793721\n",
      "  batch 53 loss: 0.00021579806343652308\n",
      "  batch 54 loss: 0.00019861123291775584\n",
      "  batch 55 loss: 0.00024426833260804415\n",
      "  batch 56 loss: 0.00017871111049316823\n",
      "  batch 57 loss: 0.0001403316273353994\n",
      "  batch 58 loss: 0.00020438767387531698\n",
      "  batch 59 loss: 0.00026782293571159244\n",
      "  batch 60 loss: 0.0002891116891987622\n",
      "  batch 61 loss: 0.00019134856120217592\n",
      "  batch 62 loss: 0.00021316524362191558\n",
      "  batch 63 loss: 0.00016474208678118885\n",
      "  batch 64 loss: 0.00012889085337519646\n",
      "  batch 65 loss: 0.0001581008400535211\n",
      "  batch 66 loss: 0.0001535426126793027\n",
      "  batch 67 loss: 0.00012812967179343104\n",
      "  batch 68 loss: 0.00017027102876454592\n",
      "  batch 69 loss: 0.00017898432270158082\n",
      "  batch 70 loss: 0.00025569560239091516\n",
      "  batch 71 loss: 0.00015105342026799917\n",
      "  batch 72 loss: 0.00016214109200518578\n",
      "  batch 73 loss: 0.00015330323367379606\n",
      "  batch 74 loss: 0.00022512685973197222\n",
      "  batch 75 loss: 0.00028217723593115807\n",
      "  batch 76 loss: 0.00017820011998992413\n",
      "  batch 77 loss: 0.00018273867317475379\n",
      "  batch 78 loss: 0.00018015544628724456\n",
      "  batch 79 loss: 0.0001748071808833629\n",
      "  batch 80 loss: 0.00021500099683180451\n",
      "  batch 81 loss: 0.00020409515127539635\n",
      "  batch 82 loss: 0.0002317728940397501\n",
      "  batch 83 loss: 0.0002216859138570726\n",
      "  batch 84 loss: 0.00016416498692706227\n",
      "  batch 85 loss: 0.0001792725670384243\n",
      "  batch 86 loss: 0.00021270863362587988\n",
      "  batch 87 loss: 0.00021423067664727569\n",
      "  batch 88 loss: 0.0002820811059791595\n",
      "  batch 89 loss: 0.00027341212262399495\n",
      "  batch 90 loss: 0.00031129299895837903\n",
      "  batch 91 loss: 0.00023991658235900104\n",
      "  batch 92 loss: 0.00020041005336679518\n",
      "  batch 93 loss: 0.0002253064449178055\n",
      "  batch 94 loss: 0.00022107912809588015\n",
      "  batch 95 loss: 0.000312312098685652\n",
      "LOSS train 0.000312312098685652 valid 0.0008557193796150386\n",
      "LOSS train 0.000312312098685652 valid 0.000967620755545795\n",
      "LOSS train 0.000312312098685652 valid 0.000911833019927144\n",
      "LOSS train 0.000312312098685652 valid 0.0008764365338720381\n",
      "LOSS train 0.000312312098685652 valid 0.0008649220690131187\n",
      "LOSS train 0.000312312098685652 valid 0.0008150297217071056\n",
      "LOSS train 0.000312312098685652 valid 0.0008560048299841583\n",
      "LOSS train 0.000312312098685652 valid 0.0008921286789700389\n",
      "LOSS train 0.000312312098685652 valid 0.0008643307955935597\n",
      "LOSS train 0.000312312098685652 valid 0.0008812945452518761\n",
      "LOSS train 0.000312312098685652 valid 0.0009309137240052223\n",
      "LOSS train 0.000312312098685652 valid 0.0009291776805184782\n",
      "LOSS train 0.000312312098685652 valid 0.0009224990499205887\n",
      "LOSS train 0.000312312098685652 valid 0.0009472217643633485\n",
      "LOSS train 0.000312312098685652 valid 0.0010157576762139797\n",
      "LOSS train 0.000312312098685652 valid 0.001049754791893065\n",
      "LOSS train 0.000312312098685652 valid 0.0010528714628890157\n",
      "LOSS train 0.000312312098685652 valid 0.0010587216820567846\n",
      "LOSS train 0.000312312098685652 valid 0.0010567773133516312\n",
      "LOSS train 0.000312312098685652 valid 0.0010592967737466097\n",
      "LOSS train 0.000312312098685652 valid 0.00104901019949466\n",
      "LOSS train 0.000312312098685652 valid 0.0010622957488521934\n",
      "LOSS train 0.000312312098685652 valid 0.0010637860978022218\n",
      "LOSS train 0.000312312098685652 valid 0.0010773076210170984\n",
      "EPOCH 48:\n",
      "  batch 1 loss: 0.00029014202300459146\n",
      "  batch 2 loss: 0.0001667800243012607\n",
      "  batch 3 loss: 0.0002703920763451606\n",
      "  batch 4 loss: 0.00022512589930556715\n",
      "  batch 5 loss: 0.00018368579912930727\n",
      "  batch 6 loss: 0.0002005088608711958\n",
      "  batch 7 loss: 0.00013627289445139468\n",
      "  batch 8 loss: 0.0001584119163453579\n",
      "  batch 9 loss: 0.00019963456725236028\n",
      "  batch 10 loss: 0.00016286331810988486\n",
      "  batch 11 loss: 0.00034900219179689884\n",
      "  batch 12 loss: 0.00027022528229281306\n",
      "  batch 13 loss: 0.0002780920476652682\n",
      "  batch 14 loss: 0.0004253006190992892\n",
      "  batch 15 loss: 0.00029710735543631017\n",
      "  batch 16 loss: 0.00022595049813389778\n",
      "  batch 17 loss: 0.0002693177666515112\n",
      "  batch 18 loss: 0.00022108093253336847\n",
      "  batch 19 loss: 0.00038740813033655286\n",
      "  batch 20 loss: 0.0003309092135168612\n",
      "  batch 21 loss: 0.00034991686698049307\n",
      "  batch 22 loss: 0.0002769542916212231\n",
      "  batch 23 loss: 0.0003486170608084649\n",
      "  batch 24 loss: 0.0003623654483817518\n",
      "  batch 25 loss: 0.0003984067589044571\n",
      "  batch 26 loss: 0.0003808154142461717\n",
      "  batch 27 loss: 0.00019368575885891914\n",
      "  batch 28 loss: 0.0003091749968007207\n",
      "  batch 29 loss: 0.00016817801224533468\n",
      "  batch 30 loss: 0.00015834925579838455\n",
      "  batch 31 loss: 0.00014115693920757622\n",
      "  batch 32 loss: 0.0002238874149043113\n",
      "  batch 33 loss: 0.0002355441392865032\n",
      "  batch 34 loss: 0.00031055082217790186\n",
      "  batch 35 loss: 0.00013837742153555155\n",
      "  batch 36 loss: 0.0001597102964296937\n",
      "  batch 37 loss: 0.00016319204587489367\n",
      "  batch 38 loss: 0.00015330096357502043\n",
      "  batch 39 loss: 0.00021981207828503102\n",
      "  batch 40 loss: 0.000355744909029454\n",
      "  batch 41 loss: 0.00013529846910387278\n",
      "  batch 42 loss: 0.00018822343554347754\n",
      "  batch 43 loss: 0.00016162512474693358\n",
      "  batch 44 loss: 0.00011504886788316071\n",
      "  batch 45 loss: 0.00014831223234068602\n",
      "  batch 46 loss: 0.00017033162293955684\n",
      "  batch 47 loss: 0.00019020818581338972\n",
      "  batch 48 loss: 0.00018183870997745544\n",
      "  batch 49 loss: 0.00021291832672432065\n",
      "  batch 50 loss: 0.00024161952023860067\n",
      "  batch 51 loss: 0.0002309997653355822\n",
      "  batch 52 loss: 0.00020461963140405715\n",
      "  batch 53 loss: 0.0002219986345153302\n",
      "  batch 54 loss: 0.00018980391905643046\n",
      "  batch 55 loss: 0.0002427801227895543\n",
      "  batch 56 loss: 0.00017117627430707216\n",
      "  batch 57 loss: 0.00014957334497012198\n",
      "  batch 58 loss: 0.00022187517606653273\n",
      "  batch 59 loss: 0.0003328113234601915\n",
      "  batch 60 loss: 0.000290399300865829\n",
      "  batch 61 loss: 0.00018887566693592817\n",
      "  batch 62 loss: 0.00022926621022634208\n",
      "  batch 63 loss: 0.00017338448378723115\n",
      "  batch 64 loss: 0.00013158748333808035\n",
      "  batch 65 loss: 0.00016058718028943986\n",
      "  batch 66 loss: 0.00015107532090041786\n",
      "  batch 67 loss: 0.00012523980694822967\n",
      "  batch 68 loss: 0.00017188799392897636\n",
      "  batch 69 loss: 0.00017726942314766347\n",
      "  batch 70 loss: 0.00025711453054100275\n",
      "  batch 71 loss: 0.00015620127669535577\n",
      "  batch 72 loss: 0.00015761614486109465\n",
      "  batch 73 loss: 0.00015306015848182142\n",
      "  batch 74 loss: 0.0002054142241831869\n",
      "  batch 75 loss: 0.00027812912594527006\n",
      "  batch 76 loss: 0.00017792708240449429\n",
      "  batch 77 loss: 0.00018360519607085735\n",
      "  batch 78 loss: 0.00017847771232482046\n",
      "  batch 79 loss: 0.00016767327906563878\n",
      "  batch 80 loss: 0.00018982947221957147\n",
      "  batch 81 loss: 0.00019280638662166893\n",
      "  batch 82 loss: 0.00021564413327723742\n",
      "  batch 83 loss: 0.00021764259145129472\n",
      "  batch 84 loss: 0.00016881671035662293\n",
      "  batch 85 loss: 0.00017732565174810588\n",
      "  batch 86 loss: 0.00020365348609630018\n",
      "  batch 87 loss: 0.00020643281459342688\n",
      "  batch 88 loss: 0.00028935755835846066\n",
      "  batch 89 loss: 0.0002579942811280489\n",
      "  batch 90 loss: 0.0003071730607189238\n",
      "  batch 91 loss: 0.00022955930035095662\n",
      "  batch 92 loss: 0.0002021938271354884\n",
      "  batch 93 loss: 0.0002199297014158219\n",
      "  batch 94 loss: 0.00021787475270684808\n",
      "  batch 95 loss: 0.00031660840613767505\n",
      "LOSS train 0.00031660840613767505 valid 0.0009238831698894501\n",
      "LOSS train 0.00031660840613767505 valid 0.001052854466252029\n",
      "LOSS train 0.00031660840613767505 valid 0.0010512362932786345\n",
      "LOSS train 0.00031660840613767505 valid 0.0010431015398353338\n",
      "LOSS train 0.00031660840613767505 valid 0.0010402488987892866\n",
      "LOSS train 0.00031660840613767505 valid 0.0009818398393690586\n",
      "LOSS train 0.00031660840613767505 valid 0.001007863087579608\n",
      "LOSS train 0.00031660840613767505 valid 0.0010316846892237663\n",
      "LOSS train 0.00031660840613767505 valid 0.001005729311145842\n",
      "LOSS train 0.00031660840613767505 valid 0.001019799499772489\n",
      "LOSS train 0.00031660840613767505 valid 0.0010582072427496314\n",
      "LOSS train 0.00031660840613767505 valid 0.001050049439072609\n",
      "LOSS train 0.00031660840613767505 valid 0.0010580564849078655\n",
      "LOSS train 0.00031660840613767505 valid 0.0010739687131717801\n",
      "LOSS train 0.00031660840613767505 valid 0.0011169112985953689\n",
      "LOSS train 0.00031660840613767505 valid 0.0011614493560045958\n",
      "LOSS train 0.00031660840613767505 valid 0.0011699240421876311\n",
      "LOSS train 0.00031660840613767505 valid 0.0011718438472598791\n",
      "LOSS train 0.00031660840613767505 valid 0.001168025191873312\n",
      "LOSS train 0.00031660840613767505 valid 0.0011704134522005916\n",
      "LOSS train 0.00031660840613767505 valid 0.0011591732036322355\n",
      "LOSS train 0.00031660840613767505 valid 0.0011702814372256398\n",
      "LOSS train 0.00031660840613767505 valid 0.0011683459160849452\n",
      "LOSS train 0.00031660840613767505 valid 0.0011625339975580573\n",
      "EPOCH 49:\n",
      "  batch 1 loss: 0.0002753457520157099\n",
      "  batch 2 loss: 0.00016662158304825425\n",
      "  batch 3 loss: 0.0002679253520909697\n",
      "  batch 4 loss: 0.00022113756858743727\n",
      "  batch 5 loss: 0.00018400319095235318\n",
      "  batch 6 loss: 0.00019068652181886137\n",
      "  batch 7 loss: 0.00014125977759249508\n",
      "  batch 8 loss: 0.00015862406871747226\n",
      "  batch 9 loss: 0.0002130727662006393\n",
      "  batch 10 loss: 0.00013530557043850422\n",
      "  batch 11 loss: 0.0003109648823738098\n",
      "  batch 12 loss: 0.0002550825593061745\n",
      "  batch 13 loss: 0.00027716346085071564\n",
      "  batch 14 loss: 0.0004252860671840608\n",
      "  batch 15 loss: 0.000280339561868459\n",
      "  batch 16 loss: 0.00020601079449988902\n",
      "  batch 17 loss: 0.00026137923123314977\n",
      "  batch 18 loss: 0.0002114761300617829\n",
      "  batch 19 loss: 0.000371936010196805\n",
      "  batch 20 loss: 0.0003133008722215891\n",
      "  batch 21 loss: 0.00033679467742331326\n",
      "  batch 22 loss: 0.0002561809669714421\n",
      "  batch 23 loss: 0.0003254824550822377\n",
      "  batch 24 loss: 0.0003255771298427135\n",
      "  batch 25 loss: 0.0003681185480672866\n",
      "  batch 26 loss: 0.00036733897286467254\n",
      "  batch 27 loss: 0.0001923314994201064\n",
      "  batch 28 loss: 0.00030255771707743406\n",
      "  batch 29 loss: 0.00016336803673766553\n",
      "  batch 30 loss: 0.00016638258239254355\n",
      "  batch 31 loss: 0.00014582733274437487\n",
      "  batch 32 loss: 0.00020951658370904624\n",
      "  batch 33 loss: 0.0002375359763391316\n",
      "  batch 34 loss: 0.0003035506815649569\n",
      "  batch 35 loss: 0.00013019454490859061\n",
      "  batch 36 loss: 0.00015651887224521488\n",
      "  batch 37 loss: 0.00016685169248376042\n",
      "  batch 38 loss: 0.00015709231956861913\n",
      "  batch 39 loss: 0.00021698202181141824\n",
      "  batch 40 loss: 0.00034991680877283216\n",
      "  batch 41 loss: 0.00011931922199437395\n",
      "  batch 42 loss: 0.0001757148711476475\n",
      "  batch 43 loss: 0.00015760136011522263\n",
      "  batch 44 loss: 0.00010501890210434794\n",
      "  batch 45 loss: 0.00013213891361374408\n",
      "  batch 46 loss: 0.00014852128515485674\n",
      "  batch 47 loss: 0.00018358574016019702\n",
      "  batch 48 loss: 0.0001703927991911769\n",
      "  batch 49 loss: 0.00021026852482464164\n",
      "  batch 50 loss: 0.00022705804440192878\n",
      "  batch 51 loss: 0.00022055202862247825\n",
      "  batch 52 loss: 0.0001852812711149454\n",
      "  batch 53 loss: 0.0002077188401017338\n",
      "  batch 54 loss: 0.0001945692638400942\n",
      "  batch 55 loss: 0.0002310390118509531\n",
      "  batch 56 loss: 0.00017005676636472344\n",
      "  batch 57 loss: 0.00014069245662540197\n",
      "  batch 58 loss: 0.00018925793119706213\n",
      "  batch 59 loss: 0.0002586805494502187\n",
      "  batch 60 loss: 0.0002744990633800626\n",
      "  batch 61 loss: 0.00016700979904271662\n",
      "  batch 62 loss: 0.0002083862345898524\n",
      "  batch 63 loss: 0.0001564901613164693\n",
      "  batch 64 loss: 0.0001259328710148111\n",
      "  batch 65 loss: 0.00014961499255150557\n",
      "  batch 66 loss: 0.00015667038678657264\n",
      "  batch 67 loss: 0.00012636100291274488\n",
      "  batch 68 loss: 0.00017486969591118395\n",
      "  batch 69 loss: 0.00017745757941156626\n",
      "  batch 70 loss: 0.0002465991419740021\n",
      "  batch 71 loss: 0.00015095289563760161\n",
      "  batch 72 loss: 0.00016358535503968596\n",
      "  batch 73 loss: 0.00015399782569147646\n",
      "  batch 74 loss: 0.00020864824182353914\n",
      "  batch 75 loss: 0.000269683834630996\n",
      "  batch 76 loss: 0.00017689360538497567\n",
      "  batch 77 loss: 0.00017437670612707734\n",
      "  batch 78 loss: 0.00017573742661625147\n",
      "  batch 79 loss: 0.00015978136798366904\n",
      "  batch 80 loss: 0.00019348681962583214\n",
      "  batch 81 loss: 0.0001907208643388003\n",
      "  batch 82 loss: 0.00021618837490677834\n",
      "  batch 83 loss: 0.00021549903613049537\n",
      "  batch 84 loss: 0.00016017700545489788\n",
      "  batch 85 loss: 0.00016812642570585012\n",
      "  batch 86 loss: 0.00019994260219391435\n",
      "  batch 87 loss: 0.00019505647651385516\n",
      "  batch 88 loss: 0.0002726581587921828\n",
      "  batch 89 loss: 0.0002456654328852892\n",
      "  batch 90 loss: 0.0002916111843660474\n",
      "  batch 91 loss: 0.00021662430663127452\n",
      "  batch 92 loss: 0.0001947119744727388\n",
      "  batch 93 loss: 0.00020828749984502792\n",
      "  batch 94 loss: 0.0002116979449056089\n",
      "  batch 95 loss: 0.0003102876362390816\n",
      "LOSS train 0.0003102876362390816 valid 0.0009638171759434044\n",
      "LOSS train 0.0003102876362390816 valid 0.0010858435416594148\n",
      "LOSS train 0.0003102876362390816 valid 0.0009647073457017541\n",
      "LOSS train 0.0003102876362390816 valid 0.0009222639491781592\n",
      "LOSS train 0.0003102876362390816 valid 0.0009026283514685929\n",
      "LOSS train 0.0003102876362390816 valid 0.0008556697866879404\n",
      "LOSS train 0.0003102876362390816 valid 0.0009065300109796226\n",
      "LOSS train 0.0003102876362390816 valid 0.0009480051230639219\n",
      "LOSS train 0.0003102876362390816 valid 0.0009108153753913939\n",
      "LOSS train 0.0003102876362390816 valid 0.0009323350386694074\n",
      "LOSS train 0.0003102876362390816 valid 0.0009839888662099838\n",
      "LOSS train 0.0003102876362390816 valid 0.0009877027478069067\n",
      "LOSS train 0.0003102876362390816 valid 0.0009719625231809914\n",
      "LOSS train 0.0003102876362390816 valid 0.0009925507474690676\n",
      "LOSS train 0.0003102876362390816 valid 0.0010745557956397533\n",
      "LOSS train 0.0003102876362390816 valid 0.0011147375917062163\n",
      "LOSS train 0.0003102876362390816 valid 0.0011159995337948203\n",
      "LOSS train 0.0003102876362390816 valid 0.0011245438363403082\n",
      "LOSS train 0.0003102876362390816 valid 0.0011224154150113463\n",
      "LOSS train 0.0003102876362390816 valid 0.0011267128866165876\n",
      "LOSS train 0.0003102876362390816 valid 0.0011161781148985028\n",
      "LOSS train 0.0003102876362390816 valid 0.0011275004362687469\n",
      "LOSS train 0.0003102876362390816 valid 0.0011342596262693405\n",
      "LOSS train 0.0003102876362390816 valid 0.001148683251813054\n",
      "EPOCH 50:\n",
      "  batch 1 loss: 0.0002784825337585062\n",
      "  batch 2 loss: 0.00015266952686943114\n",
      "  batch 3 loss: 0.00025567010743543506\n",
      "  batch 4 loss: 0.0002085555752273649\n",
      "  batch 5 loss: 0.00017300553736276925\n",
      "  batch 6 loss: 0.0002015533100347966\n",
      "  batch 7 loss: 0.00012349363532848656\n",
      "  batch 8 loss: 0.00015974306734278798\n",
      "  batch 9 loss: 0.00020960284746252\n",
      "  batch 10 loss: 0.00014423116226680577\n",
      "  batch 11 loss: 0.0002878509694710374\n",
      "  batch 12 loss: 0.0002626319765113294\n",
      "  batch 13 loss: 0.00029250624356791377\n",
      "  batch 14 loss: 0.000440764706581831\n",
      "  batch 15 loss: 0.00027896653045900166\n",
      "  batch 16 loss: 0.00022173950856085867\n",
      "  batch 17 loss: 0.0002469588362146169\n",
      "  batch 18 loss: 0.00020926048455294222\n",
      "  batch 19 loss: 0.0003621285723056644\n",
      "  batch 20 loss: 0.00031506732921116054\n",
      "  batch 21 loss: 0.0003435763355810195\n",
      "  batch 22 loss: 0.00026495649944990873\n",
      "  batch 23 loss: 0.00034028978552669287\n",
      "  batch 24 loss: 0.00033819430973380804\n",
      "  batch 25 loss: 0.00037911219988018274\n",
      "  batch 26 loss: 0.0003602601354941726\n",
      "  batch 27 loss: 0.00019127936684526503\n",
      "  batch 28 loss: 0.000302002823445946\n",
      "  batch 29 loss: 0.00017481553368270397\n",
      "  batch 30 loss: 0.00015216686006169766\n",
      "  batch 31 loss: 0.00013703065633308142\n",
      "  batch 32 loss: 0.00020809739362448454\n",
      "  batch 33 loss: 0.00021815163199789822\n",
      "  batch 34 loss: 0.0002796593471430242\n",
      "  batch 35 loss: 0.00014940407709218562\n",
      "  batch 36 loss: 0.00014307242236100137\n",
      "  batch 37 loss: 0.00014646635099779814\n",
      "  batch 38 loss: 0.00014435993216466159\n",
      "  batch 39 loss: 0.0002074859366985038\n",
      "  batch 40 loss: 0.00029628764605149627\n",
      "  batch 41 loss: 0.00012732637696899474\n",
      "  batch 42 loss: 0.00015781862020958215\n",
      "  batch 43 loss: 0.00016351058729924262\n",
      "  batch 44 loss: 0.00010513993038330227\n",
      "  batch 45 loss: 0.0001432501303497702\n",
      "  batch 46 loss: 0.00017079871031455696\n",
      "  batch 47 loss: 0.0001864634978119284\n",
      "  batch 48 loss: 0.0001742144813761115\n",
      "  batch 49 loss: 0.00021256229956634343\n",
      "  batch 50 loss: 0.00022372610692400485\n",
      "  batch 51 loss: 0.00021635807934217155\n",
      "  batch 52 loss: 0.00017558268154971302\n",
      "  batch 53 loss: 0.00019471690757200122\n",
      "  batch 54 loss: 0.00018366400036029518\n",
      "  batch 55 loss: 0.00024279355420731008\n",
      "  batch 56 loss: 0.0001694194506853819\n",
      "  batch 57 loss: 0.0001328583457507193\n",
      "  batch 58 loss: 0.000188836085726507\n",
      "  batch 59 loss: 0.00024213141296058893\n",
      "  batch 60 loss: 0.00025639496743679047\n",
      "  batch 61 loss: 0.00014932821795810014\n",
      "  batch 62 loss: 0.00020000620861537755\n",
      "  batch 63 loss: 0.00015010972856543958\n",
      "  batch 64 loss: 0.00012203205551486462\n",
      "  batch 65 loss: 0.00014832185115665197\n",
      "  batch 66 loss: 0.0001486175024183467\n",
      "  batch 67 loss: 0.00012123007763875648\n",
      "  batch 68 loss: 0.00016551805310882628\n",
      "  batch 69 loss: 0.00016472223796881735\n",
      "  batch 70 loss: 0.00024809359456412494\n",
      "  batch 71 loss: 0.00014599108544643968\n",
      "  batch 72 loss: 0.0001516857009846717\n",
      "  batch 73 loss: 0.0001494388561695814\n",
      "  batch 74 loss: 0.00020391774887684733\n",
      "  batch 75 loss: 0.0002737582544796169\n",
      "  batch 76 loss: 0.00017041873070411384\n",
      "  batch 77 loss: 0.00016571210289839655\n",
      "  batch 78 loss: 0.00016594362386967987\n",
      "  batch 79 loss: 0.00015158255700953305\n",
      "  batch 80 loss: 0.0001858403702499345\n",
      "  batch 81 loss: 0.00018972779798787087\n",
      "  batch 82 loss: 0.00020891681197099388\n",
      "  batch 83 loss: 0.00020385411335155368\n",
      "  batch 84 loss: 0.00015595011063851416\n",
      "  batch 85 loss: 0.00016496435273438692\n",
      "  batch 86 loss: 0.00019386710482649505\n",
      "  batch 87 loss: 0.00019504471856635064\n",
      "  batch 88 loss: 0.0002712173736654222\n",
      "  batch 89 loss: 0.00023900365340523422\n",
      "  batch 90 loss: 0.00028642817051149905\n",
      "  batch 91 loss: 0.00020744380890391767\n",
      "  batch 92 loss: 0.00018998478481080383\n",
      "  batch 93 loss: 0.0002071603521471843\n",
      "  batch 94 loss: 0.00020632738596759737\n",
      "  batch 95 loss: 0.00031686376314610243\n",
      "LOSS train 0.00031686376314610243 valid 0.0009894984541460872\n",
      "LOSS train 0.00031686376314610243 valid 0.0011115779634565115\n",
      "LOSS train 0.00031686376314610243 valid 0.0009811033960431814\n",
      "LOSS train 0.00031686376314610243 valid 0.0009305172134190798\n",
      "LOSS train 0.00031686376314610243 valid 0.0009178539621643722\n",
      "LOSS train 0.00031686376314610243 valid 0.0008708855020813644\n",
      "LOSS train 0.00031686376314610243 valid 0.000928831344936043\n",
      "LOSS train 0.00031686376314610243 valid 0.0009583613136783242\n",
      "LOSS train 0.00031686376314610243 valid 0.000923088111449033\n",
      "LOSS train 0.00031686376314610243 valid 0.0009443787857890129\n",
      "LOSS train 0.00031686376314610243 valid 0.0010148212313652039\n",
      "LOSS train 0.00031686376314610243 valid 0.0010132542811334133\n",
      "LOSS train 0.00031686376314610243 valid 0.0009998829336836934\n",
      "LOSS train 0.00031686376314610243 valid 0.0010183898266404867\n",
      "LOSS train 0.00031686376314610243 valid 0.0010905832750722766\n",
      "LOSS train 0.00031686376314610243 valid 0.0011281269835308194\n",
      "LOSS train 0.00031686376314610243 valid 0.0011319933691993356\n",
      "LOSS train 0.00031686376314610243 valid 0.0011416940251365304\n",
      "LOSS train 0.00031686376314610243 valid 0.001140193548053503\n",
      "LOSS train 0.00031686376314610243 valid 0.0011492184130474925\n",
      "LOSS train 0.00031686376314610243 valid 0.0011374398600310087\n",
      "LOSS train 0.00031686376314610243 valid 0.0011498909443616867\n",
      "LOSS train 0.00031686376314610243 valid 0.0011571594513952732\n",
      "LOSS train 0.00031686376314610243 valid 0.0011745127849280834\n",
      "EPOCH 51:\n",
      "  batch 1 loss: 0.00026345252990722656\n",
      "  batch 2 loss: 0.0001533575268695131\n",
      "  batch 3 loss: 0.00024742173263803124\n",
      "  batch 4 loss: 0.00020533084170892835\n",
      "  batch 5 loss: 0.00017304005450569093\n",
      "  batch 6 loss: 0.00019000729662366211\n",
      "  batch 7 loss: 0.00012266042176634073\n",
      "  batch 8 loss: 0.00014500090037472546\n",
      "  batch 9 loss: 0.00018198142061010003\n",
      "  batch 10 loss: 0.00013348640641197562\n",
      "  batch 11 loss: 0.00028145595570094883\n",
      "  batch 12 loss: 0.00026198840350843966\n",
      "  batch 13 loss: 0.00027665073866955936\n",
      "  batch 14 loss: 0.0005285833030939102\n",
      "  batch 15 loss: 0.00026945117861032486\n",
      "  batch 16 loss: 0.00021187007951084524\n",
      "  batch 17 loss: 0.0002332118892809376\n",
      "  batch 18 loss: 0.00021988303342368454\n",
      "  batch 19 loss: 0.0003804167499765754\n",
      "  batch 20 loss: 0.0003215011674910784\n",
      "  batch 21 loss: 0.00035041142837144434\n",
      "  batch 22 loss: 0.00026936366339214146\n",
      "  batch 23 loss: 0.0003288764855824411\n",
      "  batch 24 loss: 0.00031215231865644455\n",
      "  batch 25 loss: 0.0003769972827285528\n",
      "  batch 26 loss: 0.0003603921504691243\n",
      "  batch 27 loss: 0.00018742827523965389\n",
      "  batch 28 loss: 0.0002937795943580568\n",
      "  batch 29 loss: 0.0001745793124428019\n",
      "  batch 30 loss: 0.0001596383226569742\n",
      "  batch 31 loss: 0.00013888282410334796\n",
      "  batch 32 loss: 0.00020676100393757224\n",
      "  batch 33 loss: 0.00021115297568030655\n",
      "  batch 34 loss: 0.0002619524602778256\n",
      "  batch 35 loss: 0.0001528372522443533\n",
      "  batch 36 loss: 0.00014659608132205904\n",
      "  batch 37 loss: 0.00017413846217095852\n",
      "  batch 38 loss: 0.0001540200028102845\n",
      "  batch 39 loss: 0.00019888748647645116\n",
      "  batch 40 loss: 0.0002658483572304249\n",
      "  batch 41 loss: 0.00012222946679685265\n",
      "  batch 42 loss: 0.00014871459279675037\n",
      "  batch 43 loss: 0.0001693431695457548\n",
      "  batch 44 loss: 0.00012252551096025854\n",
      "  batch 45 loss: 0.00014649797230958939\n",
      "  batch 46 loss: 0.00016093124577309936\n",
      "  batch 47 loss: 0.00019696526578627527\n",
      "  batch 48 loss: 0.0001664908486418426\n",
      "  batch 49 loss: 0.00020066078286617994\n",
      "  batch 50 loss: 0.00022129202261567116\n",
      "  batch 51 loss: 0.00023588731710333377\n",
      "  batch 52 loss: 0.0001789874950191006\n",
      "  batch 53 loss: 0.00020530790789052844\n",
      "  batch 54 loss: 0.00018468403141014278\n",
      "  batch 55 loss: 0.00022693834034726024\n",
      "  batch 56 loss: 0.00016111310105770826\n",
      "  batch 57 loss: 0.0001282258308492601\n",
      "  batch 58 loss: 0.0001853724243119359\n",
      "  batch 59 loss: 0.0002577344421297312\n",
      "  batch 60 loss: 0.0002584723988547921\n",
      "  batch 61 loss: 0.0001467432884965092\n",
      "  batch 62 loss: 0.0001983875408768654\n",
      "  batch 63 loss: 0.00014667575305793434\n",
      "  batch 64 loss: 0.0001281245204154402\n",
      "  batch 65 loss: 0.00014460927923209965\n",
      "  batch 66 loss: 0.00015042844461277127\n",
      "  batch 67 loss: 0.0001192872878164053\n",
      "  batch 68 loss: 0.00016197662625927478\n",
      "  batch 69 loss: 0.00016120183863677084\n",
      "  batch 70 loss: 0.0002458824310451746\n",
      "  batch 71 loss: 0.0001528895227238536\n",
      "  batch 72 loss: 0.00015078141586855054\n",
      "  batch 73 loss: 0.0001467225083615631\n",
      "  batch 74 loss: 0.00020543395658023655\n",
      "  batch 75 loss: 0.0002716397284530103\n",
      "  batch 76 loss: 0.000166130528668873\n",
      "  batch 77 loss: 0.00016620737733319402\n",
      "  batch 78 loss: 0.00016843572666402906\n",
      "  batch 79 loss: 0.0001550759479869157\n",
      "  batch 80 loss: 0.00017888323054648936\n",
      "  batch 81 loss: 0.0001897373585961759\n",
      "  batch 82 loss: 0.00020996836246922612\n",
      "  batch 83 loss: 0.00020625734759960324\n",
      "  batch 84 loss: 0.0001544450642541051\n",
      "  batch 85 loss: 0.00016772252274677157\n",
      "  batch 86 loss: 0.00019633217016234994\n",
      "  batch 87 loss: 0.0001854165457189083\n",
      "  batch 88 loss: 0.0002668459201231599\n",
      "  batch 89 loss: 0.0002398113429080695\n",
      "  batch 90 loss: 0.00028067646780982614\n",
      "  batch 91 loss: 0.0002043624990619719\n",
      "  batch 92 loss: 0.00018842605641111732\n",
      "  batch 93 loss: 0.0002055398072116077\n",
      "  batch 94 loss: 0.00021069817012175918\n",
      "  batch 95 loss: 0.0003139276523143053\n",
      "LOSS train 0.0003139276523143053 valid 0.0009227520204149187\n",
      "LOSS train 0.0003139276523143053 valid 0.0010628548916429281\n",
      "LOSS train 0.0003139276523143053 valid 0.0009436460677534342\n",
      "LOSS train 0.0003139276523143053 valid 0.0009053167887032032\n",
      "LOSS train 0.0003139276523143053 valid 0.0008976749959401786\n",
      "LOSS train 0.0003139276523143053 valid 0.0008438233635388315\n",
      "LOSS train 0.0003139276523143053 valid 0.0008963254513218999\n",
      "LOSS train 0.0003139276523143053 valid 0.0009344780119135976\n",
      "LOSS train 0.0003139276523143053 valid 0.0008990362985059619\n",
      "LOSS train 0.0003139276523143053 valid 0.0009222053340636194\n",
      "LOSS train 0.0003139276523143053 valid 0.0009877141565084457\n",
      "LOSS train 0.0003139276523143053 valid 0.000986749422736466\n",
      "LOSS train 0.0003139276523143053 valid 0.0009735053172335029\n",
      "LOSS train 0.0003139276523143053 valid 0.000994217349216342\n",
      "LOSS train 0.0003139276523143053 valid 0.0010610384633764625\n",
      "LOSS train 0.0003139276523143053 valid 0.001097433501854539\n",
      "LOSS train 0.0003139276523143053 valid 0.001099836896173656\n",
      "LOSS train 0.0003139276523143053 valid 0.0011103857541456819\n",
      "LOSS train 0.0003139276523143053 valid 0.001110787969082594\n",
      "LOSS train 0.0003139276523143053 valid 0.001120877917855978\n",
      "LOSS train 0.0003139276523143053 valid 0.0011115416418761015\n",
      "LOSS train 0.0003139276523143053 valid 0.0011268772650510073\n",
      "LOSS train 0.0003139276523143053 valid 0.0011331760324537754\n",
      "LOSS train 0.0003139276523143053 valid 0.001146694296039641\n",
      "EPOCH 52:\n",
      "  batch 1 loss: 0.0002596470294520259\n",
      "  batch 2 loss: 0.0001522237143944949\n",
      "  batch 3 loss: 0.00023854561732150614\n",
      "  batch 4 loss: 0.00021448699408210814\n",
      "  batch 5 loss: 0.00017111783381551504\n",
      "  batch 6 loss: 0.00017784989904612303\n",
      "  batch 7 loss: 0.00012473631068132818\n",
      "  batch 8 loss: 0.00013990490697324276\n",
      "  batch 9 loss: 0.0001756919955369085\n",
      "  batch 10 loss: 0.00012931021046824753\n",
      "  batch 11 loss: 0.0002899568062275648\n",
      "  batch 12 loss: 0.0002584273461252451\n",
      "  batch 13 loss: 0.00026429485296830535\n",
      "  batch 14 loss: 0.0005226557841524482\n",
      "  batch 15 loss: 0.00030273047741502523\n",
      "  batch 16 loss: 0.00021669635316357017\n",
      "  batch 17 loss: 0.0002281510242028162\n",
      "  batch 18 loss: 0.00022656279907096177\n",
      "  batch 19 loss: 0.000422392477048561\n",
      "  batch 20 loss: 0.0003228205314371735\n",
      "  batch 21 loss: 0.0003503531916067004\n",
      "  batch 22 loss: 0.00028719822876155376\n",
      "  batch 23 loss: 0.00036676059244200587\n",
      "  batch 24 loss: 0.0003390072379261255\n",
      "  batch 25 loss: 0.0003753359487745911\n",
      "  batch 26 loss: 0.0004030599375255406\n",
      "  batch 27 loss: 0.00021183078933972865\n",
      "  batch 28 loss: 0.0003418135456740856\n",
      "  batch 29 loss: 0.00019015095313079655\n",
      "  batch 30 loss: 0.00015785472351126373\n",
      "  batch 31 loss: 0.00014162407023832202\n",
      "  batch 32 loss: 0.0002206898934673518\n",
      "  batch 33 loss: 0.00021703723177779466\n",
      "  batch 34 loss: 0.00027751486049965024\n",
      "  batch 35 loss: 0.00014630597434006631\n",
      "  batch 36 loss: 0.00014427717542275786\n",
      "  batch 37 loss: 0.0001607871090527624\n",
      "  batch 38 loss: 0.00016261276323348284\n",
      "  batch 39 loss: 0.00020464450062718242\n",
      "  batch 40 loss: 0.0002565942704677582\n",
      "  batch 41 loss: 0.00012031671940349042\n",
      "  batch 42 loss: 0.00015305893612094223\n",
      "  batch 43 loss: 0.0001458492042729631\n",
      "  batch 44 loss: 0.00010421741171739995\n",
      "  batch 45 loss: 0.00013746580225415528\n",
      "  batch 46 loss: 0.0001725811162032187\n",
      "  batch 47 loss: 0.00018959949375130236\n",
      "  batch 48 loss: 0.00017011724412441254\n",
      "  batch 49 loss: 0.00020790647249668837\n",
      "  batch 50 loss: 0.00022315864043775946\n",
      "  batch 51 loss: 0.00022352686210069805\n",
      "  batch 52 loss: 0.00017732821288518608\n",
      "  batch 53 loss: 0.00021378759993240237\n",
      "  batch 54 loss: 0.00018681824440136552\n",
      "  batch 55 loss: 0.00023942312691360712\n",
      "  batch 56 loss: 0.0001797749864635989\n",
      "  batch 57 loss: 0.000129879146697931\n",
      "  batch 58 loss: 0.00017488974845036864\n",
      "  batch 59 loss: 0.0002546491159591824\n",
      "  batch 60 loss: 0.0002735643065534532\n",
      "  batch 61 loss: 0.00014458116493187845\n",
      "  batch 62 loss: 0.00019035932200495154\n",
      "  batch 63 loss: 0.0001536612689960748\n",
      "  batch 64 loss: 0.00012663210509344935\n",
      "  batch 65 loss: 0.0001465952955186367\n",
      "  batch 66 loss: 0.00015641923528164625\n",
      "  batch 67 loss: 0.00015994338900782168\n",
      "  batch 68 loss: 0.00017688317166175693\n",
      "  batch 69 loss: 0.00018103173351846635\n",
      "  batch 70 loss: 0.00026734184939414263\n",
      "  batch 71 loss: 0.00017692572146188468\n",
      "  batch 72 loss: 0.00016631517792120576\n",
      "  batch 73 loss: 0.00016596577188465744\n",
      "  batch 74 loss: 0.00020346180826891214\n",
      "  batch 75 loss: 0.00027665618108585477\n",
      "  batch 76 loss: 0.00018385780276730657\n",
      "  batch 77 loss: 0.0001830569381127134\n",
      "  batch 78 loss: 0.00018392503261566162\n",
      "  batch 79 loss: 0.00019161363888997585\n",
      "  batch 80 loss: 0.00019615863857325166\n",
      "  batch 81 loss: 0.00020165253954473883\n",
      "  batch 82 loss: 0.0002226363867521286\n",
      "  batch 83 loss: 0.00022402257309295237\n",
      "  batch 84 loss: 0.00016463332576677203\n",
      "  batch 85 loss: 0.00017296783335041255\n",
      "  batch 86 loss: 0.00021251876023598015\n",
      "  batch 87 loss: 0.00020519294776022434\n",
      "  batch 88 loss: 0.0003652172745205462\n",
      "  batch 89 loss: 0.000268889736616984\n",
      "  batch 90 loss: 0.0003254854236729443\n",
      "  batch 91 loss: 0.000247084884904325\n",
      "  batch 92 loss: 0.00023537565721198916\n",
      "  batch 93 loss: 0.00021870761702302843\n",
      "  batch 94 loss: 0.00023403766681440175\n",
      "  batch 95 loss: 0.0003238597419112921\n",
      "LOSS train 0.0003238597419112921 valid 0.0009935474954545498\n",
      "LOSS train 0.0003238597419112921 valid 0.0010994034819304943\n",
      "LOSS train 0.0003238597419112921 valid 0.0009971432154998183\n",
      "LOSS train 0.0003238597419112921 valid 0.0009503281908109784\n",
      "LOSS train 0.0003238597419112921 valid 0.000935685820877552\n",
      "LOSS train 0.0003238597419112921 valid 0.0008803600794635713\n",
      "LOSS train 0.0003238597419112921 valid 0.0009280896047130227\n",
      "LOSS train 0.0003238597419112921 valid 0.000975768081843853\n",
      "LOSS train 0.0003238597419112921 valid 0.0009391927742399275\n",
      "LOSS train 0.0003238597419112921 valid 0.0009640157222747803\n",
      "LOSS train 0.0003238597419112921 valid 0.0010362186003476381\n",
      "LOSS train 0.0003238597419112921 valid 0.001034051296301186\n",
      "LOSS train 0.0003238597419112921 valid 0.001027386635541916\n",
      "LOSS train 0.0003238597419112921 valid 0.0010493122972548008\n",
      "LOSS train 0.0003238597419112921 valid 0.0011396899353712797\n",
      "LOSS train 0.0003238597419112921 valid 0.0011772527359426022\n",
      "LOSS train 0.0003238597419112921 valid 0.0011776305036619306\n",
      "LOSS train 0.0003238597419112921 valid 0.0011856240453198552\n",
      "LOSS train 0.0003238597419112921 valid 0.001187120913527906\n",
      "LOSS train 0.0003238597419112921 valid 0.0011898104567080736\n",
      "LOSS train 0.0003238597419112921 valid 0.0011754183797165751\n",
      "LOSS train 0.0003238597419112921 valid 0.001183102373033762\n",
      "LOSS train 0.0003238597419112921 valid 0.0011867660796269774\n",
      "LOSS train 0.0003238597419112921 valid 0.0011874252231791615\n",
      "EPOCH 53:\n",
      "  batch 1 loss: 0.0002927868627011776\n",
      "  batch 2 loss: 0.00017602549633011222\n",
      "  batch 3 loss: 0.00027529129874892533\n",
      "  batch 4 loss: 0.00022359807917382568\n",
      "  batch 5 loss: 0.00018827403255272657\n",
      "  batch 6 loss: 0.00019719482224900275\n",
      "  batch 7 loss: 0.00012883907766081393\n",
      "  batch 8 loss: 0.00017418290372006595\n",
      "  batch 9 loss: 0.0001995418278966099\n",
      "  batch 10 loss: 0.00015420152340084314\n",
      "  batch 11 loss: 0.0002569213102106005\n",
      "  batch 12 loss: 0.0002692827256396413\n",
      "  batch 13 loss: 0.0002783052623271942\n",
      "  batch 14 loss: 0.0005380066577345133\n",
      "  batch 15 loss: 0.00030227808747440577\n",
      "  batch 16 loss: 0.00020426561241038144\n",
      "  batch 17 loss: 0.0002409329463262111\n",
      "  batch 18 loss: 0.00022267733584158123\n",
      "  batch 19 loss: 0.00038692221278324723\n",
      "  batch 20 loss: 0.00031614655745215714\n",
      "  batch 21 loss: 0.00033911841455847025\n",
      "  batch 22 loss: 0.00030494233942590654\n",
      "  batch 23 loss: 0.0003350743791088462\n",
      "  batch 24 loss: 0.00033929149503819644\n",
      "  batch 25 loss: 0.00038496628985740244\n",
      "  batch 26 loss: 0.00037354661617428064\n",
      "  batch 27 loss: 0.0002021310938289389\n",
      "  batch 28 loss: 0.00033359762164764106\n",
      "  batch 29 loss: 0.0001729901268845424\n",
      "  batch 30 loss: 0.00016921221686061472\n",
      "  batch 31 loss: 0.00014566084428224713\n",
      "  batch 32 loss: 0.00022167127463035285\n",
      "  batch 33 loss: 0.00021366827422752976\n",
      "  batch 34 loss: 0.0002631967072375119\n",
      "  batch 35 loss: 0.00013526121620088816\n",
      "  batch 36 loss: 0.0001496239856351167\n",
      "  batch 37 loss: 0.00015182755305431783\n",
      "  batch 38 loss: 0.00014788692351430655\n",
      "  batch 39 loss: 0.00019896746380254626\n",
      "  batch 40 loss: 0.00024241788196377456\n",
      "  batch 41 loss: 0.00013988598948344588\n",
      "  batch 42 loss: 0.00015799606626387686\n",
      "  batch 43 loss: 0.00014657413703389466\n",
      "  batch 44 loss: 9.670973668107763e-05\n",
      "  batch 45 loss: 0.00012067120405845344\n",
      "  batch 46 loss: 0.00014382362132892013\n",
      "  batch 47 loss: 0.00017447082791477442\n",
      "  batch 48 loss: 0.0001703629968687892\n",
      "  batch 49 loss: 0.00020785126253031194\n",
      "  batch 50 loss: 0.00022993586026132107\n",
      "  batch 51 loss: 0.00022005342179909348\n",
      "  batch 52 loss: 0.00018175700097344816\n",
      "  batch 53 loss: 0.0001987819850910455\n",
      "  batch 54 loss: 0.00018776379874907434\n",
      "  batch 55 loss: 0.0002290065458510071\n",
      "  batch 56 loss: 0.00016430502000730485\n",
      "  batch 57 loss: 0.00012701954983640462\n",
      "  batch 58 loss: 0.00017395842587575316\n",
      "  batch 59 loss: 0.00034517317544668913\n",
      "  batch 60 loss: 0.0002530314086470753\n",
      "  batch 61 loss: 0.00015020131831988692\n",
      "  batch 62 loss: 0.0002209461817983538\n",
      "  batch 63 loss: 0.00015320754027925432\n",
      "  batch 64 loss: 0.0001338053261861205\n",
      "  batch 65 loss: 0.00014601924340240657\n",
      "  batch 66 loss: 0.00017953655333258212\n",
      "  batch 67 loss: 0.00013712188228964806\n",
      "  batch 68 loss: 0.00018508416542317718\n",
      "  batch 69 loss: 0.0001798760931706056\n",
      "  batch 70 loss: 0.00026617234107106924\n",
      "  batch 71 loss: 0.00016805430641397834\n",
      "  batch 72 loss: 0.00016572164895478636\n",
      "  batch 73 loss: 0.0001571296452311799\n",
      "  batch 74 loss: 0.00019182704272679985\n",
      "  batch 75 loss: 0.0002643723273649812\n",
      "  batch 76 loss: 0.0001828106469474733\n",
      "  batch 77 loss: 0.0001885126403067261\n",
      "  batch 78 loss: 0.0001901164068840444\n",
      "  batch 79 loss: 0.00016595289343968034\n",
      "  batch 80 loss: 0.00019507206161506474\n",
      "  batch 81 loss: 0.00019115235772915184\n",
      "  batch 82 loss: 0.0002134566311724484\n",
      "  batch 83 loss: 0.0002162712044082582\n",
      "  batch 84 loss: 0.00015883284504525363\n",
      "  batch 85 loss: 0.00017188669880852103\n",
      "  batch 86 loss: 0.0002061855047941208\n",
      "  batch 87 loss: 0.0002024169807555154\n",
      "  batch 88 loss: 0.00029169252957217395\n",
      "  batch 89 loss: 0.00026083533884957433\n",
      "  batch 90 loss: 0.00033834652276709676\n",
      "  batch 91 loss: 0.000267052004346624\n",
      "  batch 92 loss: 0.0002099907724186778\n",
      "  batch 93 loss: 0.00021505373297259212\n",
      "  batch 94 loss: 0.00021991616813465953\n",
      "  batch 95 loss: 0.0003628970589488745\n",
      "LOSS train 0.0003628970589488745 valid 0.001027504331432283\n",
      "LOSS train 0.0003628970589488745 valid 0.0011322465725243092\n",
      "LOSS train 0.0003628970589488745 valid 0.000991696841083467\n",
      "LOSS train 0.0003628970589488745 valid 0.0009356814553029835\n",
      "LOSS train 0.0003628970589488745 valid 0.0009321702527813613\n",
      "LOSS train 0.0003628970589488745 valid 0.0008772578439675272\n",
      "LOSS train 0.0003628970589488745 valid 0.0009268245776183903\n",
      "LOSS train 0.0003628970589488745 valid 0.000961321173235774\n",
      "LOSS train 0.0003628970589488745 valid 0.0009255062905140221\n",
      "LOSS train 0.0003628970589488745 valid 0.0009536397992633283\n",
      "LOSS train 0.0003628970589488745 valid 0.0010329186916351318\n",
      "LOSS train 0.0003628970589488745 valid 0.0010376684367656708\n",
      "LOSS train 0.0003628970589488745 valid 0.0010220983531326056\n",
      "LOSS train 0.0003628970589488745 valid 0.001046106917783618\n",
      "LOSS train 0.0003628970589488745 valid 0.001119571621529758\n",
      "LOSS train 0.0003628970589488745 valid 0.0011606767075136304\n",
      "LOSS train 0.0003628970589488745 valid 0.0011622318997979164\n",
      "LOSS train 0.0003628970589488745 valid 0.0011717875022441149\n",
      "LOSS train 0.0003628970589488745 valid 0.0011720714392140508\n",
      "LOSS train 0.0003628970589488745 valid 0.0011731063714250922\n",
      "LOSS train 0.0003628970589488745 valid 0.0011595372343435884\n",
      "LOSS train 0.0003628970589488745 valid 0.001169509021565318\n",
      "LOSS train 0.0003628970589488745 valid 0.0011797130573540926\n",
      "LOSS train 0.0003628970589488745 valid 0.0011925387661904097\n",
      "EPOCH 54:\n",
      "  batch 1 loss: 0.00029002700466662645\n",
      "  batch 2 loss: 0.0001780836028046906\n",
      "  batch 3 loss: 0.00027216458693146706\n",
      "  batch 4 loss: 0.00022657454246655107\n",
      "  batch 5 loss: 0.00018192469724453986\n",
      "  batch 6 loss: 0.00019757993868552148\n",
      "  batch 7 loss: 0.00013212652993388474\n",
      "  batch 8 loss: 0.0001645283482503146\n",
      "  batch 9 loss: 0.00019786828488577157\n",
      "  batch 10 loss: 0.000148828694364056\n",
      "  batch 11 loss: 0.000262897607171908\n",
      "  batch 12 loss: 0.0002609459334053099\n",
      "  batch 13 loss: 0.0002763729135040194\n",
      "  batch 14 loss: 0.0004818118759430945\n",
      "  batch 15 loss: 0.0003132237761747092\n",
      "  batch 16 loss: 0.00020635260443668813\n",
      "  batch 17 loss: 0.0002204668417107314\n",
      "  batch 18 loss: 0.0002060200204141438\n",
      "  batch 19 loss: 0.0003603684017434716\n",
      "  batch 20 loss: 0.0003138762549497187\n",
      "  batch 21 loss: 0.00034207088174298406\n",
      "  batch 22 loss: 0.00025400909362360835\n",
      "  batch 23 loss: 0.00035212101647630334\n",
      "  batch 24 loss: 0.00032859036582522094\n",
      "  batch 25 loss: 0.000368238688679412\n",
      "  batch 26 loss: 0.000375227042241022\n",
      "  batch 27 loss: 0.00019814381084870547\n",
      "  batch 28 loss: 0.0003131309640593827\n",
      "  batch 29 loss: 0.00018246723629999906\n",
      "  batch 30 loss: 0.00016560991934966296\n",
      "  batch 31 loss: 0.00013889578985981643\n",
      "  batch 32 loss: 0.0002057669044006616\n",
      "  batch 33 loss: 0.00021161817130632699\n",
      "  batch 34 loss: 0.0002679607714526355\n",
      "  batch 35 loss: 0.00013688081526197493\n",
      "  batch 36 loss: 0.000137679890030995\n",
      "  batch 37 loss: 0.00014946043665986508\n",
      "  batch 38 loss: 0.00013996334746479988\n",
      "  batch 39 loss: 0.0001945164694916457\n",
      "  batch 40 loss: 0.00024461332941427827\n",
      "  batch 41 loss: 0.00011817098129540682\n",
      "  batch 42 loss: 0.00014840303629171103\n",
      "  batch 43 loss: 0.00014457941870205104\n",
      "  batch 44 loss: 9.745647548697889e-05\n",
      "  batch 45 loss: 0.0001294191461056471\n",
      "  batch 46 loss: 0.00014171094517223537\n",
      "  batch 47 loss: 0.0001633035863051191\n",
      "  batch 48 loss: 0.0001609110040590167\n",
      "  batch 49 loss: 0.00019665587751660496\n",
      "  batch 50 loss: 0.0002141456789104268\n",
      "  batch 51 loss: 0.0001955233165062964\n",
      "  batch 52 loss: 0.00017233905964531004\n",
      "  batch 53 loss: 0.00018740675295703113\n",
      "  batch 54 loss: 0.00017966626910492778\n",
      "  batch 55 loss: 0.0002301027561770752\n",
      "  batch 56 loss: 0.00015524544869549572\n",
      "  batch 57 loss: 0.00012411280476953834\n",
      "  batch 58 loss: 0.00017821742221713066\n",
      "  batch 59 loss: 0.00024512031814083457\n",
      "  batch 60 loss: 0.0002412123285466805\n",
      "  batch 61 loss: 0.00013349487562663853\n",
      "  batch 62 loss: 0.00028279892285354435\n",
      "  batch 63 loss: 0.0001610223262105137\n",
      "  batch 64 loss: 0.00011782535875681788\n",
      "  batch 65 loss: 0.00013427904923446476\n",
      "  batch 66 loss: 0.00013914689770899713\n",
      "  batch 67 loss: 0.00012662784138228744\n",
      "  batch 68 loss: 0.00016116943152155727\n",
      "  batch 69 loss: 0.00016876187874004245\n",
      "  batch 70 loss: 0.0002488296595402062\n",
      "  batch 71 loss: 0.00015677677583880723\n",
      "  batch 72 loss: 0.00015060704026836902\n",
      "  batch 73 loss: 0.00014762600767426193\n",
      "  batch 74 loss: 0.00019153239554725587\n",
      "  batch 75 loss: 0.0002674860879778862\n",
      "  batch 76 loss: 0.00016529564163647592\n",
      "  batch 77 loss: 0.0001714529498713091\n",
      "  batch 78 loss: 0.0001681148132774979\n",
      "  batch 79 loss: 0.0001539811200927943\n",
      "  batch 80 loss: 0.00018330200691707432\n",
      "  batch 81 loss: 0.0001786015200195834\n",
      "  batch 82 loss: 0.00020764154032804072\n",
      "  batch 83 loss: 0.00019957649055868387\n",
      "  batch 84 loss: 0.00015036482363939285\n",
      "  batch 85 loss: 0.00016199139645323157\n",
      "  batch 86 loss: 0.00019328013877384365\n",
      "  batch 87 loss: 0.00018642543000169098\n",
      "  batch 88 loss: 0.00027394614880904555\n",
      "  batch 89 loss: 0.00022871448891237378\n",
      "  batch 90 loss: 0.00029123792774043977\n",
      "  batch 91 loss: 0.00020546471932902932\n",
      "  batch 92 loss: 0.00020099099492654204\n",
      "  batch 93 loss: 0.00019735241949092597\n",
      "  batch 94 loss: 0.00020498340018093586\n",
      "  batch 95 loss: 0.0002901488041970879\n",
      "LOSS train 0.0002901488041970879 valid 0.0009196748724207282\n",
      "LOSS train 0.0002901488041970879 valid 0.0010651776101440191\n",
      "LOSS train 0.0002901488041970879 valid 0.0009515599813312292\n",
      "LOSS train 0.0002901488041970879 valid 0.0009018031414598227\n",
      "LOSS train 0.0002901488041970879 valid 0.0008987937471829355\n",
      "LOSS train 0.0002901488041970879 valid 0.0008504573488608003\n",
      "LOSS train 0.0002901488041970879 valid 0.0009037951240316033\n",
      "LOSS train 0.0002901488041970879 valid 0.0009380913688801229\n",
      "LOSS train 0.0002901488041970879 valid 0.0009043654426932335\n",
      "LOSS train 0.0002901488041970879 valid 0.0009301946847699583\n",
      "LOSS train 0.0002901488041970879 valid 0.0009995829313993454\n",
      "LOSS train 0.0002901488041970879 valid 0.0010005232179537416\n",
      "LOSS train 0.0002901488041970879 valid 0.0009898701682686806\n",
      "LOSS train 0.0002901488041970879 valid 0.0010123224928975105\n",
      "LOSS train 0.0002901488041970879 valid 0.001080145942978561\n",
      "LOSS train 0.0002901488041970879 valid 0.0011206167982891202\n",
      "LOSS train 0.0002901488041970879 valid 0.001123670139349997\n",
      "LOSS train 0.0002901488041970879 valid 0.0011352291330695152\n",
      "LOSS train 0.0002901488041970879 valid 0.0011344479862600565\n",
      "LOSS train 0.0002901488041970879 valid 0.0011389371939003468\n",
      "LOSS train 0.0002901488041970879 valid 0.001126856543123722\n",
      "LOSS train 0.0002901488041970879 valid 0.0011394761968404055\n",
      "LOSS train 0.0002901488041970879 valid 0.0011440475936979055\n",
      "LOSS train 0.0002901488041970879 valid 0.0011523962020874023\n",
      "EPOCH 55:\n",
      "  batch 1 loss: 0.0002641192404553294\n",
      "  batch 2 loss: 0.0001540005614515394\n",
      "  batch 3 loss: 0.0002520082052797079\n",
      "  batch 4 loss: 0.00022352101223077625\n",
      "  batch 5 loss: 0.0001793080591596663\n",
      "  batch 6 loss: 0.00018956779967993498\n",
      "  batch 7 loss: 0.00012263745884411037\n",
      "  batch 8 loss: 0.00014463107800111175\n",
      "  batch 9 loss: 0.00019018223974853754\n",
      "  batch 10 loss: 0.00012956401042174548\n",
      "  batch 11 loss: 0.00024020284763537347\n",
      "  batch 12 loss: 0.00026987597811967134\n",
      "  batch 13 loss: 0.0002882909611798823\n",
      "  batch 14 loss: 0.00043063252815045416\n",
      "  batch 15 loss: 0.0002736553142312914\n",
      "  batch 16 loss: 0.0001962676178663969\n",
      "  batch 17 loss: 0.00021412230853457004\n",
      "  batch 18 loss: 0.00018768240988720208\n",
      "  batch 19 loss: 0.0003276728675700724\n",
      "  batch 20 loss: 0.0002782861702144146\n",
      "  batch 21 loss: 0.0003161535132676363\n",
      "  batch 22 loss: 0.00022831332171335816\n",
      "  batch 23 loss: 0.0003249617584515363\n",
      "  batch 24 loss: 0.00030153855914250016\n",
      "  batch 25 loss: 0.00034623831743374467\n",
      "  batch 26 loss: 0.0003543553175404668\n",
      "  batch 27 loss: 0.00017850860604085028\n",
      "  batch 28 loss: 0.0002770979190245271\n",
      "  batch 29 loss: 0.00015778400120325387\n",
      "  batch 30 loss: 0.00013991654850542545\n",
      "  batch 31 loss: 0.00011505447764648125\n",
      "  batch 32 loss: 0.00018667065887711942\n",
      "  batch 33 loss: 0.0001850050175562501\n",
      "  batch 34 loss: 0.0002383343962719664\n",
      "  batch 35 loss: 0.00012593710562214255\n",
      "  batch 36 loss: 0.0001252522342838347\n",
      "  batch 37 loss: 0.0001369897654512897\n",
      "  batch 38 loss: 0.00013306637993082404\n",
      "  batch 39 loss: 0.00017958783428184688\n",
      "  batch 40 loss: 0.0002250237885164097\n",
      "  batch 41 loss: 0.00011007833381881937\n",
      "  batch 42 loss: 0.00013563461834564805\n",
      "  batch 43 loss: 0.00012863495794590563\n",
      "  batch 44 loss: 8.825493569020182e-05\n",
      "  batch 45 loss: 0.00012662162771448493\n",
      "  batch 46 loss: 0.0001260758435819298\n",
      "  batch 47 loss: 0.00017234265396837145\n",
      "  batch 48 loss: 0.00014736346201971173\n",
      "  batch 49 loss: 0.00018149884999729693\n",
      "  batch 50 loss: 0.0002030040486715734\n",
      "  batch 51 loss: 0.00019222110859118402\n",
      "  batch 52 loss: 0.00015830027405172586\n",
      "  batch 53 loss: 0.00017455668421462178\n",
      "  batch 54 loss: 0.00017251759709324688\n",
      "  batch 55 loss: 0.0002260331530123949\n",
      "  batch 56 loss: 0.00015455877291969955\n",
      "  batch 57 loss: 0.00011755116429412737\n",
      "  batch 58 loss: 0.0001705573231447488\n",
      "  batch 59 loss: 0.0002000993408728391\n",
      "  batch 60 loss: 0.00022972634178586304\n",
      "  batch 61 loss: 0.00012202595826238394\n",
      "  batch 62 loss: 0.00018923942116089165\n",
      "  batch 63 loss: 0.00015281292144209146\n",
      "  batch 64 loss: 0.00011393248860258609\n",
      "  batch 65 loss: 0.00013040121120866388\n",
      "  batch 66 loss: 0.00014019935042597353\n",
      "  batch 67 loss: 0.00011956373054999858\n",
      "  batch 68 loss: 0.00015172759594861418\n",
      "  batch 69 loss: 0.0001577962830197066\n",
      "  batch 70 loss: 0.00023766308731865138\n",
      "  batch 71 loss: 0.00014563469449058175\n",
      "  batch 72 loss: 0.00013910254347138107\n",
      "  batch 73 loss: 0.00013276521349325776\n",
      "  batch 74 loss: 0.00017148201004602015\n",
      "  batch 75 loss: 0.00024876047973521054\n",
      "  batch 76 loss: 0.00015730527229607105\n",
      "  batch 77 loss: 0.00016499304911121726\n",
      "  batch 78 loss: 0.00016108262934722006\n",
      "  batch 79 loss: 0.00014764911611564457\n",
      "  batch 80 loss: 0.00016651501937303692\n",
      "  batch 81 loss: 0.00016862880147527903\n",
      "  batch 82 loss: 0.00018879445269703865\n",
      "  batch 83 loss: 0.00018367895972914994\n",
      "  batch 84 loss: 0.00014582084259018302\n",
      "  batch 85 loss: 0.00016046775272116065\n",
      "  batch 86 loss: 0.00018346923752687871\n",
      "  batch 87 loss: 0.00018317182548344135\n",
      "  batch 88 loss: 0.0002596780250314623\n",
      "  batch 89 loss: 0.00021805624419357628\n",
      "  batch 90 loss: 0.00027707117260433733\n",
      "  batch 91 loss: 0.0001878248294815421\n",
      "  batch 92 loss: 0.00018610624829307199\n",
      "  batch 93 loss: 0.00018673681188374758\n",
      "  batch 94 loss: 0.00019472096755634993\n",
      "  batch 95 loss: 0.00028131305589340627\n",
      "LOSS train 0.00028131305589340627 valid 0.0010778044816106558\n",
      "LOSS train 0.00028131305589340627 valid 0.0012336119543761015\n",
      "LOSS train 0.00028131305589340627 valid 0.0010817756410688162\n",
      "LOSS train 0.00028131305589340627 valid 0.0010278373956680298\n",
      "LOSS train 0.00028131305589340627 valid 0.0010225862497463822\n",
      "LOSS train 0.00028131305589340627 valid 0.0009595378651283681\n",
      "LOSS train 0.00028131305589340627 valid 0.001025754027068615\n",
      "LOSS train 0.00028131305589340627 valid 0.001069114776328206\n",
      "LOSS train 0.00028131305589340627 valid 0.0010239965049549937\n",
      "LOSS train 0.00028131305589340627 valid 0.0010590578895062208\n",
      "LOSS train 0.00028131305589340627 valid 0.0011469329474493861\n",
      "LOSS train 0.00028131305589340627 valid 0.0011475516948848963\n",
      "LOSS train 0.00028131305589340627 valid 0.0011303330538794398\n",
      "LOSS train 0.00028131305589340627 valid 0.0011563461739569902\n",
      "LOSS train 0.00028131305589340627 valid 0.0012479877332225442\n",
      "LOSS train 0.00028131305589340627 valid 0.0012962990440428257\n",
      "LOSS train 0.00028131305589340627 valid 0.00129835051484406\n",
      "LOSS train 0.00028131305589340627 valid 0.0013144537806510925\n",
      "LOSS train 0.00028131305589340627 valid 0.0013164624106138945\n",
      "LOSS train 0.00028131305589340627 valid 0.0013153237523511052\n",
      "LOSS train 0.00028131305589340627 valid 0.0012978761224076152\n",
      "LOSS train 0.00028131305589340627 valid 0.001310194144025445\n",
      "LOSS train 0.00028131305589340627 valid 0.0013133122120052576\n",
      "LOSS train 0.00028131305589340627 valid 0.0013113458408042789\n",
      "EPOCH 56:\n",
      "  batch 1 loss: 0.00023761403281241655\n",
      "  batch 2 loss: 0.0001497158082202077\n",
      "  batch 3 loss: 0.00022879400057718158\n",
      "  batch 4 loss: 0.00020845071412622929\n",
      "  batch 5 loss: 0.00016730603238102049\n",
      "  batch 6 loss: 0.00018886136240325868\n",
      "  batch 7 loss: 0.00011879982048412785\n",
      "  batch 8 loss: 0.00013790573575533926\n",
      "  batch 9 loss: 0.00017048782319761813\n",
      "  batch 10 loss: 0.00012052748206770048\n",
      "  batch 11 loss: 0.00020667631179094315\n",
      "  batch 12 loss: 0.0002306508831679821\n",
      "  batch 13 loss: 0.0002699777251109481\n",
      "  batch 14 loss: 0.00044232868822291493\n",
      "  batch 15 loss: 0.00028800833388231695\n",
      "  batch 16 loss: 0.0002372635353822261\n",
      "  batch 17 loss: 0.00024252924777101725\n",
      "  batch 18 loss: 0.0001948700228240341\n",
      "  batch 19 loss: 0.0003705819835886359\n",
      "  batch 20 loss: 0.0003141030319966376\n",
      "  batch 21 loss: 0.0003339010872878134\n",
      "  batch 22 loss: 0.000249479926424101\n",
      "  batch 23 loss: 0.0003173152217641473\n",
      "  batch 24 loss: 0.00030584525666199625\n",
      "  batch 25 loss: 0.00035018115886487067\n",
      "  batch 26 loss: 0.0004436324234120548\n",
      "  batch 27 loss: 0.000231257698033005\n",
      "  batch 28 loss: 0.00027638886240310967\n",
      "  batch 29 loss: 0.0001668260374572128\n",
      "  batch 30 loss: 0.0001527908316347748\n",
      "  batch 31 loss: 0.00014866534911561757\n",
      "  batch 32 loss: 0.0002527614706195891\n",
      "  batch 33 loss: 0.00018795047071762383\n",
      "  batch 34 loss: 0.0002444105048198253\n",
      "  batch 35 loss: 0.0001423811772838235\n",
      "  batch 36 loss: 0.00013981711526867002\n",
      "  batch 37 loss: 0.00015741573588456959\n",
      "  batch 38 loss: 0.00015906071348581463\n",
      "  batch 39 loss: 0.00019282840366940945\n",
      "  batch 40 loss: 0.0002476166409906\n",
      "  batch 41 loss: 0.00012653028534259647\n",
      "  batch 42 loss: 0.0001631341001484543\n",
      "  batch 43 loss: 0.00014315552834887058\n",
      "  batch 44 loss: 0.00010050288256024942\n",
      "  batch 45 loss: 0.00013034460425842553\n",
      "  batch 46 loss: 0.0001386409130645916\n",
      "  batch 47 loss: 0.00016969206626527011\n",
      "  batch 48 loss: 0.0001583552802912891\n",
      "  batch 49 loss: 0.0001914497115649283\n",
      "  batch 50 loss: 0.00022154403268359601\n",
      "  batch 51 loss: 0.00024165849026758224\n",
      "  batch 52 loss: 0.00016874459106475115\n",
      "  batch 53 loss: 0.00019031607371289283\n",
      "  batch 54 loss: 0.00017172735533677042\n",
      "  batch 55 loss: 0.0002282983041368425\n",
      "  batch 56 loss: 0.00015467451885342598\n",
      "  batch 57 loss: 0.00012358793173916638\n",
      "  batch 58 loss: 0.00017598792328499258\n",
      "  batch 59 loss: 0.00022227743465919048\n",
      "  batch 60 loss: 0.0002557100378908217\n",
      "  batch 61 loss: 0.0001345413620583713\n",
      "  batch 62 loss: 0.00019223842537030578\n",
      "  batch 63 loss: 0.00014203977480065078\n",
      "  batch 64 loss: 0.00011496502702357247\n",
      "  batch 65 loss: 0.00013239843246992677\n",
      "  batch 66 loss: 0.00013803635374642909\n",
      "  batch 67 loss: 0.0001250257482752204\n",
      "  batch 68 loss: 0.00014868445578031242\n",
      "  batch 69 loss: 0.00016369170043617487\n",
      "  batch 70 loss: 0.00023875328770373017\n",
      "  batch 71 loss: 0.00015823624562472105\n",
      "  batch 72 loss: 0.00014176152762956917\n",
      "  batch 73 loss: 0.0001404291542712599\n",
      "  batch 74 loss: 0.00016977086488623172\n",
      "  batch 75 loss: 0.00024022749857977033\n",
      "  batch 76 loss: 0.00015909246576484293\n",
      "  batch 77 loss: 0.00016193761257454753\n",
      "  batch 78 loss: 0.0001623924181330949\n",
      "  batch 79 loss: 0.0001542652607895434\n",
      "  batch 80 loss: 0.0001736620906740427\n",
      "  batch 81 loss: 0.00017460410890635103\n",
      "  batch 82 loss: 0.00018975032435264438\n",
      "  batch 83 loss: 0.00019034664728678763\n",
      "  batch 84 loss: 0.00014769526023883373\n",
      "  batch 85 loss: 0.00016154246986843646\n",
      "  batch 86 loss: 0.00018000960699282587\n",
      "  batch 87 loss: 0.00017379087512381375\n",
      "  batch 88 loss: 0.0002573290839791298\n",
      "  batch 89 loss: 0.00020752577984239906\n",
      "  batch 90 loss: 0.0002764450618997216\n",
      "  batch 91 loss: 0.00018499218276701868\n",
      "  batch 92 loss: 0.0001842914061853662\n",
      "  batch 93 loss: 0.00018759677186608315\n",
      "  batch 94 loss: 0.00020192720694467425\n",
      "  batch 95 loss: 0.0002834224433172494\n",
      "LOSS train 0.0002834224433172494 valid 0.001209819340147078\n",
      "LOSS train 0.0002834224433172494 valid 0.0013538063503801823\n",
      "LOSS train 0.0002834224433172494 valid 0.0011630411026999354\n",
      "LOSS train 0.0002834224433172494 valid 0.0011120782000944018\n",
      "LOSS train 0.0002834224433172494 valid 0.0010899397311732173\n",
      "LOSS train 0.0002834224433172494 valid 0.0010279793059453368\n",
      "LOSS train 0.0002834224433172494 valid 0.0010994361946359277\n",
      "LOSS train 0.0002834224433172494 valid 0.0011400666553527117\n",
      "LOSS train 0.0002834224433172494 valid 0.0010876964079216123\n",
      "LOSS train 0.0002834224433172494 valid 0.0011231822427362204\n",
      "LOSS train 0.0002834224433172494 valid 0.001230209250934422\n",
      "LOSS train 0.0002834224433172494 valid 0.001235920237377286\n",
      "LOSS train 0.0002834224433172494 valid 0.0012082798639312387\n",
      "LOSS train 0.0002834224433172494 valid 0.0012295916676521301\n",
      "LOSS train 0.0002834224433172494 valid 0.0013127147685736418\n",
      "LOSS train 0.0002834224433172494 valid 0.0013582090614363551\n",
      "LOSS train 0.0002834224433172494 valid 0.0013555643381550908\n",
      "LOSS train 0.0002834224433172494 valid 0.001375483931042254\n",
      "LOSS train 0.0002834224433172494 valid 0.001383599708788097\n",
      "LOSS train 0.0002834224433172494 valid 0.001389618730172515\n",
      "LOSS train 0.0002834224433172494 valid 0.0013759898720309138\n",
      "LOSS train 0.0002834224433172494 valid 0.0013906210660934448\n",
      "LOSS train 0.0002834224433172494 valid 0.0014009345322847366\n",
      "LOSS train 0.0002834224433172494 valid 0.0014344961382448673\n",
      "EPOCH 57:\n",
      "  batch 1 loss: 0.0002381358644925058\n",
      "  batch 2 loss: 0.00014445724082179368\n",
      "  batch 3 loss: 0.00023914044140838087\n",
      "  batch 4 loss: 0.00021089355868753046\n",
      "  batch 5 loss: 0.00016134872566908598\n",
      "  batch 6 loss: 0.00016457827587146312\n",
      "  batch 7 loss: 0.00011446037387941033\n",
      "  batch 8 loss: 0.00013079895870760083\n",
      "  batch 9 loss: 0.00015090095985215157\n",
      "  batch 10 loss: 0.00011141200957354158\n",
      "  batch 11 loss: 0.00020411043078638613\n",
      "  batch 12 loss: 0.00021680493955500424\n",
      "  batch 13 loss: 0.00024674367159605026\n",
      "  batch 14 loss: 0.00045902066631242633\n",
      "  batch 15 loss: 0.0002685386862140149\n",
      "  batch 16 loss: 0.0002403489052085206\n",
      "  batch 17 loss: 0.0002785758697427809\n",
      "  batch 18 loss: 0.00022180232917889953\n",
      "  batch 19 loss: 0.00046919487067498267\n",
      "  batch 20 loss: 0.00034804127062670887\n",
      "  batch 21 loss: 0.0003373167128302157\n",
      "  batch 22 loss: 0.00026764231733977795\n",
      "  batch 23 loss: 0.00037235504714772105\n",
      "  batch 24 loss: 0.00037851190427318215\n",
      "  batch 25 loss: 0.0003835000970866531\n",
      "  batch 26 loss: 0.00043816084507852793\n",
      "  batch 27 loss: 0.00025319174164906144\n",
      "  batch 28 loss: 0.00035524129634723067\n",
      "  batch 29 loss: 0.00019149048603139818\n",
      "  batch 30 loss: 0.00018816135707311332\n",
      "  batch 31 loss: 0.00016648447490297258\n",
      "  batch 32 loss: 0.00021019620180595666\n",
      "  batch 33 loss: 0.00022074044682085514\n",
      "  batch 34 loss: 0.0002443549456074834\n",
      "  batch 35 loss: 0.0001377122534904629\n",
      "  batch 36 loss: 0.00017877836944535375\n",
      "  batch 37 loss: 0.00017142485012300313\n",
      "  batch 38 loss: 0.00017738051246851683\n",
      "  batch 39 loss: 0.00021364979329518974\n",
      "  batch 40 loss: 0.00030537258135154843\n",
      "  batch 41 loss: 0.00012421382416505367\n",
      "  batch 42 loss: 0.0001969005388673395\n",
      "  batch 43 loss: 0.000148202947457321\n",
      "  batch 44 loss: 0.0001141194807132706\n",
      "  batch 45 loss: 0.00012522918405011296\n",
      "  batch 46 loss: 0.00013384807971306145\n",
      "  batch 47 loss: 0.0001711499207885936\n",
      "  batch 48 loss: 0.00016496930038556457\n",
      "  batch 49 loss: 0.000192580497241579\n",
      "  batch 50 loss: 0.0002239370660390705\n",
      "  batch 51 loss: 0.00022300792625173926\n",
      "  batch 52 loss: 0.0001769515365594998\n",
      "  batch 53 loss: 0.0001949690340552479\n",
      "  batch 54 loss: 0.00018526933854445815\n",
      "  batch 55 loss: 0.00022895036090631038\n",
      "  batch 56 loss: 0.00015571345284115523\n",
      "  batch 57 loss: 0.00012778483505826443\n",
      "  batch 58 loss: 0.00017136737005785108\n",
      "  batch 59 loss: 0.00020466771093197167\n",
      "  batch 60 loss: 0.00022291927598416805\n",
      "  batch 61 loss: 0.00012862845323979855\n",
      "  batch 62 loss: 0.00019476472516544163\n",
      "  batch 63 loss: 0.000144073273986578\n",
      "  batch 64 loss: 0.0001191399569506757\n",
      "  batch 65 loss: 0.00013022360508330166\n",
      "  batch 66 loss: 0.00013088859850540757\n",
      "  batch 67 loss: 0.00011128876940347254\n",
      "  batch 68 loss: 0.00015553711273241788\n",
      "  batch 69 loss: 0.0001614112698007375\n",
      "  batch 70 loss: 0.0002375938493059948\n",
      "  batch 71 loss: 0.0001468518457841128\n",
      "  batch 72 loss: 0.00014004715194460005\n",
      "  batch 73 loss: 0.00013481470523402095\n",
      "  batch 74 loss: 0.00016811385285109282\n",
      "  batch 75 loss: 0.0002356424811296165\n",
      "  batch 76 loss: 0.00015385448932647705\n",
      "  batch 77 loss: 0.00015954585978761315\n",
      "  batch 78 loss: 0.0001570429012645036\n",
      "  batch 79 loss: 0.00015702916425652802\n",
      "  batch 80 loss: 0.0001813374547054991\n",
      "  batch 81 loss: 0.00017000731895677745\n",
      "  batch 82 loss: 0.00018780387472361326\n",
      "  batch 83 loss: 0.00018370409088674933\n",
      "  batch 84 loss: 0.00013990407751407474\n",
      "  batch 85 loss: 0.0001604071876499802\n",
      "  batch 86 loss: 0.00018778695084620267\n",
      "  batch 87 loss: 0.00017633683455642313\n",
      "  batch 88 loss: 0.0002525320742279291\n",
      "  batch 89 loss: 0.0002042859559878707\n",
      "  batch 90 loss: 0.00025757646653801203\n",
      "  batch 91 loss: 0.00017845191177912056\n",
      "  batch 92 loss: 0.00017835645121522248\n",
      "  batch 93 loss: 0.0001790351961972192\n",
      "  batch 94 loss: 0.00019571991288103163\n",
      "  batch 95 loss: 0.00027532881358638406\n",
      "LOSS train 0.00027532881358638406 valid 0.0010339433792978525\n",
      "LOSS train 0.00027532881358638406 valid 0.0012113489210605621\n",
      "LOSS train 0.00027532881358638406 valid 0.001060256501659751\n",
      "LOSS train 0.00027532881358638406 valid 0.0010131276212632656\n",
      "LOSS train 0.00027532881358638406 valid 0.0010014359140768647\n",
      "LOSS train 0.00027532881358638406 valid 0.0009596031159162521\n",
      "LOSS train 0.00027532881358638406 valid 0.0010379513259977102\n",
      "LOSS train 0.00027532881358638406 valid 0.0010867591481655836\n",
      "LOSS train 0.00027532881358638406 valid 0.0010404923232272267\n",
      "LOSS train 0.00027532881358638406 valid 0.0010798309231176972\n",
      "LOSS train 0.00027532881358638406 valid 0.00116175995208323\n",
      "LOSS train 0.00027532881358638406 valid 0.0011689253151416779\n",
      "LOSS train 0.00027532881358638406 valid 0.0011463692644611\n",
      "LOSS train 0.00027532881358638406 valid 0.0011641958262771368\n",
      "LOSS train 0.00027532881358638406 valid 0.0012542878976091743\n",
      "LOSS train 0.00027532881358638406 valid 0.0012936630519106984\n",
      "LOSS train 0.00027532881358638406 valid 0.001295047695748508\n",
      "LOSS train 0.00027532881358638406 valid 0.0013067165855318308\n",
      "LOSS train 0.00027532881358638406 valid 0.00130671844817698\n",
      "LOSS train 0.00027532881358638406 valid 0.0013092674780637026\n",
      "LOSS train 0.00027532881358638406 valid 0.0012916728155687451\n",
      "LOSS train 0.00027532881358638406 valid 0.0013025797670707107\n",
      "LOSS train 0.00027532881358638406 valid 0.0013062129728496075\n",
      "LOSS train 0.00027532881358638406 valid 0.0013231553602963686\n",
      "EPOCH 58:\n",
      "  batch 1 loss: 0.00023366186360362917\n",
      "  batch 2 loss: 0.0001422137429472059\n",
      "  batch 3 loss: 0.00023097009398043156\n",
      "  batch 4 loss: 0.00020444953406695276\n",
      "  batch 5 loss: 0.00018252739391755313\n",
      "  batch 6 loss: 0.00017941741680260748\n",
      "  batch 7 loss: 0.00012097989383619279\n",
      "  batch 8 loss: 0.00014114286750555038\n",
      "  batch 9 loss: 0.00015403353609144688\n",
      "  batch 10 loss: 0.00010714000381994992\n",
      "  batch 11 loss: 0.00019301314023323357\n",
      "  batch 12 loss: 0.00020712368132080883\n",
      "  batch 13 loss: 0.0002392205933574587\n",
      "  batch 14 loss: 0.00043000269215554\n",
      "  batch 15 loss: 0.00024267293338198215\n",
      "  batch 16 loss: 0.00017582692089490592\n",
      "  batch 17 loss: 0.00019803731993306428\n",
      "  batch 18 loss: 0.00017757219029590487\n",
      "  batch 19 loss: 0.00037802240694873035\n",
      "  batch 20 loss: 0.00033973163226619363\n",
      "  batch 21 loss: 0.00033340539084747434\n",
      "  batch 22 loss: 0.0002860389358829707\n",
      "  batch 23 loss: 0.00034090469125658274\n",
      "  batch 24 loss: 0.00030635501025244594\n",
      "  batch 25 loss: 0.00035719218431040645\n",
      "  batch 26 loss: 0.00038131314795464277\n",
      "  batch 27 loss: 0.00022066812380217016\n",
      "  batch 28 loss: 0.0003394039231352508\n",
      "  batch 29 loss: 0.0002591642551124096\n",
      "  batch 30 loss: 0.00030825816793367267\n",
      "  batch 31 loss: 0.0002822494716383517\n",
      "  batch 32 loss: 0.00020607226178981364\n",
      "  batch 33 loss: 0.00019636214710772038\n",
      "  batch 34 loss: 0.00025011724210344255\n",
      "  batch 35 loss: 0.00014209089567884803\n",
      "  batch 36 loss: 0.00018523741164244711\n",
      "  batch 37 loss: 0.00019139982759952545\n",
      "  batch 38 loss: 0.00017011802992783487\n",
      "  batch 39 loss: 0.0002453525667078793\n",
      "  batch 40 loss: 0.0003737127408385277\n",
      "  batch 41 loss: 0.00014332664432004094\n",
      "  batch 42 loss: 0.00026419200003147125\n",
      "  batch 43 loss: 0.00015796169464010745\n",
      "  batch 44 loss: 0.0001188773603644222\n",
      "  batch 45 loss: 0.00014168425695970654\n",
      "  batch 46 loss: 0.00018120158347301185\n",
      "  batch 47 loss: 0.00021351805480662733\n",
      "  batch 48 loss: 0.00018651492428034544\n",
      "  batch 49 loss: 0.00021177614689804614\n",
      "  batch 50 loss: 0.00021970155648887157\n",
      "  batch 51 loss: 0.0002265443472424522\n",
      "  batch 52 loss: 0.00018223896040581167\n",
      "  batch 53 loss: 0.00020689207303803414\n",
      "  batch 54 loss: 0.00020083526032976806\n",
      "  batch 55 loss: 0.00022141689260024577\n",
      "  batch 56 loss: 0.00016567815328016877\n",
      "  batch 57 loss: 0.00013227411545813084\n",
      "  batch 58 loss: 0.00017818406922742724\n",
      "  batch 59 loss: 0.0002645959903020412\n",
      "  batch 60 loss: 0.00021597716840915382\n",
      "  batch 61 loss: 0.00012924318434670568\n",
      "  batch 62 loss: 0.00019998932839371264\n",
      "  batch 63 loss: 0.00016890387632884085\n",
      "  batch 64 loss: 0.000121685465273913\n",
      "  batch 65 loss: 0.00013330263027455658\n",
      "  batch 66 loss: 0.00013261503772810102\n",
      "  batch 67 loss: 0.00011488259042380378\n",
      "  batch 68 loss: 0.0001461926440242678\n",
      "  batch 69 loss: 0.00015340684331022203\n",
      "  batch 70 loss: 0.00023792052525095642\n",
      "  batch 71 loss: 0.00014774472219869494\n",
      "  batch 72 loss: 0.0001399508910253644\n",
      "  batch 73 loss: 0.00013633864000439644\n",
      "  batch 74 loss: 0.00017577926337253302\n",
      "  batch 75 loss: 0.000236273102927953\n",
      "  batch 76 loss: 0.00016794528346508741\n",
      "  batch 77 loss: 0.00016540236538276076\n",
      "  batch 78 loss: 0.00015941438323352486\n",
      "  batch 79 loss: 0.0001494697789894417\n",
      "  batch 80 loss: 0.0001684195303823799\n",
      "  batch 81 loss: 0.0001729669893393293\n",
      "  batch 82 loss: 0.0001936914341058582\n",
      "  batch 83 loss: 0.00018620718037709594\n",
      "  batch 84 loss: 0.0001387772790621966\n",
      "  batch 85 loss: 0.00015792092017363757\n",
      "  batch 86 loss: 0.00018010710482485592\n",
      "  batch 87 loss: 0.00017226739146281034\n",
      "  batch 88 loss: 0.0002567042538430542\n",
      "  batch 89 loss: 0.00020878291979897767\n",
      "  batch 90 loss: 0.0002661604667082429\n",
      "  batch 91 loss: 0.00018182559870183468\n",
      "  batch 92 loss: 0.00018649536650627851\n",
      "  batch 93 loss: 0.00018993689445778728\n",
      "  batch 94 loss: 0.0001987891737371683\n",
      "  batch 95 loss: 0.00027914182282984257\n",
      "LOSS train 0.00027914182282984257 valid 0.0011111395433545113\n",
      "LOSS train 0.00027914182282984257 valid 0.0013512122677639127\n",
      "LOSS train 0.00027914182282984257 valid 0.0011713596759364009\n",
      "LOSS train 0.00027914182282984257 valid 0.0011264579370617867\n",
      "LOSS train 0.00027914182282984257 valid 0.001112846308387816\n",
      "LOSS train 0.00027914182282984257 valid 0.0010620465036481619\n",
      "LOSS train 0.00027914182282984257 valid 0.00116012804210186\n",
      "LOSS train 0.00027914182282984257 valid 0.0012121610343456268\n",
      "LOSS train 0.00027914182282984257 valid 0.0011532685020938516\n",
      "LOSS train 0.00027914182282984257 valid 0.001186756999231875\n",
      "LOSS train 0.00027914182282984257 valid 0.0012721525272354484\n",
      "LOSS train 0.00027914182282984257 valid 0.0012719007208943367\n",
      "LOSS train 0.00027914182282984257 valid 0.0012461249716579914\n",
      "LOSS train 0.00027914182282984257 valid 0.0012751756003126502\n",
      "LOSS train 0.00027914182282984257 valid 0.0013782085152342916\n",
      "LOSS train 0.00027914182282984257 valid 0.0014337872853502631\n",
      "LOSS train 0.00027914182282984257 valid 0.0014346963725984097\n",
      "LOSS train 0.00027914182282984257 valid 0.0014521864941343665\n",
      "LOSS train 0.00027914182282984257 valid 0.0014611929655075073\n",
      "LOSS train 0.00027914182282984257 valid 0.0014618014683946967\n",
      "LOSS train 0.00027914182282984257 valid 0.0014405157417058945\n",
      "LOSS train 0.00027914182282984257 valid 0.0014581158757209778\n",
      "LOSS train 0.00027914182282984257 valid 0.0014650067314505577\n",
      "LOSS train 0.00027914182282984257 valid 0.001497551565989852\n",
      "EPOCH 59:\n",
      "  batch 1 loss: 0.00023860123474150896\n",
      "  batch 2 loss: 0.00014750479022040963\n",
      "  batch 3 loss: 0.00022632685431744903\n",
      "  batch 4 loss: 0.0002073518990073353\n",
      "  batch 5 loss: 0.0001682183356024325\n",
      "  batch 6 loss: 0.00016526025137864053\n",
      "  batch 7 loss: 0.00011757626634789631\n",
      "  batch 8 loss: 0.00013943834346719086\n",
      "  batch 9 loss: 0.0001522678940091282\n",
      "  batch 10 loss: 0.00011471330071799457\n",
      "  batch 11 loss: 0.00020088607561774552\n",
      "  batch 12 loss: 0.00021322503744158894\n",
      "  batch 13 loss: 0.0002451061736792326\n",
      "  batch 14 loss: 0.0004236612585373223\n",
      "  batch 15 loss: 0.0002560013090260327\n",
      "  batch 16 loss: 0.00016644161951262504\n",
      "  batch 17 loss: 0.00021083958563394845\n",
      "  batch 18 loss: 0.0001791154791135341\n",
      "  batch 19 loss: 0.00034669277374632657\n",
      "  batch 20 loss: 0.0002719453477766365\n",
      "  batch 21 loss: 0.00032629320048727095\n",
      "  batch 22 loss: 0.0002454012865200639\n",
      "  batch 23 loss: 0.00033112202072516084\n",
      "  batch 24 loss: 0.00031421007588505745\n",
      "  batch 25 loss: 0.00034811749355867505\n",
      "  batch 26 loss: 0.0003494200063869357\n",
      "  batch 27 loss: 0.00019355252152308822\n",
      "  batch 28 loss: 0.00029705328051932156\n",
      "  batch 29 loss: 0.00018909055506810546\n",
      "  batch 30 loss: 0.00017355772433802485\n",
      "  batch 31 loss: 0.0001996123173739761\n",
      "  batch 32 loss: 0.0003119093307759613\n",
      "  batch 33 loss: 0.0004041855572722852\n",
      "  batch 34 loss: 0.00033520697616040707\n",
      "  batch 35 loss: 0.00012621827772818506\n",
      "  batch 36 loss: 0.0001346027129329741\n",
      "  batch 37 loss: 0.00013959058560431004\n",
      "  batch 38 loss: 0.0001435991725884378\n",
      "  batch 39 loss: 0.00019576562044676393\n",
      "  batch 40 loss: 0.0002852383186109364\n",
      "  batch 41 loss: 0.00012890415382571518\n",
      "  batch 42 loss: 0.00015441821597050875\n",
      "  batch 43 loss: 0.00016725502791814506\n",
      "  batch 44 loss: 9.568899986334145e-05\n",
      "  batch 45 loss: 0.00012614180741366\n",
      "  batch 46 loss: 0.0001250896748388186\n",
      "  batch 47 loss: 0.00018084813200403005\n",
      "  batch 48 loss: 0.00017909641610458493\n",
      "  batch 49 loss: 0.0002107002364937216\n",
      "  batch 50 loss: 0.00025171617744490504\n",
      "  batch 51 loss: 0.00019111151050310582\n",
      "  batch 52 loss: 0.00017870607553049922\n",
      "  batch 53 loss: 0.00021943059982731938\n",
      "  batch 54 loss: 0.00018335398635827005\n",
      "  batch 55 loss: 0.00022661415277980268\n",
      "  batch 56 loss: 0.00015783518028911203\n",
      "  batch 57 loss: 0.0001254567614523694\n",
      "  batch 58 loss: 0.0001693598460406065\n",
      "  batch 59 loss: 0.00019963848171755672\n",
      "  batch 60 loss: 0.00022503601212520152\n",
      "  batch 61 loss: 0.00013014735304750502\n",
      "  batch 62 loss: 0.0001931510923895985\n",
      "  batch 63 loss: 0.00013736591790802777\n",
      "  batch 64 loss: 0.0001235514646396041\n",
      "  batch 65 loss: 0.00014005035336595029\n",
      "  batch 66 loss: 0.0001213658251799643\n",
      "  batch 67 loss: 0.00011077200178988278\n",
      "  batch 68 loss: 0.00014219683362171054\n",
      "  batch 69 loss: 0.00015024218009784818\n",
      "  batch 70 loss: 0.00022658443776890635\n",
      "  batch 71 loss: 0.00013759350986219943\n",
      "  batch 72 loss: 0.00013250437041278929\n",
      "  batch 73 loss: 0.0001320011797361076\n",
      "  batch 74 loss: 0.00017122515419032425\n",
      "  batch 75 loss: 0.00022720562992617488\n",
      "  batch 76 loss: 0.00014692175318486989\n",
      "  batch 77 loss: 0.0001554992632009089\n",
      "  batch 78 loss: 0.00014932028716430068\n",
      "  batch 79 loss: 0.0001363134797429666\n",
      "  batch 80 loss: 0.00016762111044954509\n",
      "  batch 81 loss: 0.00017037824727594852\n",
      "  batch 82 loss: 0.0001857136085163802\n",
      "  batch 83 loss: 0.00017842136730905622\n",
      "  batch 84 loss: 0.0001354450942017138\n",
      "  batch 85 loss: 0.00014947728777769953\n",
      "  batch 86 loss: 0.00017551094060763717\n",
      "  batch 87 loss: 0.00016506772954016924\n",
      "  batch 88 loss: 0.000246770738158375\n",
      "  batch 89 loss: 0.00019069394329562783\n",
      "  batch 90 loss: 0.00024841135018505156\n",
      "  batch 91 loss: 0.00017616318655200303\n",
      "  batch 92 loss: 0.00017374445451423526\n",
      "  batch 93 loss: 0.00018556165741756558\n",
      "  batch 94 loss: 0.00018635393644217402\n",
      "  batch 95 loss: 0.00026171241188421845\n",
      "LOSS train 0.00026171241188421845 valid 0.0009684930555522442\n",
      "LOSS train 0.00026171241188421845 valid 0.0011456875363364816\n",
      "LOSS train 0.00026171241188421845 valid 0.001011153683066368\n",
      "LOSS train 0.00026171241188421845 valid 0.0009677158668637276\n",
      "LOSS train 0.00026171241188421845 valid 0.0009645335376262665\n",
      "LOSS train 0.00026171241188421845 valid 0.0009049682412296534\n",
      "LOSS train 0.00026171241188421845 valid 0.0009594657458364964\n",
      "LOSS train 0.00026171241188421845 valid 0.0010046500246971846\n",
      "LOSS train 0.00026171241188421845 valid 0.0009630269487388432\n",
      "LOSS train 0.00026171241188421845 valid 0.0009883519960567355\n",
      "LOSS train 0.00026171241188421845 valid 0.0010378025472164154\n",
      "LOSS train 0.00026171241188421845 valid 0.0010394807904958725\n",
      "LOSS train 0.00026171241188421845 valid 0.0010307658230885863\n",
      "LOSS train 0.00026171241188421845 valid 0.0010560080409049988\n",
      "LOSS train 0.00026171241188421845 valid 0.0011368367122486234\n",
      "LOSS train 0.00026171241188421845 valid 0.001190992770716548\n",
      "LOSS train 0.00026171241188421845 valid 0.0011977910762652755\n",
      "LOSS train 0.00026171241188421845 valid 0.0012127712834626436\n",
      "LOSS train 0.00026171241188421845 valid 0.0012207868276163936\n",
      "LOSS train 0.00026171241188421845 valid 0.001224213163368404\n",
      "LOSS train 0.00026171241188421845 valid 0.0012109822127968073\n",
      "LOSS train 0.00026171241188421845 valid 0.0012266788398846984\n",
      "LOSS train 0.00026171241188421845 valid 0.0012299546506255865\n",
      "LOSS train 0.00026171241188421845 valid 0.0012455368414521217\n",
      "EPOCH 60:\n",
      "  batch 1 loss: 0.00023571753990836442\n",
      "  batch 2 loss: 0.00013938918709754944\n",
      "  batch 3 loss: 0.0002123248268617317\n",
      "  batch 4 loss: 0.00019358475401531905\n",
      "  batch 5 loss: 0.00015783132403157651\n",
      "  batch 6 loss: 0.0001560250821057707\n",
      "  batch 7 loss: 0.00011610187357291579\n",
      "  batch 8 loss: 0.00012536297435872257\n",
      "  batch 9 loss: 0.00014261723845265806\n",
      "  batch 10 loss: 0.00010428403038531542\n",
      "  batch 11 loss: 0.00019086258544120938\n",
      "  batch 12 loss: 0.00020524866704363376\n",
      "  batch 13 loss: 0.00023675401462242007\n",
      "  batch 14 loss: 0.0003982378402724862\n",
      "  batch 15 loss: 0.00024261568614747375\n",
      "  batch 16 loss: 0.00015528174117207527\n",
      "  batch 17 loss: 0.00017349694098811597\n",
      "  batch 18 loss: 0.00016655883518978953\n",
      "  batch 19 loss: 0.00033575049019418657\n",
      "  batch 20 loss: 0.0002575663384050131\n",
      "  batch 21 loss: 0.0003027068742085248\n",
      "  batch 22 loss: 0.00020893948385491967\n",
      "  batch 23 loss: 0.00028061913326382637\n",
      "  batch 24 loss: 0.00026558543322607875\n",
      "  batch 25 loss: 0.00032707955688238144\n",
      "  batch 26 loss: 0.00034832715755328536\n",
      "  batch 27 loss: 0.00018820229161065072\n",
      "  batch 28 loss: 0.00028003077022731304\n",
      "  batch 29 loss: 0.00018205509695690125\n",
      "  batch 30 loss: 0.00016289587074425071\n",
      "  batch 31 loss: 0.00014397490303963423\n",
      "  batch 32 loss: 0.00021923232998233289\n",
      "  batch 33 loss: 0.0002529597841203213\n",
      "  batch 34 loss: 0.0002602689201012254\n",
      "  batch 35 loss: 0.00013084607780911028\n",
      "  batch 36 loss: 0.00013478875916916877\n",
      "  batch 37 loss: 0.0001341069146292284\n",
      "  batch 38 loss: 0.00012978767335880548\n",
      "  batch 39 loss: 0.00017448315338697284\n",
      "  batch 40 loss: 0.00023830181453377008\n",
      "  batch 41 loss: 0.000116731011075899\n",
      "  batch 42 loss: 0.00014055664360057563\n",
      "  batch 43 loss: 0.00014770901179872453\n",
      "  batch 44 loss: 7.997443026397377e-05\n",
      "  batch 45 loss: 0.00010849404498003423\n",
      "  batch 46 loss: 0.00011422699753893539\n",
      "  batch 47 loss: 0.0001505753316450864\n",
      "  batch 48 loss: 0.00015541024913545698\n",
      "  batch 49 loss: 0.00018183811334893107\n",
      "  batch 50 loss: 0.0002058827376458794\n",
      "  batch 51 loss: 0.00020204682368785143\n",
      "  batch 52 loss: 0.000154892448335886\n",
      "  batch 53 loss: 0.00017784515512175858\n",
      "  batch 54 loss: 0.0001697637781035155\n",
      "  batch 55 loss: 0.0002141638397006318\n",
      "  batch 56 loss: 0.0001488623965997249\n",
      "  batch 57 loss: 0.00011132618237752467\n",
      "  batch 58 loss: 0.00015234397142194211\n",
      "  batch 59 loss: 0.00018254801398143172\n",
      "  batch 60 loss: 0.00021642891806550324\n",
      "  batch 61 loss: 0.00011044049460906535\n",
      "  batch 62 loss: 0.00017260391905438155\n",
      "  batch 63 loss: 0.00012435123790055513\n",
      "  batch 64 loss: 0.00011028083099517971\n",
      "  batch 65 loss: 0.00012143018102506176\n",
      "  batch 66 loss: 0.00010960925283143297\n",
      "  batch 67 loss: 0.00010967048001475632\n",
      "  batch 68 loss: 0.00013634905917569995\n",
      "  batch 69 loss: 0.00014629239740315825\n",
      "  batch 70 loss: 0.00022544956300407648\n",
      "  batch 71 loss: 0.00013180500536691397\n",
      "  batch 72 loss: 0.00012923417671117932\n",
      "  batch 73 loss: 0.00012976460857316852\n",
      "  batch 74 loss: 0.00016596174100413918\n",
      "  batch 75 loss: 0.00022296272800303996\n",
      "  batch 76 loss: 0.0001471400319132954\n",
      "  batch 77 loss: 0.00014638688298873603\n",
      "  batch 78 loss: 0.00013978520291857421\n",
      "  batch 79 loss: 0.00012992628035135567\n",
      "  batch 80 loss: 0.00015851977514103055\n",
      "  batch 81 loss: 0.00015905036707408726\n",
      "  batch 82 loss: 0.00017642350576352328\n",
      "  batch 83 loss: 0.00017047065193764865\n",
      "  batch 84 loss: 0.00013270735507830977\n",
      "  batch 85 loss: 0.0001442429202143103\n",
      "  batch 86 loss: 0.00016150384908542037\n",
      "  batch 87 loss: 0.000157383838086389\n",
      "  batch 88 loss: 0.0002356928016524762\n",
      "  batch 89 loss: 0.00018323941912967712\n",
      "  batch 90 loss: 0.00023442128440365195\n",
      "  batch 91 loss: 0.00016048089310061187\n",
      "  batch 92 loss: 0.0001639449328649789\n",
      "  batch 93 loss: 0.00017100450349971652\n",
      "  batch 94 loss: 0.00017675000708550215\n",
      "  batch 95 loss: 0.00025071619893424213\n",
      "LOSS train 0.00025071619893424213 valid 0.0010888815158978105\n",
      "LOSS train 0.00025071619893424213 valid 0.0013153208419680595\n",
      "LOSS train 0.00025071619893424213 valid 0.0011485135182738304\n",
      "LOSS train 0.00025071619893424213 valid 0.0010976559715345502\n",
      "LOSS train 0.00025071619893424213 valid 0.0010912802536040545\n",
      "LOSS train 0.00025071619893424213 valid 0.0010237102396786213\n",
      "LOSS train 0.00025071619893424213 valid 0.0010975368786603212\n",
      "LOSS train 0.00025071619893424213 valid 0.0011361591750755906\n",
      "LOSS train 0.00025071619893424213 valid 0.0010856051230803132\n",
      "LOSS train 0.00025071619893424213 valid 0.0011178520508110523\n",
      "LOSS train 0.00025071619893424213 valid 0.001210949383676052\n",
      "LOSS train 0.00025071619893424213 valid 0.0012097316794097424\n",
      "LOSS train 0.00025071619893424213 valid 0.0011932047782465816\n",
      "LOSS train 0.00025071619893424213 valid 0.0012176454765722156\n",
      "LOSS train 0.00025071619893424213 valid 0.0013046046951785684\n",
      "LOSS train 0.00025071619893424213 valid 0.0013590893941000104\n",
      "LOSS train 0.00025071619893424213 valid 0.0013636115472763777\n",
      "LOSS train 0.00025071619893424213 valid 0.0013810412492603064\n",
      "LOSS train 0.00025071619893424213 valid 0.001389009295962751\n",
      "LOSS train 0.00025071619893424213 valid 0.0013977768830955029\n",
      "LOSS train 0.00025071619893424213 valid 0.0013827488292008638\n",
      "LOSS train 0.00025071619893424213 valid 0.0013980790972709656\n",
      "LOSS train 0.00025071619893424213 valid 0.001403462840244174\n",
      "LOSS train 0.00025071619893424213 valid 0.0014271694235503674\n",
      "EPOCH 61:\n",
      "  batch 1 loss: 0.0002297468890901655\n",
      "  batch 2 loss: 0.00013733188097830862\n",
      "  batch 3 loss: 0.00021423035650514066\n",
      "  batch 4 loss: 0.0001850173866841942\n",
      "  batch 5 loss: 0.00014811361324973404\n",
      "  batch 6 loss: 0.00014883351104799658\n",
      "  batch 7 loss: 0.00010639989341143519\n",
      "  batch 8 loss: 0.00011854337935801595\n",
      "  batch 9 loss: 0.00013781845336779952\n",
      "  batch 10 loss: 0.00010061782813863829\n",
      "  batch 11 loss: 0.0001728006318444386\n",
      "  batch 12 loss: 0.00018870607891585678\n",
      "  batch 13 loss: 0.00022573991736862808\n",
      "  batch 14 loss: 0.00036032285424880683\n",
      "  batch 15 loss: 0.00022755260579288006\n",
      "  batch 16 loss: 0.00015642360085621476\n",
      "  batch 17 loss: 0.0001666877360548824\n",
      "  batch 18 loss: 0.00015695876209065318\n",
      "  batch 19 loss: 0.00031417456921190023\n",
      "  batch 20 loss: 0.00023236617562361062\n",
      "  batch 21 loss: 0.0002886903821490705\n",
      "  batch 22 loss: 0.00020952936029061675\n",
      "  batch 23 loss: 0.0003259680815972388\n",
      "  batch 24 loss: 0.0002690035616979003\n",
      "  batch 25 loss: 0.00031046662479639053\n",
      "  batch 26 loss: 0.00034988531842827797\n",
      "  batch 27 loss: 0.00016317007248289883\n",
      "  batch 28 loss: 0.00028056930750608444\n",
      "  batch 29 loss: 0.00015857411199249327\n",
      "  batch 30 loss: 0.00013972987653687596\n",
      "  batch 31 loss: 0.00013074284652248025\n",
      "  batch 32 loss: 0.00021038774866610765\n",
      "  batch 33 loss: 0.00021984014892950654\n",
      "  batch 34 loss: 0.00029522471595555544\n",
      "  batch 35 loss: 0.00014860258670523763\n",
      "  batch 36 loss: 0.00014924861898180097\n",
      "  batch 37 loss: 0.00014356365136336535\n",
      "  batch 38 loss: 0.00013491905701812357\n",
      "  batch 39 loss: 0.00017532572383061051\n",
      "  batch 40 loss: 0.00023254734696820378\n",
      "  batch 41 loss: 0.00011292577255517244\n",
      "  batch 42 loss: 0.00014869170263409615\n",
      "  batch 43 loss: 0.0001511054579168558\n",
      "  batch 44 loss: 0.00010694233060348779\n",
      "  batch 45 loss: 0.00013755523832514882\n",
      "  batch 46 loss: 0.00016234206850640476\n",
      "  batch 47 loss: 0.00015132782573346049\n",
      "  batch 48 loss: 0.0001475853641750291\n",
      "  batch 49 loss: 0.0001709865900920704\n",
      "  batch 50 loss: 0.00020091947226319462\n",
      "  batch 51 loss: 0.000180108065251261\n",
      "  batch 52 loss: 0.00015292546595446765\n",
      "  batch 53 loss: 0.00017314639990217984\n",
      "  batch 54 loss: 0.0001701684668660164\n",
      "  batch 55 loss: 0.00021472517983056605\n",
      "  batch 56 loss: 0.000151907152030617\n",
      "  batch 57 loss: 0.00012467397027648985\n",
      "  batch 58 loss: 0.00014896599168423563\n",
      "  batch 59 loss: 0.00016478242469020188\n",
      "  batch 60 loss: 0.0002186048513976857\n",
      "  batch 61 loss: 0.00011734510189853609\n",
      "  batch 62 loss: 0.00018265307880938053\n",
      "  batch 63 loss: 0.00012146691733505577\n",
      "  batch 64 loss: 0.00011123363219667226\n",
      "  batch 65 loss: 0.00011982420983258635\n",
      "  batch 66 loss: 0.00011452050239313394\n",
      "  batch 67 loss: 0.00011732266284525394\n",
      "  batch 68 loss: 0.00014674363774247468\n",
      "  batch 69 loss: 0.00014654223923571408\n",
      "  batch 70 loss: 0.00021913269301876426\n",
      "  batch 71 loss: 0.00012925885675940663\n",
      "  batch 72 loss: 0.00013379077427089214\n",
      "  batch 73 loss: 0.00013188543380238116\n",
      "  batch 74 loss: 0.00017090135952457786\n",
      "  batch 75 loss: 0.0002340990031370893\n",
      "  batch 76 loss: 0.00015597982564941049\n",
      "  batch 77 loss: 0.0001491260773036629\n",
      "  batch 78 loss: 0.00014734521391801536\n",
      "  batch 79 loss: 0.00013152070459909737\n",
      "  batch 80 loss: 0.00016413474804721773\n",
      "  batch 81 loss: 0.00016255752416327596\n",
      "  batch 82 loss: 0.0001804086787160486\n",
      "  batch 83 loss: 0.00017458517686463892\n",
      "  batch 84 loss: 0.0001340277085546404\n",
      "  batch 85 loss: 0.0001465905224904418\n",
      "  batch 86 loss: 0.00016892516578081995\n",
      "  batch 87 loss: 0.00015813278150744736\n",
      "  batch 88 loss: 0.00024174508871510625\n",
      "  batch 89 loss: 0.00018055917462334037\n",
      "  batch 90 loss: 0.0002333500888198614\n",
      "  batch 91 loss: 0.00015657918993383646\n",
      "  batch 92 loss: 0.0001682498404989019\n",
      "  batch 93 loss: 0.0001731161173665896\n",
      "  batch 94 loss: 0.0001759113511070609\n",
      "  batch 95 loss: 0.00026416152832098305\n",
      "LOSS train 0.00026416152832098305 valid 0.0010722011793404818\n",
      "LOSS train 0.00026416152832098305 valid 0.0012800507247447968\n",
      "LOSS train 0.00026416152832098305 valid 0.0011286295484751463\n",
      "LOSS train 0.00026416152832098305 valid 0.0010779105359688401\n",
      "LOSS train 0.00026416152832098305 valid 0.001065557124093175\n",
      "LOSS train 0.00026416152832098305 valid 0.0009974280837923288\n",
      "LOSS train 0.00026416152832098305 valid 0.0010799148585647345\n",
      "LOSS train 0.00026416152832098305 valid 0.0011209058575332165\n",
      "LOSS train 0.00026416152832098305 valid 0.001071336679160595\n",
      "LOSS train 0.00026416152832098305 valid 0.0010992836905643344\n",
      "LOSS train 0.00026416152832098305 valid 0.001176929916255176\n",
      "LOSS train 0.00026416152832098305 valid 0.0011744900839403272\n",
      "LOSS train 0.00026416152832098305 valid 0.0011604288592934608\n",
      "LOSS train 0.00026416152832098305 valid 0.0011860019294545054\n",
      "LOSS train 0.00026416152832098305 valid 0.001280172960832715\n",
      "LOSS train 0.00026416152832098305 valid 0.001333211432211101\n",
      "LOSS train 0.00026416152832098305 valid 0.0013373299734666944\n",
      "LOSS train 0.00026416152832098305 valid 0.001354093081317842\n",
      "LOSS train 0.00026416152832098305 valid 0.0013596337521448731\n",
      "LOSS train 0.00026416152832098305 valid 0.0013601983664557338\n",
      "LOSS train 0.00026416152832098305 valid 0.0013455018633976579\n",
      "LOSS train 0.00026416152832098305 valid 0.0013616225915029645\n",
      "LOSS train 0.00026416152832098305 valid 0.0013708729529753327\n",
      "LOSS train 0.00026416152832098305 valid 0.0013986551202833652\n",
      "EPOCH 62:\n",
      "  batch 1 loss: 0.00022232996707316488\n",
      "  batch 2 loss: 0.00013741053408011794\n",
      "  batch 3 loss: 0.0002166936465073377\n",
      "  batch 4 loss: 0.00018928575445897877\n",
      "  batch 5 loss: 0.00016149233852047473\n",
      "  batch 6 loss: 0.00015885895118117332\n",
      "  batch 7 loss: 0.00011395826004445553\n",
      "  batch 8 loss: 0.00012575049186125398\n",
      "  batch 9 loss: 0.0001362306356895715\n",
      "  batch 10 loss: 0.0001025678648147732\n",
      "  batch 11 loss: 0.00016867434896994382\n",
      "  batch 12 loss: 0.00019339527352713048\n",
      "  batch 13 loss: 0.0002283349313074723\n",
      "  batch 14 loss: 0.00040381529834121466\n",
      "  batch 15 loss: 0.00023086986038833857\n",
      "  batch 16 loss: 0.00015953257388900965\n",
      "  batch 17 loss: 0.0001846938394010067\n",
      "  batch 18 loss: 0.00016239422257058322\n",
      "  batch 19 loss: 0.0003039714938495308\n",
      "  batch 20 loss: 0.00026548723690211773\n",
      "  batch 21 loss: 0.0002964802260976285\n",
      "  batch 22 loss: 0.00019470573170110583\n",
      "  batch 23 loss: 0.0002880701213143766\n",
      "  batch 24 loss: 0.000254205078817904\n",
      "  batch 25 loss: 0.0003167831455357373\n",
      "  batch 26 loss: 0.0003325272991787642\n",
      "  batch 27 loss: 0.00017369906709063798\n",
      "  batch 28 loss: 0.0002804749528877437\n",
      "  batch 29 loss: 0.0001634533837204799\n",
      "  batch 30 loss: 0.00014539339463226497\n",
      "  batch 31 loss: 0.00013367827341426164\n",
      "  batch 32 loss: 0.00020331089035607874\n",
      "  batch 33 loss: 0.00023120900732465088\n",
      "  batch 34 loss: 0.00024759204825386405\n",
      "  batch 35 loss: 0.00014336220920085907\n",
      "  batch 36 loss: 0.0001278346317121759\n",
      "  batch 37 loss: 0.00014974104124121368\n",
      "  batch 38 loss: 0.00014159330748952925\n",
      "  batch 39 loss: 0.0001711393124423921\n",
      "  batch 40 loss: 0.00019476257148198783\n",
      "  batch 41 loss: 0.00010136494529433548\n",
      "  batch 42 loss: 0.0001414890866726637\n",
      "  batch 43 loss: 0.00012083108595106751\n",
      "  batch 44 loss: 7.764266047161072e-05\n",
      "  batch 45 loss: 0.00010590349847916514\n",
      "  batch 46 loss: 0.00013318980927579105\n",
      "  batch 47 loss: 0.00016135418263729662\n",
      "  batch 48 loss: 0.0001480474165873602\n",
      "  batch 49 loss: 0.00018623592040967196\n",
      "  batch 50 loss: 0.00020734022837132215\n",
      "  batch 51 loss: 0.00018921981973107904\n",
      "  batch 52 loss: 0.0001627173915039748\n",
      "  batch 53 loss: 0.00015815693768672645\n",
      "  batch 54 loss: 0.00015234581951517612\n",
      "  batch 55 loss: 0.00019998164498247206\n",
      "  batch 56 loss: 0.00014560935960616916\n",
      "  batch 57 loss: 0.00012487638741731644\n",
      "  batch 58 loss: 0.00014519566320814192\n",
      "  batch 59 loss: 0.00015772550250403583\n",
      "  batch 60 loss: 0.00020342065545264632\n",
      "  batch 61 loss: 0.00010202688281424344\n",
      "  batch 62 loss: 0.00017375618335790932\n",
      "  batch 63 loss: 0.00012176401651231572\n",
      "  batch 64 loss: 0.00010297419066773728\n",
      "  batch 65 loss: 0.00010972705786116421\n",
      "  batch 66 loss: 0.00010894873412325978\n",
      "  batch 67 loss: 0.00010784345795400441\n",
      "  batch 68 loss: 0.00012892995437141508\n",
      "  batch 69 loss: 0.00013707048492506146\n",
      "  batch 70 loss: 0.00021366574219428003\n",
      "  batch 71 loss: 0.0001287812483496964\n",
      "  batch 72 loss: 0.00013103208038955927\n",
      "  batch 73 loss: 0.00012488993525039405\n",
      "  batch 74 loss: 0.00016523277736268938\n",
      "  batch 75 loss: 0.0002216215361841023\n",
      "  batch 76 loss: 0.0001484211243223399\n",
      "  batch 77 loss: 0.00014015231863595545\n",
      "  batch 78 loss: 0.00014403903333004564\n",
      "  batch 79 loss: 0.00012300591333769262\n",
      "  batch 80 loss: 0.00015688949497416615\n",
      "  batch 81 loss: 0.00015608090325258672\n",
      "  batch 82 loss: 0.0001771448296494782\n",
      "  batch 83 loss: 0.00016868420061655343\n",
      "  batch 84 loss: 0.00012778995733242482\n",
      "  batch 85 loss: 0.00013902272621635348\n",
      "  batch 86 loss: 0.00016439550381619483\n",
      "  batch 87 loss: 0.0001588656014064327\n",
      "  batch 88 loss: 0.00023929777671582997\n",
      "  batch 89 loss: 0.00016967739793471992\n",
      "  batch 90 loss: 0.00022913841530680656\n",
      "  batch 91 loss: 0.00015409974730573595\n",
      "  batch 92 loss: 0.000158244016347453\n",
      "  batch 93 loss: 0.00016882954514585435\n",
      "  batch 94 loss: 0.00016949088603723794\n",
      "  batch 95 loss: 0.00026462131063453853\n",
      "LOSS train 0.00026462131063453853 valid 0.0011992142535746098\n",
      "LOSS train 0.00026462131063453853 valid 0.0013710653875023127\n",
      "LOSS train 0.00026462131063453853 valid 0.0012065268820151687\n",
      "LOSS train 0.00026462131063453853 valid 0.0011560814455151558\n",
      "LOSS train 0.00026462131063453853 valid 0.0011499529937282205\n",
      "LOSS train 0.00026462131063453853 valid 0.0010788871441036463\n",
      "LOSS train 0.00026462131063453853 valid 0.0011453998740762472\n",
      "LOSS train 0.00026462131063453853 valid 0.001188511960208416\n",
      "LOSS train 0.00026462131063453853 valid 0.0011358516057953238\n",
      "LOSS train 0.00026462131063453853 valid 0.001168109942227602\n",
      "LOSS train 0.00026462131063453853 valid 0.0012615645537152886\n",
      "LOSS train 0.00026462131063453853 valid 0.0012664900859817863\n",
      "LOSS train 0.00026462131063453853 valid 0.0012489046202972531\n",
      "LOSS train 0.00026462131063453853 valid 0.0012748830486088991\n",
      "LOSS train 0.00026462131063453853 valid 0.0013868861133232713\n",
      "LOSS train 0.00026462131063453853 valid 0.001440207357518375\n",
      "LOSS train 0.00026462131063453853 valid 0.0014410396106541157\n",
      "LOSS train 0.00026462131063453853 valid 0.0014585718745365739\n",
      "LOSS train 0.00026462131063453853 valid 0.0014676018618047237\n",
      "LOSS train 0.00026462131063453853 valid 0.0014726875815540552\n",
      "LOSS train 0.00026462131063453853 valid 0.0014571829233318567\n",
      "LOSS train 0.00026462131063453853 valid 0.0014728314708918333\n",
      "LOSS train 0.00026462131063453853 valid 0.0014778305776417255\n",
      "LOSS train 0.00026462131063453853 valid 0.0015029991045594215\n",
      "EPOCH 63:\n",
      "  batch 1 loss: 0.000216001775697805\n",
      "  batch 2 loss: 0.00013025557564105839\n",
      "  batch 3 loss: 0.00020969004253856838\n",
      "  batch 4 loss: 0.0001837722084019333\n",
      "  batch 5 loss: 0.00015201146015897393\n",
      "  batch 6 loss: 0.0001566528808325529\n",
      "  batch 7 loss: 0.0001087087148334831\n",
      "  batch 8 loss: 0.000126753919175826\n",
      "  batch 9 loss: 0.00013190701429266483\n",
      "  batch 10 loss: 0.00010158864461118355\n",
      "  batch 11 loss: 0.0001667942851781845\n",
      "  batch 12 loss: 0.00019777815032284707\n",
      "  batch 13 loss: 0.00022625242127105594\n",
      "  batch 14 loss: 0.0003538985620252788\n",
      "  batch 15 loss: 0.00021915629622526467\n",
      "  batch 16 loss: 0.0001517247874289751\n",
      "  batch 17 loss: 0.00017778133042156696\n",
      "  batch 18 loss: 0.00015638230252079666\n",
      "  batch 19 loss: 0.0002987575135193765\n",
      "  batch 20 loss: 0.00023819046327844262\n",
      "  batch 21 loss: 0.00034906924702227116\n",
      "  batch 22 loss: 0.00020690588280558586\n",
      "  batch 23 loss: 0.0002801741939038038\n",
      "  batch 24 loss: 0.0002635940327309072\n",
      "  batch 25 loss: 0.0003213781164959073\n",
      "  batch 26 loss: 0.00033690384589135647\n",
      "  batch 27 loss: 0.00017295073485001922\n",
      "  batch 28 loss: 0.00030291173607110977\n",
      "  batch 29 loss: 0.000168569793459028\n",
      "  batch 30 loss: 0.00015995014109648764\n",
      "  batch 31 loss: 0.00012479763245210052\n",
      "  batch 32 loss: 0.00019441504264250398\n",
      "  batch 33 loss: 0.00020733720157295465\n",
      "  batch 34 loss: 0.00026156072271987796\n",
      "  batch 35 loss: 0.00016185533604584634\n",
      "  batch 36 loss: 0.00015763539704494178\n",
      "  batch 37 loss: 0.00018034345703199506\n",
      "  batch 38 loss: 0.00014798282063566148\n",
      "  batch 39 loss: 0.00017179219867102802\n",
      "  batch 40 loss: 0.00020689373195637017\n",
      "  batch 41 loss: 0.00011032579641323537\n",
      "  batch 42 loss: 0.00013697828399017453\n",
      "  batch 43 loss: 0.00013647793093696237\n",
      "  batch 44 loss: 8.853204781189561e-05\n",
      "  batch 45 loss: 0.0001182298146886751\n",
      "  batch 46 loss: 0.00013026056694798172\n",
      "  batch 47 loss: 0.0001730042858980596\n",
      "  batch 48 loss: 0.00016089901328086853\n",
      "  batch 49 loss: 0.00019791623344644904\n",
      "  batch 50 loss: 0.00022162217646837234\n",
      "  batch 51 loss: 0.00019297102699056268\n",
      "  batch 52 loss: 0.00017305205983575433\n",
      "  batch 53 loss: 0.0001883358636405319\n",
      "  batch 54 loss: 0.0001788055815268308\n",
      "  batch 55 loss: 0.00022754690144211054\n",
      "  batch 56 loss: 0.0001539302320452407\n",
      "  batch 57 loss: 0.00012952060205861926\n",
      "  batch 58 loss: 0.0001467623806092888\n",
      "  batch 59 loss: 0.00017007632413879037\n",
      "  batch 60 loss: 0.00021239297348074615\n",
      "  batch 61 loss: 0.00011881689715664834\n",
      "  batch 62 loss: 0.00018650297715794295\n",
      "  batch 63 loss: 0.0001434476871509105\n",
      "  batch 64 loss: 0.00010753075912361965\n",
      "  batch 65 loss: 0.0001182176565635018\n",
      "  batch 66 loss: 0.00011335514136590064\n",
      "  batch 67 loss: 0.00011461975373094901\n",
      "  batch 68 loss: 0.0001313945685978979\n",
      "  batch 69 loss: 0.0001410038094036281\n",
      "  batch 70 loss: 0.0002150794316548854\n",
      "  batch 71 loss: 0.00013135335757397115\n",
      "  batch 72 loss: 0.00012807473831344396\n",
      "  batch 73 loss: 0.00012387394963297993\n",
      "  batch 74 loss: 0.00015863511362113059\n",
      "  batch 75 loss: 0.00022201878891792148\n",
      "  batch 76 loss: 0.00014961499255150557\n",
      "  batch 77 loss: 0.00014771288260817528\n",
      "  batch 78 loss: 0.00013939781638327986\n",
      "  batch 79 loss: 0.00012074584083165973\n",
      "  batch 80 loss: 0.0001518284116173163\n",
      "  batch 81 loss: 0.0001510781585238874\n",
      "  batch 82 loss: 0.0001715438556857407\n",
      "  batch 83 loss: 0.00017023472173605114\n",
      "  batch 84 loss: 0.00012486043851822615\n",
      "  batch 85 loss: 0.00014164972526486963\n",
      "  batch 86 loss: 0.00016323896124958992\n",
      "  batch 87 loss: 0.00016117635823320597\n",
      "  batch 88 loss: 0.00024524860782548785\n",
      "  batch 89 loss: 0.0001802070764824748\n",
      "  batch 90 loss: 0.0002439395902911201\n",
      "  batch 91 loss: 0.0001587836304679513\n",
      "  batch 92 loss: 0.00015929268556647003\n",
      "  batch 93 loss: 0.00016833332483656704\n",
      "  batch 94 loss: 0.00017209214274771512\n",
      "  batch 95 loss: 0.0002604008768685162\n",
      "LOSS train 0.0002604008768685162 valid 0.0011787201510742307\n",
      "LOSS train 0.0002604008768685162 valid 0.00129650067538023\n",
      "LOSS train 0.0002604008768685162 valid 0.0011675971327349544\n",
      "LOSS train 0.0002604008768685162 valid 0.001122859655879438\n",
      "LOSS train 0.0002604008768685162 valid 0.0011254738783463836\n",
      "LOSS train 0.0002604008768685162 valid 0.0010571152670308948\n",
      "LOSS train 0.0002604008768685162 valid 0.0011102028656750917\n",
      "LOSS train 0.0002604008768685162 valid 0.0011559759732335806\n",
      "LOSS train 0.0002604008768685162 valid 0.0011122506111860275\n",
      "LOSS train 0.0002604008768685162 valid 0.0011356229661032557\n",
      "LOSS train 0.0002604008768685162 valid 0.001204665401019156\n",
      "LOSS train 0.0002604008768685162 valid 0.0012058708816766739\n",
      "LOSS train 0.0002604008768685162 valid 0.0011998413829132915\n",
      "LOSS train 0.0002604008768685162 valid 0.001228684326633811\n",
      "LOSS train 0.0002604008768685162 valid 0.001350002596154809\n",
      "LOSS train 0.0002604008768685162 valid 0.0014071185141801834\n",
      "LOSS train 0.0002604008768685162 valid 0.0014076322549954057\n",
      "LOSS train 0.0002604008768685162 valid 0.0014230153756216168\n",
      "LOSS train 0.0002604008768685162 valid 0.001429612166248262\n",
      "LOSS train 0.0002604008768685162 valid 0.0014245876809582114\n",
      "LOSS train 0.0002604008768685162 valid 0.0014065077994018793\n",
      "LOSS train 0.0002604008768685162 valid 0.001420111395418644\n",
      "LOSS train 0.0002604008768685162 valid 0.0014209530781954527\n",
      "LOSS train 0.0002604008768685162 valid 0.0014327368699014187\n",
      "EPOCH 64:\n",
      "  batch 1 loss: 0.00022557030024472624\n",
      "  batch 2 loss: 0.00014191147056408226\n",
      "  batch 3 loss: 0.0002173162647522986\n",
      "  batch 4 loss: 0.00020013337780255824\n",
      "  batch 5 loss: 0.0001623565622139722\n",
      "  batch 6 loss: 0.00016114464960992336\n",
      "  batch 7 loss: 0.0001132025572587736\n",
      "  batch 8 loss: 0.00012452484224922955\n",
      "  batch 9 loss: 0.00013760788715444505\n",
      "  batch 10 loss: 0.00010000728070735931\n",
      "  batch 11 loss: 0.00018562027253210545\n",
      "  batch 12 loss: 0.000197204586584121\n",
      "  batch 13 loss: 0.0002307500399183482\n",
      "  batch 14 loss: 0.00035065627889707685\n",
      "  batch 15 loss: 0.00022462356719188392\n",
      "  batch 16 loss: 0.00015105260536074638\n",
      "  batch 17 loss: 0.0001797411823645234\n",
      "  batch 18 loss: 0.00015097395225893706\n",
      "  batch 19 loss: 0.0002814058680087328\n",
      "  batch 20 loss: 0.00023653777316212654\n",
      "  batch 21 loss: 0.00030191632686182857\n",
      "  batch 22 loss: 0.0001882525102701038\n",
      "  batch 23 loss: 0.0002685548970475793\n",
      "  batch 24 loss: 0.00024557943106628954\n",
      "  batch 25 loss: 0.00030329020228236914\n",
      "  batch 26 loss: 0.00030883977888152003\n",
      "  batch 27 loss: 0.00015598494792357087\n",
      "  batch 28 loss: 0.00024321598175447434\n",
      "  batch 29 loss: 0.00014343303337227553\n",
      "  batch 30 loss: 0.0001430798729415983\n",
      "  batch 31 loss: 0.00011468502634670585\n",
      "  batch 32 loss: 0.0001960296358447522\n",
      "  batch 33 loss: 0.00017534816288389266\n",
      "  batch 34 loss: 0.00022297620307654142\n",
      "  batch 35 loss: 0.00013680188567377627\n",
      "  batch 36 loss: 0.00012277317000553012\n",
      "  batch 37 loss: 0.00017356104217469692\n",
      "  batch 38 loss: 0.00017519821994937956\n",
      "  batch 39 loss: 0.00019768084166571498\n",
      "  batch 40 loss: 0.0002038044622167945\n",
      "  batch 41 loss: 0.0001226786116603762\n",
      "  batch 42 loss: 0.00014277774607762694\n",
      "  batch 43 loss: 0.00011764081136789173\n",
      "  batch 44 loss: 7.935296889627352e-05\n",
      "  batch 45 loss: 0.00010221792035736144\n",
      "  batch 46 loss: 0.0001262305595446378\n",
      "  batch 47 loss: 0.00016474651056341827\n",
      "  batch 48 loss: 0.0001472338626626879\n",
      "  batch 49 loss: 0.0001830664259614423\n",
      "  batch 50 loss: 0.00019964121747761965\n",
      "  batch 51 loss: 0.0002511437050998211\n",
      "  batch 52 loss: 0.00017664514598436654\n",
      "  batch 53 loss: 0.00017245238996110857\n",
      "  batch 54 loss: 0.00017079085228033364\n",
      "  batch 55 loss: 0.00019820334273390472\n",
      "  batch 56 loss: 0.00013846413639839739\n",
      "  batch 57 loss: 0.00011703970085363835\n",
      "  batch 58 loss: 0.0001429891271982342\n",
      "  batch 59 loss: 0.00018398778047412634\n",
      "  batch 60 loss: 0.00021342521358747035\n",
      "  batch 61 loss: 0.00010138873767573386\n",
      "  batch 62 loss: 0.00021798476518597454\n",
      "  batch 63 loss: 0.0001564541453262791\n",
      "  batch 64 loss: 0.00010908905824180692\n",
      "  batch 65 loss: 0.0001169843235402368\n",
      "  batch 66 loss: 0.00012044775212416425\n",
      "  batch 67 loss: 0.000126257335068658\n",
      "  batch 68 loss: 0.00014832754095550627\n",
      "  batch 69 loss: 0.00014151108916848898\n",
      "  batch 70 loss: 0.00020871913875453174\n",
      "  batch 71 loss: 0.00013825291534885764\n",
      "  batch 72 loss: 0.0001346699136774987\n",
      "  batch 73 loss: 0.00013262478751130402\n",
      "  batch 74 loss: 0.00016178638907149434\n",
      "  batch 75 loss: 0.0002234016137663275\n",
      "  batch 76 loss: 0.0001493606250733137\n",
      "  batch 77 loss: 0.0001556983042974025\n",
      "  batch 78 loss: 0.00014283074415288866\n",
      "  batch 79 loss: 0.00011974160588579252\n",
      "  batch 80 loss: 0.0001534142211312428\n",
      "  batch 81 loss: 0.00015597025048919022\n",
      "  batch 82 loss: 0.0001664127630647272\n",
      "  batch 83 loss: 0.00016432549455203116\n",
      "  batch 84 loss: 0.00012942132889293134\n",
      "  batch 85 loss: 0.00014296775043476373\n",
      "  batch 86 loss: 0.0001569082960486412\n",
      "  batch 87 loss: 0.000154361710883677\n",
      "  batch 88 loss: 0.0002263979840790853\n",
      "  batch 89 loss: 0.0001741319429129362\n",
      "  batch 90 loss: 0.0002320767380297184\n",
      "  batch 91 loss: 0.000152832071762532\n",
      "  batch 92 loss: 0.0001640649716136977\n",
      "  batch 93 loss: 0.00017635792028158903\n",
      "  batch 94 loss: 0.00018347709556110203\n",
      "  batch 95 loss: 0.00026423903182148933\n",
      "LOSS train 0.00026423903182148933 valid 0.0010388764785602689\n",
      "LOSS train 0.00026423903182148933 valid 0.0011643980396911502\n",
      "LOSS train 0.00026423903182148933 valid 0.0010733407689258456\n",
      "LOSS train 0.00026423903182148933 valid 0.00103689671959728\n",
      "LOSS train 0.00026423903182148933 valid 0.0010335433762520552\n",
      "LOSS train 0.00026423903182148933 valid 0.0009685463737696409\n",
      "LOSS train 0.00026423903182148933 valid 0.0010211477056145668\n",
      "LOSS train 0.00026423903182148933 valid 0.001073409803211689\n",
      "LOSS train 0.00026423903182148933 valid 0.0010363549226894975\n",
      "LOSS train 0.00026423903182148933 valid 0.0010575816268101335\n",
      "LOSS train 0.00026423903182148933 valid 0.0011053583584725857\n",
      "LOSS train 0.00026423903182148933 valid 0.001109390053898096\n",
      "LOSS train 0.00026423903182148933 valid 0.0011085617588832974\n",
      "LOSS train 0.00026423903182148933 valid 0.0011309250257909298\n",
      "LOSS train 0.00026423903182148933 valid 0.0012164624640718102\n",
      "LOSS train 0.00026423903182148933 valid 0.0012589279795065522\n",
      "LOSS train 0.00026423903182148933 valid 0.0012598985340446234\n",
      "LOSS train 0.00026423903182148933 valid 0.0012766836443915963\n",
      "LOSS train 0.00026423903182148933 valid 0.0012800212716683745\n",
      "LOSS train 0.00026423903182148933 valid 0.001275074784643948\n",
      "LOSS train 0.00026423903182148933 valid 0.0012587725650519133\n",
      "LOSS train 0.00026423903182148933 valid 0.0012740511447191238\n",
      "LOSS train 0.00026423903182148933 valid 0.0012710096780210733\n",
      "LOSS train 0.00026423903182148933 valid 0.0012696338817477226\n",
      "EPOCH 65:\n",
      "  batch 1 loss: 0.00021890403877478093\n",
      "  batch 2 loss: 0.0001284610916627571\n",
      "  batch 3 loss: 0.0002162957825930789\n",
      "  batch 4 loss: 0.00018767223809845746\n",
      "  batch 5 loss: 0.00016169917944353074\n",
      "  batch 6 loss: 0.00015903828898444772\n",
      "  batch 7 loss: 0.00013249009498395026\n",
      "  batch 8 loss: 0.00014705973444506526\n",
      "  batch 9 loss: 0.00013688229955732822\n",
      "  batch 10 loss: 0.00010572883184067905\n",
      "  batch 11 loss: 0.00016466539818793535\n",
      "  batch 12 loss: 0.00019919616170227528\n",
      "  batch 13 loss: 0.0002320234343642369\n",
      "  batch 14 loss: 0.00035135296639055014\n",
      "  batch 15 loss: 0.000227318552788347\n",
      "  batch 16 loss: 0.00014687042857985944\n",
      "  batch 17 loss: 0.00016583409160375595\n",
      "  batch 18 loss: 0.00015234603779390454\n",
      "  batch 19 loss: 0.0002704631770029664\n",
      "  batch 20 loss: 0.0002374046598561108\n",
      "  batch 21 loss: 0.0002829373988788575\n",
      "  batch 22 loss: 0.00018258989439345896\n",
      "  batch 23 loss: 0.0002814854378812015\n",
      "  batch 24 loss: 0.00024532972020097077\n",
      "  batch 25 loss: 0.000306320987874642\n",
      "  batch 26 loss: 0.00031266664154827595\n",
      "  batch 27 loss: 0.00014251016546040773\n",
      "  batch 28 loss: 0.00024024352023843676\n",
      "  batch 29 loss: 0.00015952676767483354\n",
      "  batch 30 loss: 0.00014122937864158303\n",
      "  batch 31 loss: 0.00010587205179035664\n",
      "  batch 32 loss: 0.00018823158461600542\n",
      "  batch 33 loss: 0.0001680981513345614\n",
      "  batch 34 loss: 0.00020821353245992213\n",
      "  batch 35 loss: 0.00011374885070836172\n",
      "  batch 36 loss: 0.0001132022007368505\n",
      "  batch 37 loss: 0.0001393041166011244\n",
      "  batch 38 loss: 0.00014113672659732401\n",
      "  batch 39 loss: 0.00018169880786444992\n",
      "  batch 40 loss: 0.00020966510055586696\n",
      "  batch 41 loss: 0.00015097938012331724\n",
      "  batch 42 loss: 0.0001472012372687459\n",
      "  batch 43 loss: 0.00011969849583692849\n",
      "  batch 44 loss: 9.749869059305638e-05\n",
      "  batch 45 loss: 9.618715557735413e-05\n",
      "  batch 46 loss: 9.814030636334792e-05\n",
      "  batch 47 loss: 0.00013620202662423253\n",
      "  batch 48 loss: 0.00014574028318747878\n",
      "  batch 49 loss: 0.0001666937314439565\n",
      "  batch 50 loss: 0.0001932717568706721\n",
      "  batch 51 loss: 0.0002071931812679395\n",
      "  batch 52 loss: 0.00016777869313955307\n",
      "  batch 53 loss: 0.00017812271835282445\n",
      "  batch 54 loss: 0.00020321215561125427\n",
      "  batch 55 loss: 0.0002321586653124541\n",
      "  batch 56 loss: 0.0001544990809634328\n",
      "  batch 57 loss: 0.00012096257705707103\n",
      "  batch 58 loss: 0.00014794511662330478\n",
      "  batch 59 loss: 0.00017695272981654853\n",
      "  batch 60 loss: 0.0002310093550477177\n",
      "  batch 61 loss: 9.347355808131397e-05\n",
      "  batch 62 loss: 0.00019775950931943953\n",
      "  batch 63 loss: 0.00016337147098965943\n",
      "  batch 64 loss: 0.000119474992970936\n",
      "  batch 65 loss: 0.0001270883803954348\n",
      "  batch 66 loss: 0.00011620121949817985\n",
      "  batch 67 loss: 0.00010208922321908176\n",
      "  batch 68 loss: 0.00014153189840726554\n",
      "  batch 69 loss: 0.00015119383169803768\n",
      "  batch 70 loss: 0.00021991957328282297\n",
      "  batch 71 loss: 0.00014582890435121953\n",
      "  batch 72 loss: 0.00013214863429311663\n",
      "  batch 73 loss: 0.00012943486217409372\n",
      "  batch 74 loss: 0.0001523916143923998\n",
      "  batch 75 loss: 0.00022179762891028076\n",
      "  batch 76 loss: 0.0001438602339476347\n",
      "  batch 77 loss: 0.00015528249787166715\n",
      "  batch 78 loss: 0.0001439887419110164\n",
      "  batch 79 loss: 0.00013932405272498727\n",
      "  batch 80 loss: 0.00017113970534410328\n",
      "  batch 81 loss: 0.0001640352129470557\n",
      "  batch 82 loss: 0.00016835983842611313\n",
      "  batch 83 loss: 0.00016297516413033009\n",
      "  batch 84 loss: 0.00012340303510427475\n",
      "  batch 85 loss: 0.00014554450171999633\n",
      "  batch 86 loss: 0.00016292942746076733\n",
      "  batch 87 loss: 0.00015796108345966786\n",
      "  batch 88 loss: 0.0002237150038126856\n",
      "  batch 89 loss: 0.0001742360764183104\n",
      "  batch 90 loss: 0.00022857784642837942\n",
      "  batch 91 loss: 0.00016206187137868255\n",
      "  batch 92 loss: 0.00016903084178920835\n",
      "  batch 93 loss: 0.00017310844850726426\n",
      "  batch 94 loss: 0.00017738535825628787\n",
      "  batch 95 loss: 0.000244916882365942\n",
      "LOSS train 0.000244916882365942 valid 0.0010033920407295227\n",
      "LOSS train 0.000244916882365942 valid 0.0011635359842330217\n",
      "LOSS train 0.000244916882365942 valid 0.0010842798510566354\n",
      "LOSS train 0.000244916882365942 valid 0.0010417720768600702\n",
      "LOSS train 0.000244916882365942 valid 0.0010394299170002341\n",
      "LOSS train 0.000244916882365942 valid 0.0009701952221803367\n",
      "LOSS train 0.000244916882365942 valid 0.001013146829791367\n",
      "LOSS train 0.000244916882365942 valid 0.001057684887200594\n",
      "LOSS train 0.000244916882365942 valid 0.00101803510915488\n",
      "LOSS train 0.000244916882365942 valid 0.0010329940123483539\n",
      "LOSS train 0.000244916882365942 valid 0.001078604138456285\n",
      "LOSS train 0.000244916882365942 valid 0.0010793282417580485\n",
      "LOSS train 0.000244916882365942 valid 0.0010796901769936085\n",
      "LOSS train 0.000244916882365942 valid 0.0011005124542862177\n",
      "LOSS train 0.000244916882365942 valid 0.001180598745122552\n",
      "LOSS train 0.000244916882365942 valid 0.0012200861237943172\n",
      "LOSS train 0.000244916882365942 valid 0.0012254550820216537\n",
      "LOSS train 0.000244916882365942 valid 0.0012404314475134015\n",
      "LOSS train 0.000244916882365942 valid 0.0012469165958464146\n",
      "LOSS train 0.000244916882365942 valid 0.0012446329928934574\n",
      "LOSS train 0.000244916882365942 valid 0.0012304364936426282\n",
      "LOSS train 0.000244916882365942 valid 0.0012485627084970474\n",
      "LOSS train 0.000244916882365942 valid 0.0012444518506526947\n",
      "LOSS train 0.000244916882365942 valid 0.0012447808403521776\n",
      "EPOCH 66:\n",
      "  batch 1 loss: 0.00020969963225070387\n",
      "  batch 2 loss: 0.00013259833212941885\n",
      "  batch 3 loss: 0.00020706279610749334\n",
      "  batch 4 loss: 0.0001842484052758664\n",
      "  batch 5 loss: 0.00015367833839263767\n",
      "  batch 6 loss: 0.00015926877676974982\n",
      "  batch 7 loss: 0.00011867219291161746\n",
      "  batch 8 loss: 0.00012654450256377459\n",
      "  batch 9 loss: 0.00013841940381098539\n",
      "  batch 10 loss: 0.00010459161421749741\n",
      "  batch 11 loss: 0.00016328325727954507\n",
      "  batch 12 loss: 0.00019234008505009115\n",
      "  batch 13 loss: 0.00021935676340945065\n",
      "  batch 14 loss: 0.0003395558742340654\n",
      "  batch 15 loss: 0.00022377376444637775\n",
      "  batch 16 loss: 0.00014422716049011797\n",
      "  batch 17 loss: 0.00015704050019849092\n",
      "  batch 18 loss: 0.0001469242270104587\n",
      "  batch 19 loss: 0.0002690746623557061\n",
      "  batch 20 loss: 0.00022117771732155234\n",
      "  batch 21 loss: 0.00027882441645488143\n",
      "  batch 22 loss: 0.00017782553914003074\n",
      "  batch 23 loss: 0.000255081889918074\n",
      "  batch 24 loss: 0.00024680158821865916\n",
      "  batch 25 loss: 0.0003053518012166023\n",
      "  batch 26 loss: 0.000337096193106845\n",
      "  batch 27 loss: 0.00013700660201720893\n",
      "  batch 28 loss: 0.0002135642134817317\n",
      "  batch 29 loss: 0.0001379010791424662\n",
      "  batch 30 loss: 0.00012547842925414443\n",
      "  batch 31 loss: 0.00010399847815278918\n",
      "  batch 32 loss: 0.00017606568871997297\n",
      "  batch 33 loss: 0.00016384373884648085\n",
      "  batch 34 loss: 0.0002035356592386961\n",
      "  batch 35 loss: 0.00010567503341007978\n",
      "  batch 36 loss: 0.00011913714115507901\n",
      "  batch 37 loss: 0.00012777461961377412\n",
      "  batch 38 loss: 0.00012251001317054033\n",
      "  batch 39 loss: 0.0001704561000224203\n",
      "  batch 40 loss: 0.00018900512077379972\n",
      "  batch 41 loss: 0.00011913971684407443\n",
      "  batch 42 loss: 0.00013570829469244927\n",
      "  batch 43 loss: 0.0001146230279118754\n",
      "  batch 44 loss: 8.438740769634023e-05\n",
      "  batch 45 loss: 0.00010307504271622747\n",
      "  batch 46 loss: 0.00010342196037527174\n",
      "  batch 47 loss: 0.00013629402383230627\n",
      "  batch 48 loss: 0.00013689123443327844\n",
      "  batch 49 loss: 0.00015125065692700446\n",
      "  batch 50 loss: 0.00017746308003552258\n",
      "  batch 51 loss: 0.00016938465705607086\n",
      "  batch 52 loss: 0.00015081322635523975\n",
      "  batch 53 loss: 0.00016037814202718437\n",
      "  batch 54 loss: 0.00018824194557964802\n",
      "  batch 55 loss: 0.0002042019332293421\n",
      "  batch 56 loss: 0.00015143831842578948\n",
      "  batch 57 loss: 0.0001307079364778474\n",
      "  batch 58 loss: 0.0001787920482456684\n",
      "  batch 59 loss: 0.00018058277782984078\n",
      "  batch 60 loss: 0.00022577581694349647\n",
      "  batch 61 loss: 0.00010591682803351432\n",
      "  batch 62 loss: 0.00020604333258233964\n",
      "  batch 63 loss: 0.00012750166933983564\n",
      "  batch 64 loss: 0.00011242220352869481\n",
      "  batch 65 loss: 0.00012797373346984386\n",
      "  batch 66 loss: 0.00012383260764181614\n",
      "  batch 67 loss: 0.00010833724809344858\n",
      "  batch 68 loss: 0.00014980632113292813\n",
      "  batch 69 loss: 0.0001599830575287342\n",
      "  batch 70 loss: 0.0002359387290198356\n",
      "  batch 71 loss: 0.0001466190442442894\n",
      "  batch 72 loss: 0.00012819880794268101\n",
      "  batch 73 loss: 0.00012541195610538125\n",
      "  batch 74 loss: 0.00014903511328157037\n",
      "  batch 75 loss: 0.00021094131807330996\n",
      "  batch 76 loss: 0.0001441584317944944\n",
      "  batch 77 loss: 0.00014675231068395078\n",
      "  batch 78 loss: 0.00014480102981906384\n",
      "  batch 79 loss: 0.00013375855633057654\n",
      "  batch 80 loss: 0.00016466525266878307\n",
      "  batch 81 loss: 0.00015416955284308642\n",
      "  batch 82 loss: 0.0001704496971797198\n",
      "  batch 83 loss: 0.00016644071729388088\n",
      "  batch 84 loss: 0.00012438971316441894\n",
      "  batch 85 loss: 0.00014637145795859396\n",
      "  batch 86 loss: 0.00016349850920960307\n",
      "  batch 87 loss: 0.00015081287710927427\n",
      "  batch 88 loss: 0.00022923652431927621\n",
      "  batch 89 loss: 0.00016782197053544223\n",
      "  batch 90 loss: 0.00022121897200122476\n",
      "  batch 91 loss: 0.0001524791878182441\n",
      "  batch 92 loss: 0.0001557103096274659\n",
      "  batch 93 loss: 0.00016366290219593793\n",
      "  batch 94 loss: 0.0001673412334639579\n",
      "  batch 95 loss: 0.00024296517949551344\n",
      "LOSS train 0.00024296517949551344 valid 0.0009575890144333243\n",
      "LOSS train 0.00024296517949551344 valid 0.0011046475265175104\n",
      "LOSS train 0.00024296517949551344 valid 0.0010367069626227021\n",
      "LOSS train 0.00024296517949551344 valid 0.0009969689417630434\n",
      "LOSS train 0.00024296517949551344 valid 0.000998380477540195\n",
      "LOSS train 0.00024296517949551344 valid 0.0009295279160141945\n",
      "LOSS train 0.00024296517949551344 valid 0.0009682452073320746\n",
      "LOSS train 0.00024296517949551344 valid 0.001017996110022068\n",
      "LOSS train 0.00024296517949551344 valid 0.000981163582764566\n",
      "LOSS train 0.00024296517949551344 valid 0.0010013317223638296\n",
      "LOSS train 0.00024296517949551344 valid 0.001039245747961104\n",
      "LOSS train 0.00024296517949551344 valid 0.0010418023448437452\n",
      "LOSS train 0.00024296517949551344 valid 0.0010448511457070708\n",
      "LOSS train 0.00024296517949551344 valid 0.0010632169432938099\n",
      "LOSS train 0.00024296517949551344 valid 0.001128751551732421\n",
      "LOSS train 0.00024296517949551344 valid 0.0011700419709086418\n",
      "LOSS train 0.00024296517949551344 valid 0.0011777746258303523\n",
      "LOSS train 0.00024296517949551344 valid 0.0011919672833755612\n",
      "LOSS train 0.00024296517949551344 valid 0.0011960917618125677\n",
      "LOSS train 0.00024296517949551344 valid 0.0011952387867495418\n",
      "LOSS train 0.00024296517949551344 valid 0.0011816973565146327\n",
      "LOSS train 0.00024296517949551344 valid 0.0011986438184976578\n",
      "LOSS train 0.00024296517949551344 valid 0.0011919270036742091\n",
      "LOSS train 0.00024296517949551344 valid 0.0012037557316944003\n",
      "EPOCH 67:\n",
      "  batch 1 loss: 0.00021152236149646342\n",
      "  batch 2 loss: 0.00013339187717065215\n",
      "  batch 3 loss: 0.00019510161655489355\n",
      "  batch 4 loss: 0.00017906699213199317\n",
      "  batch 5 loss: 0.0001393763959640637\n",
      "  batch 6 loss: 0.00013921847858000547\n",
      "  batch 7 loss: 0.00010683477739803493\n",
      "  batch 8 loss: 0.00012000856077065691\n",
      "  batch 9 loss: 0.0001369039819110185\n",
      "  batch 10 loss: 9.808743925532326e-05\n",
      "  batch 11 loss: 0.00016037125897128135\n",
      "  batch 12 loss: 0.00018064190226141363\n",
      "  batch 13 loss: 0.00022039597388356924\n",
      "  batch 14 loss: 0.0003360100672580302\n",
      "  batch 15 loss: 0.00021750816085841507\n",
      "  batch 16 loss: 0.00014777181786485016\n",
      "  batch 17 loss: 0.00014711960102431476\n",
      "  batch 18 loss: 0.00014836143236607313\n",
      "  batch 19 loss: 0.00025693883071653545\n",
      "  batch 20 loss: 0.00021406875748652965\n",
      "  batch 21 loss: 0.0002527821925468743\n",
      "  batch 22 loss: 0.0001730881631374359\n",
      "  batch 23 loss: 0.0002446402213536203\n",
      "  batch 24 loss: 0.00023109227186068892\n",
      "  batch 25 loss: 0.00027860255795530975\n",
      "  batch 26 loss: 0.0002825356787070632\n",
      "  batch 27 loss: 0.0001320662267971784\n",
      "  batch 28 loss: 0.00020594018860720098\n",
      "  batch 29 loss: 0.00015095697017386556\n",
      "  batch 30 loss: 0.00014617005945183337\n",
      "  batch 31 loss: 0.00011135209206258878\n",
      "  batch 32 loss: 0.00018645686213858426\n",
      "  batch 33 loss: 0.00016180990496650338\n",
      "  batch 34 loss: 0.000204302225029096\n",
      "  batch 35 loss: 0.00010380470484960824\n",
      "  batch 36 loss: 0.00010701222345232964\n",
      "  batch 37 loss: 0.0001361252216156572\n",
      "  batch 38 loss: 0.00015136295405682176\n",
      "  batch 39 loss: 0.0001730963704176247\n",
      "  batch 40 loss: 0.00018367676238995045\n",
      "  batch 41 loss: 0.0001264209859073162\n",
      "  batch 42 loss: 0.00013878093159291893\n",
      "  batch 43 loss: 0.00011234944395255297\n",
      "  batch 44 loss: 8.118850382743403e-05\n",
      "  batch 45 loss: 0.00010113374446518719\n",
      "  batch 46 loss: 9.336602670373395e-05\n",
      "  batch 47 loss: 0.00013192411279305816\n",
      "  batch 48 loss: 0.00013164861593395472\n",
      "  batch 49 loss: 0.0001611926272744313\n",
      "  batch 50 loss: 0.00017980561824515462\n",
      "  batch 51 loss: 0.00016996223712339997\n",
      "  batch 52 loss: 0.00014403836394194514\n",
      "  batch 53 loss: 0.0001741764135658741\n",
      "  batch 54 loss: 0.00015936465933918953\n",
      "  batch 55 loss: 0.00018562612240202725\n",
      "  batch 56 loss: 0.00012750158202834427\n",
      "  batch 57 loss: 0.00010951846343232319\n",
      "  batch 58 loss: 0.00018194213043898344\n",
      "  batch 59 loss: 0.00015793867351021618\n",
      "  batch 60 loss: 0.0002241063048131764\n",
      "  batch 61 loss: 0.000109040571260266\n",
      "  batch 62 loss: 0.0002075362717732787\n",
      "  batch 63 loss: 0.00013076129835098982\n",
      "  batch 64 loss: 0.00011016677308361977\n",
      "  batch 65 loss: 0.0001229046902153641\n",
      "  batch 66 loss: 0.00011273024574620649\n",
      "  batch 67 loss: 0.00010688550537452102\n",
      "  batch 68 loss: 0.00015431921929121017\n",
      "  batch 69 loss: 0.00014874336193315685\n",
      "  batch 70 loss: 0.00023573596263304353\n",
      "  batch 71 loss: 0.00013251500786282122\n",
      "  batch 72 loss: 0.00013622789992950857\n",
      "  batch 73 loss: 0.00012677680933848023\n",
      "  batch 74 loss: 0.00017465368728153408\n",
      "  batch 75 loss: 0.00022130252909846604\n",
      "  batch 76 loss: 0.0001449598785256967\n",
      "  batch 77 loss: 0.000149103143485263\n",
      "  batch 78 loss: 0.00014523352729156613\n",
      "  batch 79 loss: 0.0001422805362381041\n",
      "  batch 80 loss: 0.00017502342234365642\n",
      "  batch 81 loss: 0.00017531003686599433\n",
      "  batch 82 loss: 0.0001775252603692934\n",
      "  batch 83 loss: 0.00017444217519368976\n",
      "  batch 84 loss: 0.00012569635873660445\n",
      "  batch 85 loss: 0.00013866304652765393\n",
      "  batch 86 loss: 0.0001617758971406147\n",
      "  batch 87 loss: 0.00015410053310915828\n",
      "  batch 88 loss: 0.00023235725529957563\n",
      "  batch 89 loss: 0.00017851950542535633\n",
      "  batch 90 loss: 0.00022967346012592316\n",
      "  batch 91 loss: 0.00016001703625079244\n",
      "  batch 92 loss: 0.00015720288502052426\n",
      "  batch 93 loss: 0.00016190895985346287\n",
      "  batch 94 loss: 0.00016252222121693194\n",
      "  batch 95 loss: 0.0002376240590820089\n",
      "LOSS train 0.0002376240590820089 valid 0.0009590040426701307\n",
      "LOSS train 0.0002376240590820089 valid 0.001137428218498826\n",
      "LOSS train 0.0002376240590820089 valid 0.0010363644687458873\n",
      "LOSS train 0.0002376240590820089 valid 0.0009962446056306362\n",
      "LOSS train 0.0002376240590820089 valid 0.0009852953953668475\n",
      "LOSS train 0.0002376240590820089 valid 0.0009078587172552943\n",
      "LOSS train 0.0002376240590820089 valid 0.0009533798438496888\n",
      "LOSS train 0.0002376240590820089 valid 0.001000908319838345\n",
      "LOSS train 0.0002376240590820089 valid 0.0009618996409699321\n",
      "LOSS train 0.0002376240590820089 valid 0.0009781720582395792\n",
      "LOSS train 0.0002376240590820089 valid 0.0010304024908691645\n",
      "LOSS train 0.0002376240590820089 valid 0.0010296367108821869\n",
      "LOSS train 0.0002376240590820089 valid 0.001024954835884273\n",
      "LOSS train 0.0002376240590820089 valid 0.0010468174004927278\n",
      "LOSS train 0.0002376240590820089 valid 0.001130068558268249\n",
      "LOSS train 0.0002376240590820089 valid 0.0011748616816475987\n",
      "LOSS train 0.0002376240590820089 valid 0.0011754193110391498\n",
      "LOSS train 0.0002376240590820089 valid 0.0011902217520400882\n",
      "LOSS train 0.0002376240590820089 valid 0.0011956357629969716\n",
      "LOSS train 0.0002376240590820089 valid 0.001199167687445879\n",
      "LOSS train 0.0002376240590820089 valid 0.0011865730630233884\n",
      "LOSS train 0.0002376240590820089 valid 0.001206881832331419\n",
      "LOSS train 0.0002376240590820089 valid 0.0012081736931577325\n",
      "LOSS train 0.0002376240590820089 valid 0.0012331863399595022\n",
      "EPOCH 68:\n",
      "  batch 1 loss: 0.00019621002138592303\n",
      "  batch 2 loss: 0.0001265660102944821\n",
      "  batch 3 loss: 0.00019569549476727843\n",
      "  batch 4 loss: 0.00017815502360463142\n",
      "  batch 5 loss: 0.0001452885044272989\n",
      "  batch 6 loss: 0.0001391195401083678\n",
      "  batch 7 loss: 0.00010417209705337882\n",
      "  batch 8 loss: 0.0001213086216012016\n",
      "  batch 9 loss: 0.00012662677909247577\n",
      "  batch 10 loss: 9.150248661171645e-05\n",
      "  batch 11 loss: 0.00015045572945382446\n",
      "  batch 12 loss: 0.00018399153486825526\n",
      "  batch 13 loss: 0.0002182775060646236\n",
      "  batch 14 loss: 0.0003582163481041789\n",
      "  batch 15 loss: 0.0002452124608680606\n",
      "  batch 16 loss: 0.00014624540926888585\n",
      "  batch 17 loss: 0.0001462491345591843\n",
      "  batch 18 loss: 0.00015571352560073137\n",
      "  batch 19 loss: 0.00028898732853122056\n",
      "  batch 20 loss: 0.00021678709890693426\n",
      "  batch 21 loss: 0.000253534410148859\n",
      "  batch 22 loss: 0.00019182224059477448\n",
      "  batch 23 loss: 0.0002533294609747827\n",
      "  batch 24 loss: 0.0002404229890089482\n",
      "  batch 25 loss: 0.0002936053788289428\n",
      "  batch 26 loss: 0.00028977170586586\n",
      "  batch 27 loss: 0.00013722595758736134\n",
      "  batch 28 loss: 0.00019908742979168892\n",
      "  batch 29 loss: 0.00012559369497466832\n",
      "  batch 30 loss: 0.00011863616964546964\n",
      "  batch 31 loss: 8.996162068797275e-05\n",
      "  batch 32 loss: 0.00016031390987336636\n",
      "  batch 33 loss: 0.00014973974612075835\n",
      "  batch 34 loss: 0.00020771811250597239\n",
      "  batch 35 loss: 0.00010754728282336146\n",
      "  batch 36 loss: 0.00010901963833021\n",
      "  batch 37 loss: 0.0001300145813729614\n",
      "  batch 38 loss: 0.00012332979531493038\n",
      "  batch 39 loss: 0.00015410994819831103\n",
      "  batch 40 loss: 0.0001807605440262705\n",
      "  batch 41 loss: 0.00011351239663781598\n",
      "  batch 42 loss: 0.00013787002535536885\n",
      "  batch 43 loss: 0.00011389747669454664\n",
      "  batch 44 loss: 8.32969817565754e-05\n",
      "  batch 45 loss: 8.471867477055639e-05\n",
      "  batch 46 loss: 8.52209486765787e-05\n",
      "  batch 47 loss: 0.00012362923007458448\n",
      "  batch 48 loss: 0.00013753872190136462\n",
      "  batch 49 loss: 0.00016598563524894416\n",
      "  batch 50 loss: 0.00018285034457221627\n",
      "  batch 51 loss: 0.00015663706290069968\n",
      "  batch 52 loss: 0.00013687681348528713\n",
      "  batch 53 loss: 0.00015281004016287625\n",
      "  batch 54 loss: 0.00014767955872230232\n",
      "  batch 55 loss: 0.00019338603306096047\n",
      "  batch 56 loss: 0.00012394628720358014\n",
      "  batch 57 loss: 0.0001061264774762094\n",
      "  batch 58 loss: 0.00015357829397544265\n",
      "  batch 59 loss: 0.00015998091839719564\n",
      "  batch 60 loss: 0.0001952454331330955\n",
      "  batch 61 loss: 9.863857121672481e-05\n",
      "  batch 62 loss: 0.00017175922403112054\n",
      "  batch 63 loss: 0.0001240566634805873\n",
      "  batch 64 loss: 0.00011162563168909401\n",
      "  batch 65 loss: 0.00011596053082030267\n",
      "  batch 66 loss: 0.00010350821685278788\n",
      "  batch 67 loss: 0.0001170211035059765\n",
      "  batch 68 loss: 0.0001544397382531315\n",
      "  batch 69 loss: 0.00015636291936971247\n",
      "  batch 70 loss: 0.00023122252605389804\n",
      "  batch 71 loss: 0.00012728771253023297\n",
      "  batch 72 loss: 0.00013821297034155577\n",
      "  batch 73 loss: 0.00014026278222445399\n",
      "  batch 74 loss: 0.00020678834698628634\n",
      "  batch 75 loss: 0.0002442069526296109\n",
      "  batch 76 loss: 0.00016896688612177968\n",
      "  batch 77 loss: 0.00014502732665278018\n",
      "  batch 78 loss: 0.00014198769349604845\n",
      "  batch 79 loss: 0.00014268691302277148\n",
      "  batch 80 loss: 0.00016585324192419648\n",
      "  batch 81 loss: 0.00017813401063904166\n",
      "  batch 82 loss: 0.0001864679652499035\n",
      "  batch 83 loss: 0.0001885899982880801\n",
      "  batch 84 loss: 0.00013870306429453194\n",
      "  batch 85 loss: 0.00014636715059168637\n",
      "  batch 86 loss: 0.0001630627957638353\n",
      "  batch 87 loss: 0.00015398827963508666\n",
      "  batch 88 loss: 0.0002306783280801028\n",
      "  batch 89 loss: 0.00017290192772634327\n",
      "  batch 90 loss: 0.000227268275921233\n",
      "  batch 91 loss: 0.00015222292859107256\n",
      "  batch 92 loss: 0.00016257631068583578\n",
      "  batch 93 loss: 0.00016957696061581373\n",
      "  batch 94 loss: 0.0001655244268476963\n",
      "  batch 95 loss: 0.0002636222343426198\n",
      "LOSS train 0.0002636222343426198 valid 0.0010525633115321398\n",
      "LOSS train 0.0002636222343426198 valid 0.0011695807334035635\n",
      "LOSS train 0.0002636222343426198 valid 0.0010632761986926198\n",
      "LOSS train 0.0002636222343426198 valid 0.0010283286683261395\n",
      "LOSS train 0.0002636222343426198 valid 0.0010326446499675512\n",
      "LOSS train 0.0002636222343426198 valid 0.0009610614506527781\n",
      "LOSS train 0.0002636222343426198 valid 0.001009817118756473\n",
      "LOSS train 0.0002636222343426198 valid 0.0010591375175863504\n",
      "LOSS train 0.0002636222343426198 valid 0.0010169128654524684\n",
      "LOSS train 0.0002636222343426198 valid 0.0010385990608483553\n",
      "LOSS train 0.0002636222343426198 valid 0.0010906594106927514\n",
      "LOSS train 0.0002636222343426198 valid 0.0010933984303846955\n",
      "LOSS train 0.0002636222343426198 valid 0.0010875845327973366\n",
      "LOSS train 0.0002636222343426198 valid 0.00110927305649966\n",
      "LOSS train 0.0002636222343426198 valid 0.0012010331265628338\n",
      "LOSS train 0.0002636222343426198 valid 0.0012511506211012602\n",
      "LOSS train 0.0002636222343426198 valid 0.0012542600743472576\n",
      "LOSS train 0.0002636222343426198 valid 0.00126763922162354\n",
      "LOSS train 0.0002636222343426198 valid 0.0012718795333057642\n",
      "LOSS train 0.0002636222343426198 valid 0.0012706693960353732\n",
      "LOSS train 0.0002636222343426198 valid 0.001254118513315916\n",
      "LOSS train 0.0002636222343426198 valid 0.0012689776485785842\n",
      "LOSS train 0.0002636222343426198 valid 0.0012661366490647197\n",
      "LOSS train 0.0002636222343426198 valid 0.00127794302534312\n",
      "EPOCH 69:\n",
      "  batch 1 loss: 0.0002035639772657305\n",
      "  batch 2 loss: 0.0001269506465177983\n",
      "  batch 3 loss: 0.00019534285820554942\n",
      "  batch 4 loss: 0.00017623511666897684\n",
      "  batch 5 loss: 0.0001430108823115006\n",
      "  batch 6 loss: 0.00014544607256539166\n",
      "  batch 7 loss: 0.00010362121247453615\n",
      "  batch 8 loss: 0.00011983318108832464\n",
      "  batch 9 loss: 0.00012630352284759283\n",
      "  batch 10 loss: 9.135082655120641e-05\n",
      "  batch 11 loss: 0.00015271994925569743\n",
      "  batch 12 loss: 0.00019018055172637105\n",
      "  batch 13 loss: 0.00021442424622364342\n",
      "  batch 14 loss: 0.0003977525921072811\n",
      "  batch 15 loss: 0.00024014046357478946\n",
      "  batch 16 loss: 0.0001395672734361142\n",
      "  batch 17 loss: 0.00014339503832161427\n",
      "  batch 18 loss: 0.00014321811613626778\n",
      "  batch 19 loss: 0.00025949731934815645\n",
      "  batch 20 loss: 0.00021689190180040896\n",
      "  batch 21 loss: 0.00025139370700344443\n",
      "  batch 22 loss: 0.00018252688460052013\n",
      "  batch 23 loss: 0.00024067258345894516\n",
      "  batch 24 loss: 0.00022133710444904864\n",
      "  batch 25 loss: 0.00027202244382351637\n",
      "  batch 26 loss: 0.0002768023405224085\n",
      "  batch 27 loss: 0.00012825164594687521\n",
      "  batch 28 loss: 0.00019448045350145549\n",
      "  batch 29 loss: 0.00011572966468520463\n",
      "  batch 30 loss: 0.00011521168926265091\n",
      "  batch 31 loss: 8.52040684549138e-05\n",
      "  batch 32 loss: 0.0001544601982459426\n",
      "  batch 33 loss: 0.00014507431478705257\n",
      "  batch 34 loss: 0.0001783923216862604\n",
      "  batch 35 loss: 9.12283721845597e-05\n",
      "  batch 36 loss: 9.645857790019363e-05\n",
      "  batch 37 loss: 0.00011301535414531827\n",
      "  batch 38 loss: 0.00011062393605243415\n",
      "  batch 39 loss: 0.00014832819579169154\n",
      "  batch 40 loss: 0.00018317258218303323\n",
      "  batch 41 loss: 9.588967805029824e-05\n",
      "  batch 42 loss: 0.00012046273332089186\n",
      "  batch 43 loss: 0.00010478538024472073\n",
      "  batch 44 loss: 0.00010453493450768292\n",
      "  batch 45 loss: 9.354506619274616e-05\n",
      "  batch 46 loss: 8.521213021595031e-05\n",
      "  batch 47 loss: 0.00013655189832206815\n",
      "  batch 48 loss: 0.00013127208512742072\n",
      "  batch 49 loss: 0.00015725652338005602\n",
      "  batch 50 loss: 0.00016262620920315385\n",
      "  batch 51 loss: 0.00015874873497523367\n",
      "  batch 52 loss: 0.00012714859622064978\n",
      "  batch 53 loss: 0.00013928583939559758\n",
      "  batch 54 loss: 0.00013236675295047462\n",
      "  batch 55 loss: 0.00017456503701396286\n",
      "  batch 56 loss: 0.000118119984108489\n",
      "  batch 57 loss: 0.00010250371997244656\n",
      "  batch 58 loss: 0.00014918911620043218\n",
      "  batch 59 loss: 0.00022819146397523582\n",
      "  batch 60 loss: 0.000191850820556283\n",
      "  batch 61 loss: 9.48291999520734e-05\n",
      "  batch 62 loss: 0.00016357674030587077\n",
      "  batch 63 loss: 0.0001096385094569996\n",
      "  batch 64 loss: 9.58693417487666e-05\n",
      "  batch 65 loss: 0.00010747789201559499\n",
      "  batch 66 loss: 0.00010412150004412979\n",
      "  batch 67 loss: 0.00010811792162712663\n",
      "  batch 68 loss: 0.00014767896209377795\n",
      "  batch 69 loss: 0.00015197084576357156\n",
      "  batch 70 loss: 0.00021650691633112729\n",
      "  batch 71 loss: 0.00012642070942092687\n",
      "  batch 72 loss: 0.0001288191124331206\n",
      "  batch 73 loss: 0.00012760440586134791\n",
      "  batch 74 loss: 0.0001785962376743555\n",
      "  batch 75 loss: 0.00023555289953947067\n",
      "  batch 76 loss: 0.00019900228653568774\n",
      "  batch 77 loss: 0.0001604692661203444\n",
      "  batch 78 loss: 0.00014731803094036877\n",
      "  batch 79 loss: 0.00013596720236819237\n",
      "  batch 80 loss: 0.0001527031563455239\n",
      "  batch 81 loss: 0.00016154063632711768\n",
      "  batch 82 loss: 0.00017492688493803144\n",
      "  batch 83 loss: 0.0001843725040089339\n",
      "  batch 84 loss: 0.00014026967983227223\n",
      "  batch 85 loss: 0.00016162911197170615\n",
      "  batch 86 loss: 0.0001741468149702996\n",
      "  batch 87 loss: 0.00015894166426733136\n",
      "  batch 88 loss: 0.00024110883532557636\n",
      "  batch 89 loss: 0.0001683505834080279\n",
      "  batch 90 loss: 0.00023829919518902898\n",
      "  batch 91 loss: 0.0001581916876602918\n",
      "  batch 92 loss: 0.00016754353418946266\n",
      "  batch 93 loss: 0.0001687327167019248\n",
      "  batch 94 loss: 0.00016595397028140724\n",
      "  batch 95 loss: 0.0002672886475920677\n",
      "LOSS train 0.0002672886475920677 valid 0.0010884396033361554\n",
      "LOSS train 0.0002672886475920677 valid 0.001220849109813571\n",
      "LOSS train 0.0002672886475920677 valid 0.0010898623149842024\n",
      "LOSS train 0.0002672886475920677 valid 0.0010461693163961172\n",
      "LOSS train 0.0002672886475920677 valid 0.0010452427668496966\n",
      "LOSS train 0.0002672886475920677 valid 0.0009890859946608543\n",
      "LOSS train 0.0002672886475920677 valid 0.0010388711234554648\n",
      "LOSS train 0.0002672886475920677 valid 0.001090510981157422\n",
      "LOSS train 0.0002672886475920677 valid 0.0010506724938750267\n",
      "LOSS train 0.0002672886475920677 valid 0.0010671097552403808\n",
      "LOSS train 0.0002672886475920677 valid 0.001133428537286818\n",
      "LOSS train 0.0002672886475920677 valid 0.001146915601566434\n",
      "LOSS train 0.0002672886475920677 valid 0.0011353178415447474\n",
      "LOSS train 0.0002672886475920677 valid 0.0011532328790053725\n",
      "LOSS train 0.0002672886475920677 valid 0.0012286871206015348\n",
      "LOSS train 0.0002672886475920677 valid 0.0012837290996685624\n",
      "LOSS train 0.0002672886475920677 valid 0.001285612233914435\n",
      "LOSS train 0.0002672886475920677 valid 0.0012952517718076706\n",
      "LOSS train 0.0002672886475920677 valid 0.001294356188736856\n",
      "LOSS train 0.0002672886475920677 valid 0.001289837178774178\n",
      "LOSS train 0.0002672886475920677 valid 0.0012723661493510008\n",
      "LOSS train 0.0002672886475920677 valid 0.0012822947464883327\n",
      "LOSS train 0.0002672886475920677 valid 0.0012762497644871473\n",
      "LOSS train 0.0002672886475920677 valid 0.0012835738016292453\n",
      "EPOCH 70:\n",
      "  batch 1 loss: 0.00022333544620778412\n",
      "  batch 2 loss: 0.0001293271780014038\n",
      "  batch 3 loss: 0.00022519880440086126\n",
      "  batch 4 loss: 0.00018454792734701186\n",
      "  batch 5 loss: 0.00014827007544226944\n",
      "  batch 6 loss: 0.00015423714648932219\n",
      "  batch 7 loss: 0.00010724570165621117\n",
      "  batch 8 loss: 0.00011934981739614159\n",
      "  batch 9 loss: 0.0001324740587733686\n",
      "  batch 10 loss: 9.974339627660811e-05\n",
      "  batch 11 loss: 0.00016851283726282418\n",
      "  batch 12 loss: 0.0001865527592599392\n",
      "  batch 13 loss: 0.00021704693790525198\n",
      "  batch 14 loss: 0.00033366651041433215\n",
      "  batch 15 loss: 0.0002456715446896851\n",
      "  batch 16 loss: 0.00015205964155029505\n",
      "  batch 17 loss: 0.00015905532927718014\n",
      "  batch 18 loss: 0.00014689630188513547\n",
      "  batch 19 loss: 0.00025283542345277965\n",
      "  batch 20 loss: 0.00021175455185584724\n",
      "  batch 21 loss: 0.00024324352853000164\n",
      "  batch 22 loss: 0.00016865544603206217\n",
      "  batch 23 loss: 0.00023911576136015356\n",
      "  batch 24 loss: 0.0002213938278146088\n",
      "  batch 25 loss: 0.0002769501297734678\n",
      "  batch 26 loss: 0.00026907739811576903\n",
      "  batch 27 loss: 0.00012407026952132583\n",
      "  batch 28 loss: 0.00019458404858596623\n",
      "  batch 29 loss: 0.00011584832827793434\n",
      "  batch 30 loss: 0.00010833601845661178\n",
      "  batch 31 loss: 8.589426579419523e-05\n",
      "  batch 32 loss: 0.0001508680870756507\n",
      "  batch 33 loss: 0.0001373637351207435\n",
      "  batch 34 loss: 0.0001646958407945931\n",
      "  batch 35 loss: 8.5169704107102e-05\n",
      "  batch 36 loss: 8.869585872162133e-05\n",
      "  batch 37 loss: 0.00010809021478053182\n",
      "  batch 38 loss: 0.00011654008267214522\n",
      "  batch 39 loss: 0.00013664827565662563\n",
      "  batch 40 loss: 0.00017233572725672275\n",
      "  batch 41 loss: 9.006151231005788e-05\n",
      "  batch 42 loss: 0.00011737234308384359\n",
      "  batch 43 loss: 9.907765343086794e-05\n",
      "  batch 44 loss: 7.152018952183425e-05\n",
      "  batch 45 loss: 8.172679372364655e-05\n",
      "  batch 46 loss: 7.481696957256645e-05\n",
      "  batch 47 loss: 0.00011561150313355029\n",
      "  batch 48 loss: 0.0001181339830509387\n",
      "  batch 49 loss: 0.0001461943902540952\n",
      "  batch 50 loss: 0.00016772439994383603\n",
      "  batch 51 loss: 0.0001619444665266201\n",
      "  batch 52 loss: 0.00013355760893318802\n",
      "  batch 53 loss: 0.00014155826647765934\n",
      "  batch 54 loss: 0.0001275543327210471\n",
      "  batch 55 loss: 0.00016473152209073305\n",
      "  batch 56 loss: 0.0001172451302409172\n",
      "  batch 57 loss: 9.7819218353834e-05\n",
      "  batch 58 loss: 0.0001434379373677075\n",
      "  batch 59 loss: 0.00016364999464713037\n",
      "  batch 60 loss: 0.000180169619852677\n",
      "  batch 61 loss: 8.827574492897838e-05\n",
      "  batch 62 loss: 0.000149629166116938\n",
      "  batch 63 loss: 0.00010577167267911136\n",
      "  batch 64 loss: 0.0001026488171191886\n",
      "  batch 65 loss: 0.00010589677549432963\n",
      "  batch 66 loss: 0.00010512764856684953\n",
      "  batch 67 loss: 8.996652468340471e-05\n",
      "  batch 68 loss: 0.00011860566155519336\n",
      "  batch 69 loss: 0.00012156297452747822\n",
      "  batch 70 loss: 0.00019320345018059015\n",
      "  batch 71 loss: 0.00010543288954067975\n",
      "  batch 72 loss: 0.00011983174772467464\n",
      "  batch 73 loss: 0.00011481134424684569\n",
      "  batch 74 loss: 0.0001539076038170606\n",
      "  batch 75 loss: 0.00022558063210453838\n",
      "  batch 76 loss: 0.00014951931370887905\n",
      "  batch 77 loss: 0.0001387538795825094\n",
      "  batch 78 loss: 0.00013243014109320939\n",
      "  batch 79 loss: 0.00011658309085760266\n",
      "  batch 80 loss: 0.0001368787488900125\n",
      "  batch 81 loss: 0.0001520405785413459\n",
      "  batch 82 loss: 0.00016954443708527833\n",
      "  batch 83 loss: 0.00017052926705218852\n",
      "  batch 84 loss: 0.00012684740067925304\n",
      "  batch 85 loss: 0.00014342815848067403\n",
      "  batch 86 loss: 0.00016107983537949622\n",
      "  batch 87 loss: 0.000155269488459453\n",
      "  batch 88 loss: 0.00022927598911337554\n",
      "  batch 89 loss: 0.00015911224181763828\n",
      "  batch 90 loss: 0.0002171564265154302\n",
      "  batch 91 loss: 0.00013466877862811089\n",
      "  batch 92 loss: 0.0001491391158197075\n",
      "  batch 93 loss: 0.00015497265849262476\n",
      "  batch 94 loss: 0.00015810156764928252\n",
      "  batch 95 loss: 0.000257700914517045\n",
      "LOSS train 0.000257700914517045 valid 0.0012515885755419731\n",
      "LOSS train 0.000257700914517045 valid 0.0013682894641533494\n",
      "LOSS train 0.000257700914517045 valid 0.0012323991395533085\n",
      "LOSS train 0.000257700914517045 valid 0.0011803200468420982\n",
      "LOSS train 0.000257700914517045 valid 0.001178583363071084\n",
      "LOSS train 0.000257700914517045 valid 0.0011187049094587564\n",
      "LOSS train 0.000257700914517045 valid 0.0011774305021390319\n",
      "LOSS train 0.000257700914517045 valid 0.0012288910802453756\n",
      "LOSS train 0.000257700914517045 valid 0.0011900574900209904\n",
      "LOSS train 0.000257700914517045 valid 0.0012116124853491783\n",
      "LOSS train 0.000257700914517045 valid 0.001278194016776979\n",
      "LOSS train 0.000257700914517045 valid 0.0012877851258963346\n",
      "LOSS train 0.000257700914517045 valid 0.0012780617689713836\n",
      "LOSS train 0.000257700914517045 valid 0.0013069043634459376\n",
      "LOSS train 0.000257700914517045 valid 0.0014262853655964136\n",
      "LOSS train 0.000257700914517045 valid 0.0014902795664966106\n",
      "LOSS train 0.000257700914517045 valid 0.001490508671849966\n",
      "LOSS train 0.000257700914517045 valid 0.0015020123682916164\n",
      "LOSS train 0.000257700914517045 valid 0.0015016840770840645\n",
      "LOSS train 0.000257700914517045 valid 0.0014949928736314178\n",
      "LOSS train 0.000257700914517045 valid 0.0014714826829731464\n",
      "LOSS train 0.000257700914517045 valid 0.001478285645134747\n",
      "LOSS train 0.000257700914517045 valid 0.0014697933802381158\n",
      "LOSS train 0.000257700914517045 valid 0.0014702233020216227\n",
      "EPOCH 71:\n",
      "  batch 1 loss: 0.0002075312368106097\n",
      "  batch 2 loss: 0.00012169172987341881\n",
      "  batch 3 loss: 0.00020290142856538296\n",
      "  batch 4 loss: 0.0001874219160526991\n",
      "  batch 5 loss: 0.00014677892613690346\n",
      "  batch 6 loss: 0.0001432153512723744\n",
      "  batch 7 loss: 0.00010335270781069994\n",
      "  batch 8 loss: 0.00012055855768267065\n",
      "  batch 9 loss: 0.00012710806913673878\n",
      "  batch 10 loss: 9.186947863781825e-05\n",
      "  batch 11 loss: 0.00015679880743846297\n",
      "  batch 12 loss: 0.00018090536468662322\n",
      "  batch 13 loss: 0.00021382301929406822\n",
      "  batch 14 loss: 0.0003416174731682986\n",
      "  batch 15 loss: 0.00021557320724241436\n",
      "  batch 16 loss: 0.00014259264571592212\n",
      "  batch 17 loss: 0.0001528123248135671\n",
      "  batch 18 loss: 0.0001555556955281645\n",
      "  batch 19 loss: 0.00029260560404509306\n",
      "  batch 20 loss: 0.0002224381605628878\n",
      "  batch 21 loss: 0.00024067923368420452\n",
      "  batch 22 loss: 0.00018809005268849432\n",
      "  batch 23 loss: 0.00024282801314257085\n",
      "  batch 24 loss: 0.0002375411568209529\n",
      "  batch 25 loss: 0.000284117937553674\n",
      "  batch 26 loss: 0.0002810083096846938\n",
      "  batch 27 loss: 0.00014092432684265077\n",
      "  batch 28 loss: 0.00020239935838617384\n",
      "  batch 29 loss: 0.00012710836017504334\n",
      "  batch 30 loss: 0.00011892734619323164\n",
      "  batch 31 loss: 9.381712879985571e-05\n",
      "  batch 32 loss: 0.00016438587044831365\n",
      "  batch 33 loss: 0.00014460507372859865\n",
      "  batch 34 loss: 0.00017446427955292165\n",
      "  batch 35 loss: 9.309295273851603e-05\n",
      "  batch 36 loss: 9.817304089665413e-05\n",
      "  batch 37 loss: 0.0001130943710450083\n",
      "  batch 38 loss: 0.00013163744006305933\n",
      "  batch 39 loss: 0.0001407496747560799\n",
      "  batch 40 loss: 0.00016865471843630075\n",
      "  batch 41 loss: 8.655145211378112e-05\n",
      "  batch 42 loss: 0.00011932143388548866\n",
      "  batch 43 loss: 0.00010087405098602176\n",
      "  batch 44 loss: 6.647205009358004e-05\n",
      "  batch 45 loss: 8.106587483780459e-05\n",
      "  batch 46 loss: 7.284565072041005e-05\n",
      "  batch 47 loss: 0.0001169875540654175\n",
      "  batch 48 loss: 0.00011080481635872275\n",
      "  batch 49 loss: 0.00013387798389885575\n",
      "  batch 50 loss: 0.00015381377306766808\n",
      "  batch 51 loss: 0.00017774684238247573\n",
      "  batch 52 loss: 0.0001468471164116636\n",
      "  batch 53 loss: 0.00015722011448815465\n",
      "  batch 54 loss: 0.0001289288920816034\n",
      "  batch 55 loss: 0.00015877795522101223\n",
      "  batch 56 loss: 0.00011180230649188161\n",
      "  batch 57 loss: 9.444243914913386e-05\n",
      "  batch 58 loss: 0.00011793064913945273\n",
      "  batch 59 loss: 0.000140958814881742\n",
      "  batch 60 loss: 0.0001990698219742626\n",
      "  batch 61 loss: 9.364484139950946e-05\n",
      "  batch 62 loss: 0.000155910529429093\n",
      "  batch 63 loss: 0.00010304577153874561\n",
      "  batch 64 loss: 9.176707681035623e-05\n",
      "  batch 65 loss: 9.441435395274311e-05\n",
      "  batch 66 loss: 9.16466087801382e-05\n",
      "  batch 67 loss: 8.648935909150168e-05\n",
      "  batch 68 loss: 0.00011163243470946327\n",
      "  batch 69 loss: 0.00011559544509509578\n",
      "  batch 70 loss: 0.00018359058594796807\n",
      "  batch 71 loss: 0.00010901892528636381\n",
      "  batch 72 loss: 0.00011785247625084594\n",
      "  batch 73 loss: 0.00011199996515642852\n",
      "  batch 74 loss: 0.00016131892334669828\n",
      "  batch 75 loss: 0.0002108734770445153\n",
      "  batch 76 loss: 0.00013972217857372016\n",
      "  batch 77 loss: 0.0001294389076065272\n",
      "  batch 78 loss: 0.00012261963274795562\n",
      "  batch 79 loss: 0.00010837262379936874\n",
      "  batch 80 loss: 0.00012412635260261595\n",
      "  batch 81 loss: 0.00014141089923214167\n",
      "  batch 82 loss: 0.00016238834359683096\n",
      "  batch 83 loss: 0.00015996403817553073\n",
      "  batch 84 loss: 0.00011739494220819324\n",
      "  batch 85 loss: 0.00013164685515221208\n",
      "  batch 86 loss: 0.00015607595560140908\n",
      "  batch 87 loss: 0.00014953126083128154\n",
      "  batch 88 loss: 0.00021701576770283282\n",
      "  batch 89 loss: 0.00015684400568716228\n",
      "  batch 90 loss: 0.00022409838857129216\n",
      "  batch 91 loss: 0.000137212555273436\n",
      "  batch 92 loss: 0.00014571574865840375\n",
      "  batch 93 loss: 0.0001499087957199663\n",
      "  batch 94 loss: 0.0001538897631689906\n",
      "  batch 95 loss: 0.00024320876400452107\n",
      "LOSS train 0.00024320876400452107 valid 0.0013545149704441428\n",
      "LOSS train 0.00024320876400452107 valid 0.0014895789790898561\n",
      "LOSS train 0.00024320876400452107 valid 0.0013207545271143317\n",
      "LOSS train 0.00024320876400452107 valid 0.0012785436119884253\n",
      "LOSS train 0.00024320876400452107 valid 0.0012665946269407868\n",
      "LOSS train 0.00024320876400452107 valid 0.0011956244707107544\n",
      "LOSS train 0.00024320876400452107 valid 0.0012714775511994958\n",
      "LOSS train 0.00024320876400452107 valid 0.0013423435157164931\n",
      "LOSS train 0.00024320876400452107 valid 0.0012958310544490814\n",
      "LOSS train 0.00024320876400452107 valid 0.001331724808551371\n",
      "LOSS train 0.00024320876400452107 valid 0.0014105159789323807\n",
      "LOSS train 0.00024320876400452107 valid 0.0014221258461475372\n",
      "LOSS train 0.00024320876400452107 valid 0.0014060307294130325\n",
      "LOSS train 0.00024320876400452107 valid 0.0014412065502256155\n",
      "LOSS train 0.00024320876400452107 valid 0.001568917534314096\n",
      "LOSS train 0.00024320876400452107 valid 0.0016306490870192647\n",
      "LOSS train 0.00024320876400452107 valid 0.0016279722331091762\n",
      "LOSS train 0.00024320876400452107 valid 0.0016459306934848428\n",
      "LOSS train 0.00024320876400452107 valid 0.0016470368718728423\n",
      "LOSS train 0.00024320876400452107 valid 0.0016378707950934768\n",
      "LOSS train 0.00024320876400452107 valid 0.0016118603525683284\n",
      "LOSS train 0.00024320876400452107 valid 0.001623661257326603\n",
      "LOSS train 0.00024320876400452107 valid 0.0016119556967169046\n",
      "LOSS train 0.00024320876400452107 valid 0.0016181253595277667\n",
      "EPOCH 72:\n",
      "  batch 1 loss: 0.00019982193771284074\n",
      "  batch 2 loss: 0.00011619989527389407\n",
      "  batch 3 loss: 0.00018994809943251312\n",
      "  batch 4 loss: 0.00018892186926677823\n",
      "  batch 5 loss: 0.00014589130296371877\n",
      "  batch 6 loss: 0.00015712443564552814\n",
      "  batch 7 loss: 0.00010355871927458793\n",
      "  batch 8 loss: 0.00011934287613257766\n",
      "  batch 9 loss: 0.0001235526433447376\n",
      "  batch 10 loss: 9.602510544937104e-05\n",
      "  batch 11 loss: 0.00015086046187207103\n",
      "  batch 12 loss: 0.00017277355073019862\n",
      "  batch 13 loss: 0.0002156267291866243\n",
      "  batch 14 loss: 0.00035563844721764326\n",
      "  batch 15 loss: 0.00021417340030893683\n",
      "  batch 16 loss: 0.00014443942927755415\n",
      "  batch 17 loss: 0.00017669213411863893\n",
      "  batch 18 loss: 0.00014917051885277033\n",
      "  batch 19 loss: 0.000320614839438349\n",
      "  batch 20 loss: 0.00021750101586803794\n",
      "  batch 21 loss: 0.00024096854031085968\n",
      "  batch 22 loss: 0.00017805653624236584\n",
      "  batch 23 loss: 0.0002526313182897866\n",
      "  batch 24 loss: 0.0002608817885629833\n",
      "  batch 25 loss: 0.00028060932527296245\n",
      "  batch 26 loss: 0.000335355696734041\n",
      "  batch 27 loss: 0.00012929289368912578\n",
      "  batch 28 loss: 0.0002157058333978057\n",
      "  batch 29 loss: 0.0001209399415529333\n",
      "  batch 30 loss: 0.00011954336514463648\n",
      "  batch 31 loss: 9.24266132642515e-05\n",
      "  batch 32 loss: 0.00017046427819877863\n",
      "  batch 33 loss: 0.0001558602089062333\n",
      "  batch 34 loss: 0.00018738809740170836\n",
      "  batch 35 loss: 9.814103395910934e-05\n",
      "  batch 36 loss: 9.445157775189728e-05\n",
      "  batch 37 loss: 0.00011420568625908345\n",
      "  batch 38 loss: 0.0001262039877474308\n",
      "  batch 39 loss: 0.00015962486213538796\n",
      "  batch 40 loss: 0.0001707088085822761\n",
      "  batch 41 loss: 8.90211813384667e-05\n",
      "  batch 42 loss: 0.00011052114859921858\n",
      "  batch 43 loss: 0.00010477441537659615\n",
      "  batch 44 loss: 7.754350372124463e-05\n",
      "  batch 45 loss: 8.313983562402427e-05\n",
      "  batch 46 loss: 8.424520638072863e-05\n",
      "  batch 47 loss: 0.00012075627455487847\n",
      "  batch 48 loss: 0.00011902557889698073\n",
      "  batch 49 loss: 0.00013808690709993243\n",
      "  batch 50 loss: 0.000159170274855569\n",
      "  batch 51 loss: 0.00016510198474861681\n",
      "  batch 52 loss: 0.00012674032768700272\n",
      "  batch 53 loss: 0.00013836409198120236\n",
      "  batch 54 loss: 0.00013730640057474375\n",
      "  batch 55 loss: 0.00016188621520996094\n",
      "  batch 56 loss: 0.00011819612700492144\n",
      "  batch 57 loss: 8.85097761056386e-05\n",
      "  batch 58 loss: 0.00011041170364478603\n",
      "  batch 59 loss: 0.00011347600957378745\n",
      "  batch 60 loss: 0.0001746314810588956\n",
      "  batch 61 loss: 9.145905642071739e-05\n",
      "  batch 62 loss: 0.00016684872389305383\n",
      "  batch 63 loss: 0.00011092460772488266\n",
      "  batch 64 loss: 0.00010871475387830287\n",
      "  batch 65 loss: 0.00010240356641588733\n",
      "  batch 66 loss: 0.00010143048712052405\n",
      "  batch 67 loss: 0.00010040744382422417\n",
      "  batch 68 loss: 0.0001259122946066782\n",
      "  batch 69 loss: 0.00011850264127133414\n",
      "  batch 70 loss: 0.00018436319078318775\n",
      "  batch 71 loss: 0.00010352442041039467\n",
      "  batch 72 loss: 0.00011493037163745612\n",
      "  batch 73 loss: 0.00012704782420769334\n",
      "  batch 74 loss: 0.00018306414131075144\n",
      "  batch 75 loss: 0.0002306470414623618\n",
      "  batch 76 loss: 0.00014954234939068556\n",
      "  batch 77 loss: 0.00014260328316595405\n",
      "  batch 78 loss: 0.0001307482016272843\n",
      "  batch 79 loss: 0.00010659455438144505\n",
      "  batch 80 loss: 0.00012382835848256946\n",
      "  batch 81 loss: 0.00014058574743103236\n",
      "  batch 82 loss: 0.0001653227227507159\n",
      "  batch 83 loss: 0.00015924140461720526\n",
      "  batch 84 loss: 0.00012168520333943889\n",
      "  batch 85 loss: 0.00013563896936830133\n",
      "  batch 86 loss: 0.00015737276407890022\n",
      "  batch 87 loss: 0.00015283757238648832\n",
      "  batch 88 loss: 0.00023742596386000514\n",
      "  batch 89 loss: 0.0001647355529712513\n",
      "  batch 90 loss: 0.00022268103202804923\n",
      "  batch 91 loss: 0.0001374729472445324\n",
      "  batch 92 loss: 0.00014747687964700162\n",
      "  batch 93 loss: 0.00014691353135276586\n",
      "  batch 94 loss: 0.00015423615695908666\n",
      "  batch 95 loss: 0.0002440677344566211\n",
      "LOSS train 0.0002440677344566211 valid 0.0012194006703794003\n",
      "LOSS train 0.0002440677344566211 valid 0.0013427375815808773\n",
      "LOSS train 0.0002440677344566211 valid 0.0012362772831693292\n",
      "LOSS train 0.0002440677344566211 valid 0.0011971081839874387\n",
      "LOSS train 0.0002440677344566211 valid 0.0011890353634953499\n",
      "LOSS train 0.0002440677344566211 valid 0.0011087682796642184\n",
      "LOSS train 0.0002440677344566211 valid 0.001150322612375021\n",
      "LOSS train 0.0002440677344566211 valid 0.0012056587729603052\n",
      "LOSS train 0.0002440677344566211 valid 0.0011681235628202558\n",
      "LOSS train 0.0002440677344566211 valid 0.001193507923744619\n",
      "LOSS train 0.0002440677344566211 valid 0.0012659068452194333\n",
      "LOSS train 0.0002440677344566211 valid 0.001272780355066061\n",
      "LOSS train 0.0002440677344566211 valid 0.001265756436623633\n",
      "LOSS train 0.0002440677344566211 valid 0.001292853383347392\n",
      "LOSS train 0.0002440677344566211 valid 0.0013883797219023108\n",
      "LOSS train 0.0002440677344566211 valid 0.001445411122404039\n",
      "LOSS train 0.0002440677344566211 valid 0.0014539140975102782\n",
      "LOSS train 0.0002440677344566211 valid 0.00146730977576226\n",
      "LOSS train 0.0002440677344566211 valid 0.001467577531002462\n",
      "LOSS train 0.0002440677344566211 valid 0.0014612035593017936\n",
      "LOSS train 0.0002440677344566211 valid 0.0014455184573307633\n",
      "LOSS train 0.0002440677344566211 valid 0.0014570443890988827\n",
      "LOSS train 0.0002440677344566211 valid 0.0014496479416266084\n",
      "LOSS train 0.0002440677344566211 valid 0.001454353565350175\n",
      "EPOCH 73:\n",
      "  batch 1 loss: 0.00018595936126075685\n",
      "  batch 2 loss: 0.00011199806613149121\n",
      "  batch 3 loss: 0.0001794132695067674\n",
      "  batch 4 loss: 0.00017006753478199244\n",
      "  batch 5 loss: 0.00014132849173620343\n",
      "  batch 6 loss: 0.00015919559518806636\n",
      "  batch 7 loss: 0.0001026079771691002\n",
      "  batch 8 loss: 0.00011410335719119757\n",
      "  batch 9 loss: 0.00012608316319528967\n",
      "  batch 10 loss: 0.00010191208275500685\n",
      "  batch 11 loss: 0.00015641370555385947\n",
      "  batch 12 loss: 0.00018133675621356815\n",
      "  batch 13 loss: 0.00020301344920881093\n",
      "  batch 14 loss: 0.0003230558068025857\n",
      "  batch 15 loss: 0.00020567460160236806\n",
      "  batch 16 loss: 0.00012782808335032314\n",
      "  batch 17 loss: 0.00013223539281170815\n",
      "  batch 18 loss: 0.00013202188711147755\n",
      "  batch 19 loss: 0.00025626629940234125\n",
      "  batch 20 loss: 0.00020689264056272805\n",
      "  batch 21 loss: 0.0002586797927506268\n",
      "  batch 22 loss: 0.0001831001864047721\n",
      "  batch 23 loss: 0.00024277034390252084\n",
      "  batch 24 loss: 0.00021600714535452425\n",
      "  batch 25 loss: 0.00025496320449747145\n",
      "  batch 26 loss: 0.00027467915788292885\n",
      "  batch 27 loss: 0.00012141955812694505\n",
      "  batch 28 loss: 0.0002641319006215781\n",
      "  batch 29 loss: 0.00012780676479451358\n",
      "  batch 30 loss: 0.00011537156387930736\n",
      "  batch 31 loss: 9.269607835449278e-05\n",
      "  batch 32 loss: 0.0001586675934959203\n",
      "  batch 33 loss: 0.0001413612044416368\n",
      "  batch 34 loss: 0.00018204975640401244\n",
      "  batch 35 loss: 0.00010080155334435403\n",
      "  batch 36 loss: 0.00011192543752258644\n",
      "  batch 37 loss: 0.00011116098175989464\n",
      "  batch 38 loss: 0.0001176753721665591\n",
      "  batch 39 loss: 0.00015080557204782963\n",
      "  batch 40 loss: 0.0001706211332930252\n",
      "  batch 41 loss: 0.00010239022958558053\n",
      "  batch 42 loss: 0.0001321082527283579\n",
      "  batch 43 loss: 0.00010000210022553802\n",
      "  batch 44 loss: 7.10014792275615e-05\n",
      "  batch 45 loss: 8.095290104392916e-05\n",
      "  batch 46 loss: 7.679782720515504e-05\n",
      "  batch 47 loss: 0.00011865839769598097\n",
      "  batch 48 loss: 0.0001251672802027315\n",
      "  batch 49 loss: 0.0001319953880738467\n",
      "  batch 50 loss: 0.00015979050658643246\n",
      "  batch 51 loss: 0.000156767011503689\n",
      "  batch 52 loss: 0.00012879828864242882\n",
      "  batch 53 loss: 0.00014521394041366875\n",
      "  batch 54 loss: 0.00012504633923526853\n",
      "  batch 55 loss: 0.00016080305795185268\n",
      "  batch 56 loss: 0.00011691196414176375\n",
      "  batch 57 loss: 9.067180508282036e-05\n",
      "  batch 58 loss: 0.00011431059101596475\n",
      "  batch 59 loss: 0.00011808797717094421\n",
      "  batch 60 loss: 0.000158626091433689\n",
      "  batch 61 loss: 8.110490307444707e-05\n",
      "  batch 62 loss: 0.00015255111793521792\n",
      "  batch 63 loss: 9.937843424268067e-05\n",
      "  batch 64 loss: 9.845529712038115e-05\n",
      "  batch 65 loss: 0.00010012638085754588\n",
      "  batch 66 loss: 9.750724711921066e-05\n",
      "  batch 67 loss: 9.670842700870708e-05\n",
      "  batch 68 loss: 0.0001375278807245195\n",
      "  batch 69 loss: 0.00013222155394032598\n",
      "  batch 70 loss: 0.00020072227925993502\n",
      "  batch 71 loss: 0.00012570247054100037\n",
      "  batch 72 loss: 0.00011768457625294104\n",
      "  batch 73 loss: 0.00011268709204159677\n",
      "  batch 74 loss: 0.00014469293819274753\n",
      "  batch 75 loss: 0.00018969085067510605\n",
      "  batch 76 loss: 0.00013617676449939609\n",
      "  batch 77 loss: 0.00013672240311279893\n",
      "  batch 78 loss: 0.00013314672105479985\n",
      "  batch 79 loss: 0.00010708483750931919\n",
      "  batch 80 loss: 0.00013004806532990187\n",
      "  batch 81 loss: 0.00014986938913352787\n",
      "  batch 82 loss: 0.00017837158520705998\n",
      "  batch 83 loss: 0.00015820161206647754\n",
      "  batch 84 loss: 0.00011548197653610259\n",
      "  batch 85 loss: 0.00013279178529046476\n",
      "  batch 86 loss: 0.0001474709715694189\n",
      "  batch 87 loss: 0.00014412654854822904\n",
      "  batch 88 loss: 0.0002324616361875087\n",
      "  batch 89 loss: 0.00016334035899490118\n",
      "  batch 90 loss: 0.00023788126418367028\n",
      "  batch 91 loss: 0.0001477356709074229\n",
      "  batch 92 loss: 0.00016330397920683026\n",
      "  batch 93 loss: 0.00016191540635190904\n",
      "  batch 94 loss: 0.00015693946625106037\n",
      "  batch 95 loss: 0.00023443199461326003\n",
      "LOSS train 0.00023443199461326003 valid 0.001173621043562889\n",
      "LOSS train 0.00023443199461326003 valid 0.0012337020598351955\n",
      "LOSS train 0.00023443199461326003 valid 0.0012211475986987352\n",
      "LOSS train 0.00023443199461326003 valid 0.0011803603265434504\n",
      "LOSS train 0.00023443199461326003 valid 0.001182542066089809\n",
      "LOSS train 0.00023443199461326003 valid 0.001099712448194623\n",
      "LOSS train 0.00023443199461326003 valid 0.00113169034011662\n",
      "LOSS train 0.00023443199461326003 valid 0.0011757663451135159\n",
      "LOSS train 0.00023443199461326003 valid 0.0011465088464319706\n",
      "LOSS train 0.00023443199461326003 valid 0.0011625341139733791\n",
      "LOSS train 0.00023443199461326003 valid 0.0011942762648686767\n",
      "LOSS train 0.00023443199461326003 valid 0.0011960217962041497\n",
      "LOSS train 0.00023443199461326003 valid 0.0012137391604483128\n",
      "LOSS train 0.00023443199461326003 valid 0.0012320035602897406\n",
      "LOSS train 0.00023443199461326003 valid 0.0012872343650087714\n",
      "LOSS train 0.00023443199461326003 valid 0.0013308919733390212\n",
      "LOSS train 0.00023443199461326003 valid 0.0013393621193245053\n",
      "LOSS train 0.00023443199461326003 valid 0.0013418408343568444\n",
      "LOSS train 0.00023443199461326003 valid 0.001340425107628107\n",
      "LOSS train 0.00023443199461326003 valid 0.0013304249150678515\n",
      "LOSS train 0.00023443199461326003 valid 0.0013193784980103374\n",
      "LOSS train 0.00023443199461326003 valid 0.001331764506176114\n",
      "LOSS train 0.00023443199461326003 valid 0.0013233204372227192\n",
      "LOSS train 0.00023443199461326003 valid 0.0013145155971869826\n",
      "EPOCH 74:\n",
      "  batch 1 loss: 0.0002005978167289868\n",
      "  batch 2 loss: 0.00011509742034832016\n",
      "  batch 3 loss: 0.00018946337513625622\n",
      "  batch 4 loss: 0.0001715304679237306\n",
      "  batch 5 loss: 0.00013245843001641333\n",
      "  batch 6 loss: 0.00014949642354622483\n",
      "  batch 7 loss: 0.00010466198727954179\n",
      "  batch 8 loss: 0.0001145157657447271\n",
      "  batch 9 loss: 0.00013139947259332985\n",
      "  batch 10 loss: 0.00011363477096892893\n",
      "  batch 11 loss: 0.00015052143135108054\n",
      "  batch 12 loss: 0.00017837202176451683\n",
      "  batch 13 loss: 0.00021017045946791768\n",
      "  batch 14 loss: 0.0003388061304576695\n",
      "  batch 15 loss: 0.0002075608936138451\n",
      "  batch 16 loss: 0.0001257650728803128\n",
      "  batch 17 loss: 0.00013244952424429357\n",
      "  batch 18 loss: 0.00012759253149852157\n",
      "  batch 19 loss: 0.00025040138280019164\n",
      "  batch 20 loss: 0.0001936443441081792\n",
      "  batch 21 loss: 0.0002401324309175834\n",
      "  batch 22 loss: 0.00017955704242922366\n",
      "  batch 23 loss: 0.00023929802409838885\n",
      "  batch 24 loss: 0.0002184038603445515\n",
      "  batch 25 loss: 0.00026763032656162977\n",
      "  batch 26 loss: 0.00025906553491950035\n",
      "  batch 27 loss: 0.00013562134699895978\n",
      "  batch 28 loss: 0.00028738105902448297\n",
      "  batch 29 loss: 0.00011960292613366619\n",
      "  batch 30 loss: 0.00010763802129076794\n",
      "  batch 31 loss: 0.00010839120659511536\n",
      "  batch 32 loss: 0.0002012497279793024\n",
      "  batch 33 loss: 0.00015455184620805085\n",
      "  batch 34 loss: 0.0002009169984376058\n",
      "  batch 35 loss: 9.374914225190878e-05\n",
      "  batch 36 loss: 9.443570161238313e-05\n",
      "  batch 37 loss: 0.00012023219460388646\n",
      "  batch 38 loss: 0.00011465746501926333\n",
      "  batch 39 loss: 0.00014398680650629103\n",
      "  batch 40 loss: 0.00017925395513884723\n",
      "  batch 41 loss: 9.213796874973923e-05\n",
      "  batch 42 loss: 0.00011692914995364845\n",
      "  batch 43 loss: 9.71662302617915e-05\n",
      "  batch 44 loss: 6.946001667529345e-05\n",
      "  batch 45 loss: 8.557196997571737e-05\n",
      "  batch 46 loss: 8.47665942274034e-05\n",
      "  batch 47 loss: 0.00011877223005285487\n",
      "  batch 48 loss: 0.00011526439629960805\n",
      "  batch 49 loss: 0.00013923767255619168\n",
      "  batch 50 loss: 0.00015605505905114114\n",
      "  batch 51 loss: 0.00016190437600016594\n",
      "  batch 52 loss: 0.00011820469080703333\n",
      "  batch 53 loss: 0.00013112547458149493\n",
      "  batch 54 loss: 0.00012748304288834333\n",
      "  batch 55 loss: 0.00016884846263565123\n",
      "  batch 56 loss: 0.0001089898869395256\n",
      "  batch 57 loss: 8.689660171512514e-05\n",
      "  batch 58 loss: 0.00011863937834277749\n",
      "  batch 59 loss: 0.00013662848505191505\n",
      "  batch 60 loss: 0.0001779035956133157\n",
      "  batch 61 loss: 8.283833449240774e-05\n",
      "  batch 62 loss: 0.00015252806770149618\n",
      "  batch 63 loss: 0.00010026316158473492\n",
      "  batch 64 loss: 8.409393194597214e-05\n",
      "  batch 65 loss: 8.668524969834834e-05\n",
      "  batch 66 loss: 8.410702866967767e-05\n",
      "  batch 67 loss: 8.414745389018208e-05\n",
      "  batch 68 loss: 0.00011412967432988808\n",
      "  batch 69 loss: 0.00010960185318253934\n",
      "  batch 70 loss: 0.0001889517589006573\n",
      "  batch 71 loss: 0.0001200292754219845\n",
      "  batch 72 loss: 0.00011662980978144333\n",
      "  batch 73 loss: 0.00011393037857487798\n",
      "  batch 74 loss: 0.0001423998619429767\n",
      "  batch 75 loss: 0.00018876635294873267\n",
      "  batch 76 loss: 0.00012680022337008268\n",
      "  batch 77 loss: 0.00013096377369947731\n",
      "  batch 78 loss: 0.00011832207383122295\n",
      "  batch 79 loss: 9.796417725738138e-05\n",
      "  batch 80 loss: 0.00012095218698959798\n",
      "  batch 81 loss: 0.0001390412508044392\n",
      "  batch 82 loss: 0.00015960530436132103\n",
      "  batch 83 loss: 0.00015272360178641975\n",
      "  batch 84 loss: 0.00011079111573053524\n",
      "  batch 85 loss: 0.00013002046034671366\n",
      "  batch 86 loss: 0.00015453831292688847\n",
      "  batch 87 loss: 0.00014077944797463715\n",
      "  batch 88 loss: 0.0002151741791749373\n",
      "  batch 89 loss: 0.00014957302482798696\n",
      "  batch 90 loss: 0.00022014864953234792\n",
      "  batch 91 loss: 0.00013899266195949167\n",
      "  batch 92 loss: 0.00015650618297513574\n",
      "  batch 93 loss: 0.0001599529350642115\n",
      "  batch 94 loss: 0.00016740916180424392\n",
      "  batch 95 loss: 0.00023159640841186047\n",
      "LOSS train 0.00023159640841186047 valid 0.0012972343247383833\n",
      "LOSS train 0.00023159640841186047 valid 0.0013368334621191025\n",
      "LOSS train 0.00023159640841186047 valid 0.0013328593922778964\n",
      "LOSS train 0.00023159640841186047 valid 0.001283249817788601\n",
      "LOSS train 0.00023159640841186047 valid 0.0013016703305765986\n",
      "LOSS train 0.00023159640841186047 valid 0.0012159619946032763\n",
      "LOSS train 0.00023159640841186047 valid 0.00123789778444916\n",
      "LOSS train 0.00023159640841186047 valid 0.0012864450691267848\n",
      "LOSS train 0.00023159640841186047 valid 0.0012630050769075751\n",
      "LOSS train 0.00023159640841186047 valid 0.001283343997783959\n",
      "LOSS train 0.00023159640841186047 valid 0.0013110152212902904\n",
      "LOSS train 0.00023159640841186047 valid 0.0013135214103385806\n",
      "LOSS train 0.00023159640841186047 valid 0.001338050700724125\n",
      "LOSS train 0.00023159640841186047 valid 0.0013523377710953355\n",
      "LOSS train 0.00023159640841186047 valid 0.0014185087056830525\n",
      "LOSS train 0.00023159640841186047 valid 0.0014693583361804485\n",
      "LOSS train 0.00023159640841186047 valid 0.0014796241885051131\n",
      "LOSS train 0.00023159640841186047 valid 0.0014827646082267165\n",
      "LOSS train 0.00023159640841186047 valid 0.0014785302337259054\n",
      "LOSS train 0.00023159640841186047 valid 0.0014644053298979998\n",
      "LOSS train 0.00023159640841186047 valid 0.00145010557025671\n",
      "LOSS train 0.00023159640841186047 valid 0.001461269916035235\n",
      "LOSS train 0.00023159640841186047 valid 0.0014502427075058222\n",
      "LOSS train 0.00023159640841186047 valid 0.0014345163945108652\n",
      "EPOCH 75:\n",
      "  batch 1 loss: 0.0001965711562661454\n",
      "  batch 2 loss: 0.00011535896919667721\n",
      "  batch 3 loss: 0.00017605157336220145\n",
      "  batch 4 loss: 0.000174977962160483\n",
      "  batch 5 loss: 0.00014379005006048828\n",
      "  batch 6 loss: 0.00017147712060250342\n",
      "  batch 7 loss: 0.00012482021702453494\n",
      "  batch 8 loss: 0.00012214586604386568\n",
      "  batch 9 loss: 0.0001276284019695595\n",
      "  batch 10 loss: 0.0001049593702191487\n",
      "  batch 11 loss: 0.00015407739556394517\n",
      "  batch 12 loss: 0.00019642960978671908\n",
      "  batch 13 loss: 0.00021426250168588012\n",
      "  batch 14 loss: 0.0003485520137473941\n",
      "  batch 15 loss: 0.0002130831708200276\n",
      "  batch 16 loss: 0.000153904125909321\n",
      "  batch 17 loss: 0.00013346932246349752\n",
      "  batch 18 loss: 0.00015401170821860433\n",
      "  batch 19 loss: 0.0002632860268931836\n",
      "  batch 20 loss: 0.00022309768246486783\n",
      "  batch 21 loss: 0.00024349711020477116\n",
      "  batch 22 loss: 0.0002119360724464059\n",
      "  batch 23 loss: 0.0002524101873859763\n",
      "  batch 24 loss: 0.00023303268244490027\n",
      "  batch 25 loss: 0.00029695883858948946\n",
      "  batch 26 loss: 0.00028891314286738634\n",
      "  batch 27 loss: 0.00014322767674457282\n",
      "  batch 28 loss: 0.00019166221318300813\n",
      "  batch 29 loss: 0.000116273935418576\n",
      "  batch 30 loss: 0.00011400117364246398\n",
      "  batch 31 loss: 9.579718607710674e-05\n",
      "  batch 32 loss: 0.00016147670976351947\n",
      "  batch 33 loss: 0.00014210492372512817\n",
      "  batch 34 loss: 0.00017746025696396828\n",
      "  batch 35 loss: 0.00011127264588139951\n",
      "  batch 36 loss: 0.00012012943625450134\n",
      "  batch 37 loss: 0.00012850078928750008\n",
      "  batch 38 loss: 0.0001295397087233141\n",
      "  batch 39 loss: 0.0001390957331750542\n",
      "  batch 40 loss: 0.0001697243278613314\n",
      "  batch 41 loss: 8.497934322804213e-05\n",
      "  batch 42 loss: 0.00012387438619043678\n",
      "  batch 43 loss: 9.898849384626374e-05\n",
      "  batch 44 loss: 7.120587542885914e-05\n",
      "  batch 45 loss: 8.222297765314579e-05\n",
      "  batch 46 loss: 7.756796549074352e-05\n",
      "  batch 47 loss: 0.00011461744725238532\n",
      "  batch 48 loss: 0.00011303799692541361\n",
      "  batch 49 loss: 0.0001355987333226949\n",
      "  batch 50 loss: 0.00018898087728302926\n",
      "  batch 51 loss: 0.00015852875367272645\n",
      "  batch 52 loss: 0.00011747218377422541\n",
      "  batch 53 loss: 0.00014026043936610222\n",
      "  batch 54 loss: 0.00013053591828793287\n",
      "  batch 55 loss: 0.00018607851234264672\n",
      "  batch 56 loss: 0.00011565672321012244\n",
      "  batch 57 loss: 8.600730507168919e-05\n",
      "  batch 58 loss: 0.00011214461119379848\n",
      "  batch 59 loss: 0.00012036194675602019\n",
      "  batch 60 loss: 0.00017886536079458892\n",
      "  batch 61 loss: 8.257988520199433e-05\n",
      "  batch 62 loss: 0.0001429857948096469\n",
      "  batch 63 loss: 9.35123534873128e-05\n",
      "  batch 64 loss: 8.689853711985052e-05\n",
      "  batch 65 loss: 9.146523370873183e-05\n",
      "  batch 66 loss: 8.851452730596066e-05\n",
      "  batch 67 loss: 8.680256723891944e-05\n",
      "  batch 68 loss: 0.00010920286877080798\n",
      "  batch 69 loss: 0.00010987443965859711\n",
      "  batch 70 loss: 0.00018234257004223764\n",
      "  batch 71 loss: 0.00010673717042664066\n",
      "  batch 72 loss: 0.00010719520651036873\n",
      "  batch 73 loss: 0.00010287742770742625\n",
      "  batch 74 loss: 0.00014115689555183053\n",
      "  batch 75 loss: 0.00018218698096461594\n",
      "  batch 76 loss: 0.00012089911615476012\n",
      "  batch 77 loss: 0.0001248432818101719\n",
      "  batch 78 loss: 0.00011724791693268344\n",
      "  batch 79 loss: 9.576711454428732e-05\n",
      "  batch 80 loss: 0.0001180505205411464\n",
      "  batch 81 loss: 0.0001320467854384333\n",
      "  batch 82 loss: 0.0001557972573209554\n",
      "  batch 83 loss: 0.00014888870646245778\n",
      "  batch 84 loss: 0.00011085062578786165\n",
      "  batch 85 loss: 0.00012294607586227357\n",
      "  batch 86 loss: 0.00014146947069093585\n",
      "  batch 87 loss: 0.0001316941052209586\n",
      "  batch 88 loss: 0.00020215558470226824\n",
      "  batch 89 loss: 0.00014425290282815695\n",
      "  batch 90 loss: 0.0002094480296364054\n",
      "  batch 91 loss: 0.0001255361712537706\n",
      "  batch 92 loss: 0.0001440101768821478\n",
      "  batch 93 loss: 0.0001472073927288875\n",
      "  batch 94 loss: 0.000152168155182153\n",
      "  batch 95 loss: 0.00021896333782933652\n",
      "LOSS train 0.00021896333782933652 valid 0.001274562207981944\n",
      "LOSS train 0.00021896333782933652 valid 0.0013249723706394434\n",
      "LOSS train 0.00021896333782933652 valid 0.0012977829901501536\n",
      "LOSS train 0.00021896333782933652 valid 0.0012585809454321861\n",
      "LOSS train 0.00021896333782933652 valid 0.0012609092518687248\n",
      "LOSS train 0.00021896333782933652 valid 0.0011703012278303504\n",
      "LOSS train 0.00021896333782933652 valid 0.001193089410662651\n",
      "LOSS train 0.00021896333782933652 valid 0.0012296668719500303\n",
      "LOSS train 0.00021896333782933652 valid 0.001202714629471302\n",
      "LOSS train 0.00021896333782933652 valid 0.0012195379240438342\n",
      "LOSS train 0.00021896333782933652 valid 0.0012435786193236709\n",
      "LOSS train 0.00021896333782933652 valid 0.001243828795850277\n",
      "LOSS train 0.00021896333782933652 valid 0.0012655218597501516\n",
      "LOSS train 0.00021896333782933652 valid 0.0012788864551112056\n",
      "LOSS train 0.00021896333782933652 valid 0.0013362412573769689\n",
      "LOSS train 0.00021896333782933652 valid 0.0013848341768607497\n",
      "LOSS train 0.00021896333782933652 valid 0.0013927791733294725\n",
      "LOSS train 0.00021896333782933652 valid 0.0014001564122736454\n",
      "LOSS train 0.00021896333782933652 valid 0.0014016669010743499\n",
      "LOSS train 0.00021896333782933652 valid 0.0013911909190937877\n",
      "LOSS train 0.00021896333782933652 valid 0.0013806241331622005\n",
      "LOSS train 0.00021896333782933652 valid 0.0013943850062787533\n",
      "LOSS train 0.00021896333782933652 valid 0.0013844204368069768\n",
      "LOSS train 0.00021896333782933652 valid 0.0013747047632932663\n",
      "EPOCH 76:\n",
      "  batch 1 loss: 0.00018236692994832993\n",
      "  batch 2 loss: 0.00011236721911700442\n",
      "  batch 3 loss: 0.00017702208424452692\n",
      "  batch 4 loss: 0.000159031871589832\n",
      "  batch 5 loss: 0.00012228517152834684\n",
      "  batch 6 loss: 0.00014885232667438686\n",
      "  batch 7 loss: 0.0001065075266524218\n",
      "  batch 8 loss: 0.00012539431918412447\n",
      "  batch 9 loss: 0.00011534751683939248\n",
      "  batch 10 loss: 8.880554378265515e-05\n",
      "  batch 11 loss: 0.00014659871521871537\n",
      "  batch 12 loss: 0.00019042627536691725\n",
      "  batch 13 loss: 0.00020658958237618208\n",
      "  batch 14 loss: 0.00034001009771600366\n",
      "  batch 15 loss: 0.00023771934502292424\n",
      "  batch 16 loss: 0.0001529585279058665\n",
      "  batch 17 loss: 0.000122882891446352\n",
      "  batch 18 loss: 0.0001281644363189116\n",
      "  batch 19 loss: 0.0002386281848885119\n",
      "  batch 20 loss: 0.00019380691810511053\n",
      "  batch 21 loss: 0.00023316906299442053\n",
      "  batch 22 loss: 0.00018329166050534695\n",
      "  batch 23 loss: 0.00023126321320887655\n",
      "  batch 24 loss: 0.00021256374020595104\n",
      "  batch 25 loss: 0.0002643119660206139\n",
      "  batch 26 loss: 0.00028910412220284343\n",
      "  batch 27 loss: 0.00013205870345700532\n",
      "  batch 28 loss: 0.00016974814934656024\n",
      "  batch 29 loss: 0.00011232167889829725\n",
      "  batch 30 loss: 0.00010794054833240807\n",
      "  batch 31 loss: 8.659646846354008e-05\n",
      "  batch 32 loss: 0.00014143236330710351\n",
      "  batch 33 loss: 0.00013755186228081584\n",
      "  batch 34 loss: 0.00017090421169996262\n",
      "  batch 35 loss: 8.613166573923081e-05\n",
      "  batch 36 loss: 7.960174843901768e-05\n",
      "  batch 37 loss: 0.00010448483226355165\n",
      "  batch 38 loss: 0.00010422685591038316\n",
      "  batch 39 loss: 0.00014031610044185072\n",
      "  batch 40 loss: 0.0001576347858645022\n",
      "  batch 41 loss: 9.585948282619938e-05\n",
      "  batch 42 loss: 0.00012335491192061454\n",
      "  batch 43 loss: 9.25631684367545e-05\n",
      "  batch 44 loss: 7.037953764665872e-05\n",
      "  batch 45 loss: 7.470150740118697e-05\n",
      "  batch 46 loss: 7.149425073293969e-05\n",
      "  batch 47 loss: 0.00010012162965722382\n",
      "  batch 48 loss: 0.00010699972335714847\n",
      "  batch 49 loss: 0.00011916756920982152\n",
      "  batch 50 loss: 0.00015592054114677012\n",
      "  batch 51 loss: 0.00013426084478851408\n",
      "  batch 52 loss: 0.00012569929822348058\n",
      "  batch 53 loss: 0.00012342365516815335\n",
      "  batch 54 loss: 0.00012074992264388129\n",
      "  batch 55 loss: 0.00014724116772413254\n",
      "  batch 56 loss: 9.919310105033219e-05\n",
      "  batch 57 loss: 8.208462531911209e-05\n",
      "  batch 58 loss: 0.00011810955766122788\n",
      "  batch 59 loss: 0.0001136474747909233\n",
      "  batch 60 loss: 0.00015884212916716933\n",
      "  batch 61 loss: 6.916542770341039e-05\n",
      "  batch 62 loss: 0.0001461473002564162\n",
      "  batch 63 loss: 9.72487687249668e-05\n",
      "  batch 64 loss: 8.242428884841502e-05\n",
      "  batch 65 loss: 8.348346455022693e-05\n",
      "  batch 66 loss: 7.95240921434015e-05\n",
      "  batch 67 loss: 8.10210476629436e-05\n",
      "  batch 68 loss: 0.00010511225264053792\n",
      "  batch 69 loss: 0.00010586588177829981\n",
      "  batch 70 loss: 0.00016870562103576958\n",
      "  batch 71 loss: 0.00011429977894295007\n",
      "  batch 72 loss: 0.00010737012780737132\n",
      "  batch 73 loss: 0.000107663705421146\n",
      "  batch 74 loss: 0.00013647216837853193\n",
      "  batch 75 loss: 0.00017510345787741244\n",
      "  batch 76 loss: 0.0001174310382339172\n",
      "  batch 77 loss: 0.00011906081635970622\n",
      "  batch 78 loss: 0.00011218558211112395\n",
      "  batch 79 loss: 8.919063111534342e-05\n",
      "  batch 80 loss: 0.00010962762462440878\n",
      "  batch 81 loss: 0.0001254149101441726\n",
      "  batch 82 loss: 0.00014283320342656225\n",
      "  batch 83 loss: 0.00013825991482008249\n",
      "  batch 84 loss: 0.00010830839164555073\n",
      "  batch 85 loss: 0.00012390351912472397\n",
      "  batch 86 loss: 0.0001392367121297866\n",
      "  batch 87 loss: 0.00012818483810406178\n",
      "  batch 88 loss: 0.00019636322394944727\n",
      "  batch 89 loss: 0.0001330075174337253\n",
      "  batch 90 loss: 0.00020097174274269491\n",
      "  batch 91 loss: 0.00012232082372065634\n",
      "  batch 92 loss: 0.00014227503561414778\n",
      "  batch 93 loss: 0.0001423656358383596\n",
      "  batch 94 loss: 0.00015308524598367512\n",
      "  batch 95 loss: 0.00021882352302782238\n",
      "LOSS train 0.00021882352302782238 valid 0.0012886773329228163\n",
      "LOSS train 0.00021882352302782238 valid 0.0013257365208119154\n",
      "LOSS train 0.00021882352302782238 valid 0.0013330094516277313\n",
      "LOSS train 0.00021882352302782238 valid 0.0012820977717638016\n",
      "LOSS train 0.00021882352302782238 valid 0.0012871617218479514\n",
      "LOSS train 0.00021882352302782238 valid 0.0011922686826437712\n",
      "LOSS train 0.00021882352302782238 valid 0.0012158183380961418\n",
      "LOSS train 0.00021882352302782238 valid 0.0012607545359060168\n",
      "LOSS train 0.00021882352302782238 valid 0.001226819702424109\n",
      "LOSS train 0.00021882352302782238 valid 0.001249504741281271\n",
      "LOSS train 0.00021882352302782238 valid 0.0012730922317132354\n",
      "LOSS train 0.00021882352302782238 valid 0.0012800039257854223\n",
      "LOSS train 0.00021882352302782238 valid 0.0012929182266816497\n",
      "LOSS train 0.00021882352302782238 valid 0.0013094624737277627\n",
      "LOSS train 0.00021882352302782238 valid 0.0013635661453008652\n",
      "LOSS train 0.00021882352302782238 valid 0.0014134623343124986\n",
      "LOSS train 0.00021882352302782238 valid 0.0014240371529012918\n",
      "LOSS train 0.00021882352302782238 valid 0.0014288404490798712\n",
      "LOSS train 0.00021882352302782238 valid 0.0014306961093097925\n",
      "LOSS train 0.00021882352302782238 valid 0.001422349945642054\n",
      "LOSS train 0.00021882352302782238 valid 0.001414149533957243\n",
      "LOSS train 0.00021882352302782238 valid 0.0014274361310526729\n",
      "LOSS train 0.00021882352302782238 valid 0.0014197287382557988\n",
      "LOSS train 0.00021882352302782238 valid 0.0014044460840523243\n",
      "EPOCH 77:\n",
      "  batch 1 loss: 0.00017110623593907803\n",
      "  batch 2 loss: 0.00010839222522918135\n",
      "  batch 3 loss: 0.00016235631483141333\n",
      "  batch 4 loss: 0.00014810565335210413\n",
      "  batch 5 loss: 0.00011479468230390921\n",
      "  batch 6 loss: 0.00013517632032744586\n",
      "  batch 7 loss: 8.865415293257684e-05\n",
      "  batch 8 loss: 0.00010193741763941944\n",
      "  batch 9 loss: 0.0001155085046775639\n",
      "  batch 10 loss: 8.528222679160535e-05\n",
      "  batch 11 loss: 0.0001295678230235353\n",
      "  batch 12 loss: 0.0001633696083445102\n",
      "  batch 13 loss: 0.0002002508845180273\n",
      "  batch 14 loss: 0.0003124697832390666\n",
      "  batch 15 loss: 0.0002297938335686922\n",
      "  batch 16 loss: 0.00012995682482142001\n",
      "  batch 17 loss: 0.00011163810268044472\n",
      "  batch 18 loss: 0.0001265854953089729\n",
      "  batch 19 loss: 0.0002623474574647844\n",
      "  batch 20 loss: 0.000192858133232221\n",
      "  batch 21 loss: 0.00022370205260813236\n",
      "  batch 22 loss: 0.00016409004456363618\n",
      "  batch 23 loss: 0.0002113828668370843\n",
      "  batch 24 loss: 0.00020469236187636852\n",
      "  batch 25 loss: 0.00025507461396045983\n",
      "  batch 26 loss: 0.00026679286384023726\n",
      "  batch 27 loss: 0.00012640684144571424\n",
      "  batch 28 loss: 0.0001700090360827744\n",
      "  batch 29 loss: 0.00010905512317549437\n",
      "  batch 30 loss: 0.00011512156925164163\n",
      "  batch 31 loss: 8.623771282145754e-05\n",
      "  batch 32 loss: 0.00013886546366848052\n",
      "  batch 33 loss: 0.00012313949991948903\n",
      "  batch 34 loss: 0.0001605791476322338\n",
      "  batch 35 loss: 8.443625119980425e-05\n",
      "  batch 36 loss: 8.796402835287154e-05\n",
      "  batch 37 loss: 9.847812179941684e-05\n",
      "  batch 38 loss: 9.871355723589659e-05\n",
      "  batch 39 loss: 0.00012657223851419985\n",
      "  batch 40 loss: 0.00014421853120438755\n",
      "  batch 41 loss: 8.670479292050004e-05\n",
      "  batch 42 loss: 0.0001176351506728679\n",
      "  batch 43 loss: 9.224344103131443e-05\n",
      "  batch 44 loss: 7.493642624467611e-05\n",
      "  batch 45 loss: 7.368474325630814e-05\n",
      "  batch 46 loss: 7.429750257870182e-05\n",
      "  batch 47 loss: 0.00011607211490627378\n",
      "  batch 48 loss: 0.00011813394667115062\n",
      "  batch 49 loss: 0.00013059914635960013\n",
      "  batch 50 loss: 0.00014915785868652165\n",
      "  batch 51 loss: 0.00014156242832541466\n",
      "  batch 52 loss: 0.00010862437193281949\n",
      "  batch 53 loss: 0.0001211755006806925\n",
      "  batch 54 loss: 0.0001212422430398874\n",
      "  batch 55 loss: 0.00014988133625593036\n",
      "  batch 56 loss: 9.847116598393768e-05\n",
      "  batch 57 loss: 8.468652231385931e-05\n",
      "  batch 58 loss: 0.00011969787010457367\n",
      "  batch 59 loss: 0.00011419866495998576\n",
      "  batch 60 loss: 0.00014750991249457002\n",
      "  batch 61 loss: 6.733615009579808e-05\n",
      "  batch 62 loss: 0.00013731035869568586\n",
      "  batch 63 loss: 8.741060446482152e-05\n",
      "  batch 64 loss: 8.016799984034151e-05\n",
      "  batch 65 loss: 8.682587213115767e-05\n",
      "  batch 66 loss: 7.661313429707661e-05\n",
      "  batch 67 loss: 7.284727325895801e-05\n",
      "  batch 68 loss: 9.414131636731327e-05\n",
      "  batch 69 loss: 9.83537538559176e-05\n",
      "  batch 70 loss: 0.00015571207040920854\n",
      "  batch 71 loss: 9.640811185818166e-05\n",
      "  batch 72 loss: 9.598469478078187e-05\n",
      "  batch 73 loss: 0.00010015859152190387\n",
      "  batch 74 loss: 0.00012124880595365539\n",
      "  batch 75 loss: 0.00016915718151722103\n",
      "  batch 76 loss: 0.00011108424223493785\n",
      "  batch 77 loss: 0.00011598438140936196\n",
      "  batch 78 loss: 0.00011169772915309295\n",
      "  batch 79 loss: 8.496487862430513e-05\n",
      "  batch 80 loss: 0.00010654886864358559\n",
      "  batch 81 loss: 0.00012215014430694282\n",
      "  batch 82 loss: 0.00013348140055313706\n",
      "  batch 83 loss: 0.00012882761075161397\n",
      "  batch 84 loss: 9.994460560847074e-05\n",
      "  batch 85 loss: 0.00010842714982572943\n",
      "  batch 86 loss: 0.00012302224058657885\n",
      "  batch 87 loss: 0.00012225558748468757\n",
      "  batch 88 loss: 0.0001924459356814623\n",
      "  batch 89 loss: 0.00013139995280653238\n",
      "  batch 90 loss: 0.00019317216356284916\n",
      "  batch 91 loss: 0.00011560618440853432\n",
      "  batch 92 loss: 0.00013001218030694872\n",
      "  batch 93 loss: 0.00013259977276902646\n",
      "  batch 94 loss: 0.00013991707237437367\n",
      "  batch 95 loss: 0.00020389775454532355\n",
      "LOSS train 0.00020389775454532355 valid 0.0011585036991164088\n",
      "LOSS train 0.00020389775454532355 valid 0.0012416901299729943\n",
      "LOSS train 0.00020389775454532355 valid 0.0011902637779712677\n",
      "LOSS train 0.00020389775454532355 valid 0.0011451125610619783\n",
      "LOSS train 0.00020389775454532355 valid 0.0011405596742406487\n",
      "LOSS train 0.00020389775454532355 valid 0.001048258156515658\n",
      "LOSS train 0.00020389775454532355 valid 0.001096353749744594\n",
      "LOSS train 0.00020389775454532355 valid 0.0011469174642115831\n",
      "LOSS train 0.00020389775454532355 valid 0.0011132813524454832\n",
      "LOSS train 0.00020389775454532355 valid 0.0011413105530664325\n",
      "LOSS train 0.00020389775454532355 valid 0.0011792491422966123\n",
      "LOSS train 0.00020389775454532355 valid 0.0011911853216588497\n",
      "LOSS train 0.00020389775454532355 valid 0.0011963049182668328\n",
      "LOSS train 0.00020389775454532355 valid 0.001219108933582902\n",
      "LOSS train 0.00020389775454532355 valid 0.0012862650910392404\n",
      "LOSS train 0.00020389775454532355 valid 0.0013407031074166298\n",
      "LOSS train 0.00020389775454532355 valid 0.0013516191393136978\n",
      "LOSS train 0.00020389775454532355 valid 0.0013617919757962227\n",
      "LOSS train 0.00020389775454532355 valid 0.0013638093369081616\n",
      "LOSS train 0.00020389775454532355 valid 0.001356866559945047\n",
      "LOSS train 0.00020389775454532355 valid 0.0013441061601042747\n",
      "LOSS train 0.00020389775454532355 valid 0.0013571224408224225\n",
      "LOSS train 0.00020389775454532355 valid 0.0013485990930348635\n",
      "LOSS train 0.00020389775454532355 valid 0.00133817782625556\n",
      "EPOCH 78:\n",
      "  batch 1 loss: 0.000163186268764548\n",
      "  batch 2 loss: 0.00010829101665876806\n",
      "  batch 3 loss: 0.00016085089009720832\n",
      "  batch 4 loss: 0.000140639633173123\n",
      "  batch 5 loss: 0.00010994855256285518\n",
      "  batch 6 loss: 0.00011800102220149711\n",
      "  batch 7 loss: 8.244434866355732e-05\n",
      "  batch 8 loss: 9.685306577011943e-05\n",
      "  batch 9 loss: 0.00010628153540892527\n",
      "  batch 10 loss: 7.442195783369243e-05\n",
      "  batch 11 loss: 0.0001309252402279526\n",
      "  batch 12 loss: 0.00015049621288198978\n",
      "  batch 13 loss: 0.00018836100935004652\n",
      "  batch 14 loss: 0.0003148569958284497\n",
      "  batch 15 loss: 0.00019407461513765156\n",
      "  batch 16 loss: 0.00011619163706200197\n",
      "  batch 17 loss: 0.00010523395758355036\n",
      "  batch 18 loss: 0.00011755699233617634\n",
      "  batch 19 loss: 0.00024496210971847177\n",
      "  batch 20 loss: 0.00018124375492334366\n",
      "  batch 21 loss: 0.0002233404666185379\n",
      "  batch 22 loss: 0.0001546325220260769\n",
      "  batch 23 loss: 0.0002126585750374943\n",
      "  batch 24 loss: 0.00019286283350083977\n",
      "  batch 25 loss: 0.0002354415482841432\n",
      "  batch 26 loss: 0.0002458557137288153\n",
      "  batch 27 loss: 0.00010434900468681008\n",
      "  batch 28 loss: 0.00015634499141015112\n",
      "  batch 29 loss: 9.73037094809115e-05\n",
      "  batch 30 loss: 9.800380212254822e-05\n",
      "  batch 31 loss: 7.698172703385353e-05\n",
      "  batch 32 loss: 0.00013126368867233396\n",
      "  batch 33 loss: 0.0001217867829836905\n",
      "  batch 34 loss: 0.00014986831229180098\n",
      "  batch 35 loss: 7.988568540895358e-05\n",
      "  batch 36 loss: 8.52308003231883e-05\n",
      "  batch 37 loss: 9.44055209401995e-05\n",
      "  batch 38 loss: 9.921026503434405e-05\n",
      "  batch 39 loss: 0.00012430509377736598\n",
      "  batch 40 loss: 0.00014534384536091238\n",
      "  batch 41 loss: 7.73708161432296e-05\n",
      "  batch 42 loss: 0.00010673056385712698\n",
      "  batch 43 loss: 8.113232615869492e-05\n",
      "  batch 44 loss: 5.8557641750667244e-05\n",
      "  batch 45 loss: 6.736302748322487e-05\n",
      "  batch 46 loss: 6.552890408784151e-05\n",
      "  batch 47 loss: 0.00010537406342336908\n",
      "  batch 48 loss: 0.00010507280239835382\n",
      "  batch 49 loss: 0.00013029685942456126\n",
      "  batch 50 loss: 0.00013916264288127422\n",
      "  batch 51 loss: 0.0001535876072011888\n",
      "  batch 52 loss: 0.00011635187547653913\n",
      "  batch 53 loss: 0.00012008746125502512\n",
      "  batch 54 loss: 0.000105661092675291\n",
      "  batch 55 loss: 0.00014235544949769974\n",
      "  batch 56 loss: 9.217115439241752e-05\n",
      "  batch 57 loss: 7.45638826629147e-05\n",
      "  batch 58 loss: 0.00010166523134103045\n",
      "  batch 59 loss: 0.00010889062832575291\n",
      "  batch 60 loss: 0.00015672980225645006\n",
      "  batch 61 loss: 6.893559475429356e-05\n",
      "  batch 62 loss: 0.00014085679140407592\n",
      "  batch 63 loss: 8.150842040777206e-05\n",
      "  batch 64 loss: 7.630756590515375e-05\n",
      "  batch 65 loss: 7.842492050258443e-05\n",
      "  batch 66 loss: 7.842959894333035e-05\n",
      "  batch 67 loss: 7.472828292520717e-05\n",
      "  batch 68 loss: 9.142252383753657e-05\n",
      "  batch 69 loss: 9.43011837080121e-05\n",
      "  batch 70 loss: 0.00015487836208194494\n",
      "  batch 71 loss: 9.376058005727828e-05\n",
      "  batch 72 loss: 9.577318269293755e-05\n",
      "  batch 73 loss: 9.691651212051511e-05\n",
      "  batch 74 loss: 0.00011971838102908805\n",
      "  batch 75 loss: 0.00016371112724300474\n",
      "  batch 76 loss: 0.00011160473513882607\n",
      "  batch 77 loss: 0.00011163116141688079\n",
      "  batch 78 loss: 0.00010345842747483402\n",
      "  batch 79 loss: 8.228796650655568e-05\n",
      "  batch 80 loss: 0.00010080889478558674\n",
      "  batch 81 loss: 0.00011719041503965855\n",
      "  batch 82 loss: 0.0001324666664004326\n",
      "  batch 83 loss: 0.0001294340763706714\n",
      "  batch 84 loss: 9.561312617734075e-05\n",
      "  batch 85 loss: 0.0001034959132084623\n",
      "  batch 86 loss: 0.00012157823221059516\n",
      "  batch 87 loss: 0.00011572888615773991\n",
      "  batch 88 loss: 0.00018199096666648984\n",
      "  batch 89 loss: 0.00012602968490682542\n",
      "  batch 90 loss: 0.00018007541075348854\n",
      "  batch 91 loss: 0.00011239598097745329\n",
      "  batch 92 loss: 0.00012179181794635952\n",
      "  batch 93 loss: 0.00012909015640616417\n",
      "  batch 94 loss: 0.0001324749318882823\n",
      "  batch 95 loss: 0.00019911995332222432\n",
      "LOSS train 0.00019911995332222432 valid 0.0010675301309674978\n",
      "LOSS train 0.00019911995332222432 valid 0.0011725734220817685\n",
      "LOSS train 0.00019911995332222432 valid 0.001109110889956355\n",
      "LOSS train 0.00019911995332222432 valid 0.001085433061234653\n",
      "LOSS train 0.00019911995332222432 valid 0.0010781771270558238\n",
      "LOSS train 0.00019911995332222432 valid 0.0009986928198486567\n",
      "LOSS train 0.00019911995332222432 valid 0.0010519103379920125\n",
      "LOSS train 0.00019911995332222432 valid 0.0010947773698717356\n",
      "LOSS train 0.00019911995332222432 valid 0.0010627196170389652\n",
      "LOSS train 0.00019911995332222432 valid 0.0010950283613055944\n",
      "LOSS train 0.00019911995332222432 valid 0.0011402617674320936\n",
      "LOSS train 0.00019911995332222432 valid 0.00115603965241462\n",
      "LOSS train 0.00019911995332222432 valid 0.0011556678218767047\n",
      "LOSS train 0.00019911995332222432 valid 0.0011792232980951667\n",
      "LOSS train 0.00019911995332222432 valid 0.001252022571861744\n",
      "LOSS train 0.00019911995332222432 valid 0.0013064859667792916\n",
      "LOSS train 0.00019911995332222432 valid 0.0013158629881218076\n",
      "LOSS train 0.00019911995332222432 valid 0.0013291587820276618\n",
      "LOSS train 0.00019911995332222432 valid 0.0013322208542376757\n",
      "LOSS train 0.00019911995332222432 valid 0.0013261300045996904\n",
      "LOSS train 0.00019911995332222432 valid 0.0013120529474690557\n",
      "LOSS train 0.00019911995332222432 valid 0.0013255927478894591\n",
      "LOSS train 0.00019911995332222432 valid 0.0013178304070606828\n",
      "LOSS train 0.00019911995332222432 valid 0.001318654976785183\n",
      "EPOCH 79:\n",
      "  batch 1 loss: 0.00017166488396469504\n",
      "  batch 2 loss: 0.00010073172597913072\n",
      "  batch 3 loss: 0.0001614286156836897\n",
      "  batch 4 loss: 0.00013696706446353346\n",
      "  batch 5 loss: 0.00010898872278630733\n",
      "  batch 6 loss: 0.00011307936802040786\n",
      "  batch 7 loss: 8.268535020761192e-05\n",
      "  batch 8 loss: 0.00010574882617220283\n",
      "  batch 9 loss: 0.00011135647946503013\n",
      "  batch 10 loss: 7.071222353260964e-05\n",
      "  batch 11 loss: 0.0001190988186863251\n",
      "  batch 12 loss: 0.00014868436846882105\n",
      "  batch 13 loss: 0.0001847643288783729\n",
      "  batch 14 loss: 0.0003014089597854763\n",
      "  batch 15 loss: 0.00019373848044779152\n",
      "  batch 16 loss: 0.00012540692114271224\n",
      "  batch 17 loss: 0.0001051374856615439\n",
      "  batch 18 loss: 0.00012050083751091734\n",
      "  batch 19 loss: 0.0002524753799661994\n",
      "  batch 20 loss: 0.00016976171173155308\n",
      "  batch 21 loss: 0.00023529265308752656\n",
      "  batch 22 loss: 0.00015705164696555585\n",
      "  batch 23 loss: 0.00023022625828161836\n",
      "  batch 24 loss: 0.00021925797045696527\n",
      "  batch 25 loss: 0.0002672867849469185\n",
      "  batch 26 loss: 0.000247788499109447\n",
      "  batch 27 loss: 9.973168198484927e-05\n",
      "  batch 28 loss: 0.00014899179222993553\n",
      "  batch 29 loss: 9.165912342723459e-05\n",
      "  batch 30 loss: 9.187177056446671e-05\n",
      "  batch 31 loss: 7.306333282031119e-05\n",
      "  batch 32 loss: 0.00012978617451153696\n",
      "  batch 33 loss: 0.00012065633200109005\n",
      "  batch 34 loss: 0.0001495235919719562\n",
      "  batch 35 loss: 7.335000555031002e-05\n",
      "  batch 36 loss: 7.776671554893255e-05\n",
      "  batch 37 loss: 9.425167081644759e-05\n",
      "  batch 38 loss: 9.350971231469885e-05\n",
      "  batch 39 loss: 0.00012081911700079218\n",
      "  batch 40 loss: 0.00014817419287282974\n",
      "  batch 41 loss: 7.969903526827693e-05\n",
      "  batch 42 loss: 0.0001033830048982054\n",
      "  batch 43 loss: 8.448887092527002e-05\n",
      "  batch 44 loss: 6.195945024956018e-05\n",
      "  batch 45 loss: 6.937925354577601e-05\n",
      "  batch 46 loss: 6.420839054044336e-05\n",
      "  batch 47 loss: 9.844396845437586e-05\n",
      "  batch 48 loss: 9.905237675411627e-05\n",
      "  batch 49 loss: 0.0001149601157521829\n",
      "  batch 50 loss: 0.00013174988271202892\n",
      "  batch 51 loss: 0.00013242787099443376\n",
      "  batch 52 loss: 0.00012058189167873934\n",
      "  batch 53 loss: 0.00012732711911667138\n",
      "  batch 54 loss: 0.00011402595555409789\n",
      "  batch 55 loss: 0.00014634261606261134\n",
      "  batch 56 loss: 0.00010194178321398795\n",
      "  batch 57 loss: 7.157590880524367e-05\n",
      "  batch 58 loss: 9.345178841613233e-05\n",
      "  batch 59 loss: 9.278306970372796e-05\n",
      "  batch 60 loss: 0.00015810805780347437\n",
      "  batch 61 loss: 6.844873132649809e-05\n",
      "  batch 62 loss: 0.0001270876091439277\n",
      "  batch 63 loss: 8.073598291957751e-05\n",
      "  batch 64 loss: 8.160810102708638e-05\n",
      "  batch 65 loss: 8.639092993689701e-05\n",
      "  batch 66 loss: 8.340851491084322e-05\n",
      "  batch 67 loss: 7.69962280173786e-05\n",
      "  batch 68 loss: 9.299833618570119e-05\n",
      "  batch 69 loss: 9.668190614320338e-05\n",
      "  batch 70 loss: 0.00015769051969982684\n",
      "  batch 71 loss: 9.116712317336351e-05\n",
      "  batch 72 loss: 9.514507837593555e-05\n",
      "  batch 73 loss: 9.716280328575522e-05\n",
      "  batch 74 loss: 0.0001276763650821522\n",
      "  batch 75 loss: 0.0001566441060276702\n",
      "  batch 76 loss: 0.00010916840983554721\n",
      "  batch 77 loss: 0.00011270381219219416\n",
      "  batch 78 loss: 0.0001055524408002384\n",
      "  batch 79 loss: 8.661471656523645e-05\n",
      "  batch 80 loss: 0.00010475519229657948\n",
      "  batch 81 loss: 0.00012134320422774181\n",
      "  batch 82 loss: 0.00014050310710445046\n",
      "  batch 83 loss: 0.00013577779463957995\n",
      "  batch 84 loss: 9.910111839417368e-05\n",
      "  batch 85 loss: 0.0001076004991773516\n",
      "  batch 86 loss: 0.00012065644841641188\n",
      "  batch 87 loss: 0.00011654537229333073\n",
      "  batch 88 loss: 0.00019106961553916335\n",
      "  batch 89 loss: 0.00012301187962293625\n",
      "  batch 90 loss: 0.0001817720476537943\n",
      "  batch 91 loss: 0.00011234244448132813\n",
      "  batch 92 loss: 0.00011791700671892613\n",
      "  batch 93 loss: 0.00013340484292712063\n",
      "  batch 94 loss: 0.00013180452515371144\n",
      "  batch 95 loss: 0.00020387191034387797\n",
      "LOSS train 0.00020387191034387797 valid 0.0011875698110088706\n",
      "LOSS train 0.00020387191034387797 valid 0.0012893169187009335\n",
      "LOSS train 0.00020387191034387797 valid 0.0011742745991796255\n",
      "LOSS train 0.00020387191034387797 valid 0.0011456501670181751\n",
      "LOSS train 0.00020387191034387797 valid 0.0011500458931550384\n",
      "LOSS train 0.00020387191034387797 valid 0.001072716899216175\n",
      "LOSS train 0.00020387191034387797 valid 0.0011390902800485492\n",
      "LOSS train 0.00020387191034387797 valid 0.0011896962532773614\n",
      "LOSS train 0.00020387191034387797 valid 0.0011505929287523031\n",
      "LOSS train 0.00020387191034387797 valid 0.0011887861182913184\n",
      "LOSS train 0.00020387191034387797 valid 0.0012576980516314507\n",
      "LOSS train 0.00020387191034387797 valid 0.0012706953566521406\n",
      "LOSS train 0.00020387191034387797 valid 0.001265248516574502\n",
      "LOSS train 0.00020387191034387797 valid 0.0012903788592666388\n",
      "LOSS train 0.00020387191034387797 valid 0.001390677411109209\n",
      "LOSS train 0.00020387191034387797 valid 0.0014500446850433946\n",
      "LOSS train 0.00020387191034387797 valid 0.0014537902316078544\n",
      "LOSS train 0.00020387191034387797 valid 0.0014733909629285336\n",
      "LOSS train 0.00020387191034387797 valid 0.001478118821978569\n",
      "LOSS train 0.00020387191034387797 valid 0.001472769188694656\n",
      "LOSS train 0.00020387191034387797 valid 0.0014506797306239605\n",
      "LOSS train 0.00020387191034387797 valid 0.0014652449171990156\n",
      "LOSS train 0.00020387191034387797 valid 0.0014560569543391466\n",
      "LOSS train 0.00020387191034387797 valid 0.0014633566606789827\n",
      "EPOCH 80:\n",
      "  batch 1 loss: 0.00016399282321799546\n",
      "  batch 2 loss: 9.41427715588361e-05\n",
      "  batch 3 loss: 0.00015042591257952154\n",
      "  batch 4 loss: 0.0001496380427852273\n",
      "  batch 5 loss: 0.00012647344556171447\n",
      "  batch 6 loss: 0.00012930057710036635\n",
      "  batch 7 loss: 8.591578807681799e-05\n",
      "  batch 8 loss: 9.73730202531442e-05\n",
      "  batch 9 loss: 0.00010287997429259121\n",
      "  batch 10 loss: 7.127904245862737e-05\n",
      "  batch 11 loss: 0.00012958821025677025\n",
      "  batch 12 loss: 0.0001556464412715286\n",
      "  batch 13 loss: 0.0002019823295995593\n",
      "  batch 14 loss: 0.0003015296533703804\n",
      "  batch 15 loss: 0.00020629582286346704\n",
      "  batch 16 loss: 0.00012158213212387636\n",
      "  batch 17 loss: 0.00010434524301672354\n",
      "  batch 18 loss: 0.00011307876411592588\n",
      "  batch 19 loss: 0.00022533119772560894\n",
      "  batch 20 loss: 0.00016983490786515176\n",
      "  batch 21 loss: 0.00021532710525207222\n",
      "  batch 22 loss: 0.00014984197332523763\n",
      "  batch 23 loss: 0.00021381658734753728\n",
      "  batch 24 loss: 0.00020196764671709388\n",
      "  batch 25 loss: 0.000246296520344913\n",
      "  batch 26 loss: 0.00026701585738919675\n",
      "  batch 27 loss: 0.0001125404960475862\n",
      "  batch 28 loss: 0.0001683921436779201\n",
      "  batch 29 loss: 9.135811706073582e-05\n",
      "  batch 30 loss: 8.40503053041175e-05\n",
      "  batch 31 loss: 8.264320786111057e-05\n",
      "  batch 32 loss: 0.00014891973114572465\n",
      "  batch 33 loss: 0.00014656162238679826\n",
      "  batch 34 loss: 0.00016379693988710642\n",
      "  batch 35 loss: 7.379172893706709e-05\n",
      "  batch 36 loss: 7.547155837528408e-05\n",
      "  batch 37 loss: 8.495138899888843e-05\n",
      "  batch 38 loss: 9.235780453309417e-05\n",
      "  batch 39 loss: 0.000122951838420704\n",
      "  batch 40 loss: 0.00014113378711044788\n",
      "  batch 41 loss: 7.722607551841065e-05\n",
      "  batch 42 loss: 0.00010882320930249989\n",
      "  batch 43 loss: 8.680814789840952e-05\n",
      "  batch 44 loss: 6.615358870476484e-05\n",
      "  batch 45 loss: 6.985767686273903e-05\n",
      "  batch 46 loss: 6.325804861262441e-05\n",
      "  batch 47 loss: 9.686380508355796e-05\n",
      "  batch 48 loss: 0.00010390217357780784\n",
      "  batch 49 loss: 0.00011428511788835749\n",
      "  batch 50 loss: 0.00013112883607391268\n",
      "  batch 51 loss: 0.00014369009295478463\n",
      "  batch 52 loss: 0.00011178958811797202\n",
      "  batch 53 loss: 0.00011556499521248043\n",
      "  batch 54 loss: 0.00010549674334470183\n",
      "  batch 55 loss: 0.00013657174713443965\n",
      "  batch 56 loss: 9.535858407616615e-05\n",
      "  batch 57 loss: 8.0720434198156e-05\n",
      "  batch 58 loss: 0.00010917686449829489\n",
      "  batch 59 loss: 9.741260873852298e-05\n",
      "  batch 60 loss: 0.00014420873776543885\n",
      "  batch 61 loss: 6.235818727873266e-05\n",
      "  batch 62 loss: 0.00013093353481963277\n",
      "  batch 63 loss: 8.069512114161626e-05\n",
      "  batch 64 loss: 7.650956104043871e-05\n",
      "  batch 65 loss: 8.045404683798552e-05\n",
      "  batch 66 loss: 8.746253297431394e-05\n",
      "  batch 67 loss: 7.937754708109424e-05\n",
      "  batch 68 loss: 9.09086738829501e-05\n",
      "  batch 69 loss: 9.584570943843573e-05\n",
      "  batch 70 loss: 0.000159499206347391\n",
      "  batch 71 loss: 0.00010316901898477226\n",
      "  batch 72 loss: 9.625320672057569e-05\n",
      "  batch 73 loss: 9.993861021939665e-05\n",
      "  batch 74 loss: 0.00012633793812710792\n",
      "  batch 75 loss: 0.0001593909109942615\n",
      "  batch 76 loss: 0.0001076179469237104\n",
      "  batch 77 loss: 0.00011046094732591882\n",
      "  batch 78 loss: 0.00010152618051506579\n",
      "  batch 79 loss: 8.292912389151752e-05\n",
      "  batch 80 loss: 0.00010382126492913812\n",
      "  batch 81 loss: 0.0001203303545480594\n",
      "  batch 82 loss: 0.00013627042062580585\n",
      "  batch 83 loss: 0.0001318602153332904\n",
      "  batch 84 loss: 0.00010054607264464721\n",
      "  batch 85 loss: 0.00010467136598890647\n",
      "  batch 86 loss: 0.00012007237819489092\n",
      "  batch 87 loss: 0.0001162817352451384\n",
      "  batch 88 loss: 0.00019106970285065472\n",
      "  batch 89 loss: 0.0001230446359841153\n",
      "  batch 90 loss: 0.00018048986385110766\n",
      "  batch 91 loss: 0.00010623714479152113\n",
      "  batch 92 loss: 0.000125314254546538\n",
      "  batch 93 loss: 0.00013035126903560013\n",
      "  batch 94 loss: 0.00013563557877205312\n",
      "  batch 95 loss: 0.00020899961236864328\n",
      "LOSS train 0.00020899961236864328 valid 0.0012403447180986404\n",
      "LOSS train 0.00020899961236864328 valid 0.001314352499321103\n",
      "LOSS train 0.00020899961236864328 valid 0.001218385179527104\n",
      "LOSS train 0.00020899961236864328 valid 0.0011745761148631573\n",
      "LOSS train 0.00020899961236864328 valid 0.001183481770567596\n",
      "LOSS train 0.00020899961236864328 valid 0.0011037627700716257\n",
      "LOSS train 0.00020899961236864328 valid 0.0011538120452314615\n",
      "LOSS train 0.00020899961236864328 valid 0.0012032244121655822\n",
      "LOSS train 0.00020899961236864328 valid 0.0011612946400418878\n",
      "LOSS train 0.00020899961236864328 valid 0.001200319267809391\n",
      "LOSS train 0.00020899961236864328 valid 0.0012584946816787124\n",
      "LOSS train 0.00020899961236864328 valid 0.0012715072371065617\n",
      "LOSS train 0.00020899961236864328 valid 0.0012695445911958814\n",
      "LOSS train 0.00020899961236864328 valid 0.001300136442296207\n",
      "LOSS train 0.00020899961236864328 valid 0.0014063335256651044\n",
      "LOSS train 0.00020899961236864328 valid 0.0014676987193524837\n",
      "LOSS train 0.00020899961236864328 valid 0.0014747282257303596\n",
      "LOSS train 0.00020899961236864328 valid 0.0014883090043440461\n",
      "LOSS train 0.00020899961236864328 valid 0.0014895813073962927\n",
      "LOSS train 0.00020899961236864328 valid 0.0014786635292693973\n",
      "LOSS train 0.00020899961236864328 valid 0.001459677005186677\n",
      "LOSS train 0.00020899961236864328 valid 0.0014728776877745986\n",
      "LOSS train 0.00020899961236864328 valid 0.0014642749447375536\n",
      "LOSS train 0.00020899961236864328 valid 0.0014602099545300007\n",
      "EPOCH 81:\n",
      "  batch 1 loss: 0.00015558776794932783\n",
      "  batch 2 loss: 9.581790072843432e-05\n",
      "  batch 3 loss: 0.00015194500156212598\n",
      "  batch 4 loss: 0.0001366500509902835\n",
      "  batch 5 loss: 0.00011320295743644238\n",
      "  batch 6 loss: 0.00013359598233364522\n",
      "  batch 7 loss: 8.789791900198907e-05\n",
      "  batch 8 loss: 0.00011887649452546611\n",
      "  batch 9 loss: 0.00010557555651757866\n",
      "  batch 10 loss: 7.619891403010115e-05\n",
      "  batch 11 loss: 0.00011724327487172559\n",
      "  batch 12 loss: 0.00014798645861446857\n",
      "  batch 13 loss: 0.0001864316436694935\n",
      "  batch 14 loss: 0.00028133526211604476\n",
      "  batch 15 loss: 0.00021144557103980333\n",
      "  batch 16 loss: 0.00011645858467090875\n",
      "  batch 17 loss: 0.00011775863822549582\n",
      "  batch 18 loss: 0.0001168651069747284\n",
      "  batch 19 loss: 0.0002270768745802343\n",
      "  batch 20 loss: 0.00017043109983205795\n",
      "  batch 21 loss: 0.00020845257677137852\n",
      "  batch 22 loss: 0.0001390052930219099\n",
      "  batch 23 loss: 0.00019914393487852067\n",
      "  batch 24 loss: 0.000192347913980484\n",
      "  batch 25 loss: 0.00023407973640132695\n",
      "  batch 26 loss: 0.00024126414791680872\n",
      "  batch 27 loss: 0.00010956780170090497\n",
      "  batch 28 loss: 0.00017795256280805916\n",
      "  batch 29 loss: 9.218767809215933e-05\n",
      "  batch 30 loss: 7.981641101650894e-05\n",
      "  batch 31 loss: 7.107589044608176e-05\n",
      "  batch 32 loss: 0.0001318722206633538\n",
      "  batch 33 loss: 0.00012277277710381895\n",
      "  batch 34 loss: 0.00015498358698096126\n",
      "  batch 35 loss: 7.501005893573165e-05\n",
      "  batch 36 loss: 8.550364873372018e-05\n",
      "  batch 37 loss: 8.405914559261873e-05\n",
      "  batch 38 loss: 9.316310024587438e-05\n",
      "  batch 39 loss: 0.00011581990111153573\n",
      "  batch 40 loss: 0.00014585955068469048\n",
      "  batch 41 loss: 7.758323772577569e-05\n",
      "  batch 42 loss: 0.00011070817708969116\n",
      "  batch 43 loss: 8.971017086878419e-05\n",
      "  batch 44 loss: 6.713379843859002e-05\n",
      "  batch 45 loss: 6.786314770579338e-05\n",
      "  batch 46 loss: 6.176151509862393e-05\n",
      "  batch 47 loss: 9.312002657679841e-05\n",
      "  batch 48 loss: 9.61816476774402e-05\n",
      "  batch 49 loss: 0.00011137753608636558\n",
      "  batch 50 loss: 0.0001239977136719972\n",
      "  batch 51 loss: 0.0001393311977153644\n",
      "  batch 52 loss: 0.00011328763503115624\n",
      "  batch 53 loss: 0.00012078638246748596\n",
      "  batch 54 loss: 0.00011006791464751586\n",
      "  batch 55 loss: 0.00013443474017549306\n",
      "  batch 56 loss: 9.036542905960232e-05\n",
      "  batch 57 loss: 6.83910766383633e-05\n",
      "  batch 58 loss: 9.300684905610979e-05\n",
      "  batch 59 loss: 8.665418135933578e-05\n",
      "  batch 60 loss: 0.0001499275240348652\n",
      "  batch 61 loss: 6.310212484095246e-05\n",
      "  batch 62 loss: 0.00013315450632944703\n",
      "  batch 63 loss: 8.306892414111644e-05\n",
      "  batch 64 loss: 7.414793071802706e-05\n",
      "  batch 65 loss: 7.333159737754613e-05\n",
      "  batch 66 loss: 7.93677318142727e-05\n",
      "  batch 67 loss: 7.198839739430696e-05\n",
      "  batch 68 loss: 8.145884930854663e-05\n",
      "  batch 69 loss: 8.699140016688034e-05\n",
      "  batch 70 loss: 0.00014787203690502793\n",
      "  batch 71 loss: 9.514260455034673e-05\n",
      "  batch 72 loss: 9.285638225264847e-05\n",
      "  batch 73 loss: 9.636990580474958e-05\n",
      "  batch 74 loss: 0.00011952720524277538\n",
      "  batch 75 loss: 0.00015607726527377963\n",
      "  batch 76 loss: 0.00010375508281867951\n",
      "  batch 77 loss: 0.00010894823935814202\n",
      "  batch 78 loss: 0.00010053295409306884\n",
      "  batch 79 loss: 8.083241118583828e-05\n",
      "  batch 80 loss: 9.946826321538538e-05\n",
      "  batch 81 loss: 0.00010958894563373178\n",
      "  batch 82 loss: 0.00012849317863583565\n",
      "  batch 83 loss: 0.00012604663788806647\n",
      "  batch 84 loss: 9.54863935476169e-05\n",
      "  batch 85 loss: 0.00010864665091503412\n",
      "  batch 86 loss: 0.00012612361751962453\n",
      "  batch 87 loss: 0.00011816262122010812\n",
      "  batch 88 loss: 0.00019070619600825012\n",
      "  batch 89 loss: 0.00012173235882073641\n",
      "  batch 90 loss: 0.00016804001643322408\n",
      "  batch 91 loss: 0.0001075950640370138\n",
      "  batch 92 loss: 0.00012983736814931035\n",
      "  batch 93 loss: 0.000132907327497378\n",
      "  batch 94 loss: 0.00013597880024462938\n",
      "  batch 95 loss: 0.00021270601428113878\n",
      "LOSS train 0.00021270601428113878 valid 0.001168102491647005\n",
      "LOSS train 0.00021270601428113878 valid 0.0012365732109174132\n",
      "LOSS train 0.00021270601428113878 valid 0.0012186163803562522\n",
      "LOSS train 0.00021270601428113878 valid 0.0011653718538582325\n",
      "LOSS train 0.00021270601428113878 valid 0.001164628891274333\n",
      "LOSS train 0.00021270601428113878 valid 0.0010759285651147366\n",
      "LOSS train 0.00021270601428113878 valid 0.001118337269872427\n",
      "LOSS train 0.00021270601428113878 valid 0.0011663876939564943\n",
      "LOSS train 0.00021270601428113878 valid 0.0011295656440779567\n",
      "LOSS train 0.00021270601428113878 valid 0.0011518382234498858\n",
      "LOSS train 0.00021270601428113878 valid 0.0011888814624398947\n",
      "LOSS train 0.00021270601428113878 valid 0.0012032401282340288\n",
      "LOSS train 0.00021270601428113878 valid 0.0012142687337473035\n",
      "LOSS train 0.00021270601428113878 valid 0.0012352147605270147\n",
      "LOSS train 0.00021270601428113878 valid 0.0013023627689108253\n",
      "LOSS train 0.00021270601428113878 valid 0.0013499332126230001\n",
      "LOSS train 0.00021270601428113878 valid 0.0013598516816273332\n",
      "LOSS train 0.00021270601428113878 valid 0.0013687476748600602\n",
      "LOSS train 0.00021270601428113878 valid 0.001369947916828096\n",
      "LOSS train 0.00021270601428113878 valid 0.0013611632166430354\n",
      "LOSS train 0.00021270601428113878 valid 0.0013497731415554881\n",
      "LOSS train 0.00021270601428113878 valid 0.0013646847801283002\n",
      "LOSS train 0.00021270601428113878 valid 0.0013572025345638394\n",
      "LOSS train 0.00021270601428113878 valid 0.001347342855297029\n",
      "EPOCH 82:\n",
      "  batch 1 loss: 0.00016249586769845337\n",
      "  batch 2 loss: 0.00010044770169770345\n",
      "  batch 3 loss: 0.0001579494564794004\n",
      "  batch 4 loss: 0.0001357554574497044\n",
      "  batch 5 loss: 0.00010873260180233046\n",
      "  batch 6 loss: 0.00012449437053874135\n",
      "  batch 7 loss: 8.620625885669142e-05\n",
      "  batch 8 loss: 0.00010276390821672976\n",
      "  batch 9 loss: 0.00011009725858457386\n",
      "  batch 10 loss: 7.463453221134841e-05\n",
      "  batch 11 loss: 0.00011636350973276421\n",
      "  batch 12 loss: 0.00014869705773890018\n",
      "  batch 13 loss: 0.0001943326205946505\n",
      "  batch 14 loss: 0.00027406291337683797\n",
      "  batch 15 loss: 0.00018554358393885195\n",
      "  batch 16 loss: 0.00011515255755512044\n",
      "  batch 17 loss: 0.00011479340901132673\n",
      "  batch 18 loss: 0.00011665736383292824\n",
      "  batch 19 loss: 0.00024000226403586566\n",
      "  batch 20 loss: 0.00016675289953127503\n",
      "  batch 21 loss: 0.000210270649404265\n",
      "  batch 22 loss: 0.00014416051271837205\n",
      "  batch 23 loss: 0.00019521525246091187\n",
      "  batch 24 loss: 0.00018437372636981308\n",
      "  batch 25 loss: 0.00023405547835864127\n",
      "  batch 26 loss: 0.00024153005506377667\n",
      "  batch 27 loss: 0.00010825123899849132\n",
      "  batch 28 loss: 0.00015348935266956687\n",
      "  batch 29 loss: 8.848118159221485e-05\n",
      "  batch 30 loss: 8.236808935180306e-05\n",
      "  batch 31 loss: 7.281650323420763e-05\n",
      "  batch 32 loss: 0.0001236811513081193\n",
      "  batch 33 loss: 0.00011647288920357823\n",
      "  batch 34 loss: 0.00014810406719334424\n",
      "  batch 35 loss: 7.519769133068621e-05\n",
      "  batch 36 loss: 8.551318023819476e-05\n",
      "  batch 37 loss: 8.731134585104883e-05\n",
      "  batch 38 loss: 9.319809032604098e-05\n",
      "  batch 39 loss: 0.00011703753261826932\n",
      "  batch 40 loss: 0.0001386330259265378\n",
      "  batch 41 loss: 7.040298078209162e-05\n",
      "  batch 42 loss: 0.00010255137749481946\n",
      "  batch 43 loss: 8.261550829047337e-05\n",
      "  batch 44 loss: 6.386406312230974e-05\n",
      "  batch 45 loss: 7.453706348314881e-05\n",
      "  batch 46 loss: 8.441190584562719e-05\n",
      "  batch 47 loss: 0.00010150300659006462\n",
      "  batch 48 loss: 9.735742787597701e-05\n",
      "  batch 49 loss: 0.00010608193406369537\n",
      "  batch 50 loss: 0.00012388074537739158\n",
      "  batch 51 loss: 0.00013445274089463055\n",
      "  batch 52 loss: 0.00010888639371842146\n",
      "  batch 53 loss: 0.00012594900908879936\n",
      "  batch 54 loss: 0.00011966026795562357\n",
      "  batch 55 loss: 0.00014549428306054324\n",
      "  batch 56 loss: 9.574602881912142e-05\n",
      "  batch 57 loss: 7.277549593709409e-05\n",
      "  batch 58 loss: 8.513060311088338e-05\n",
      "  batch 59 loss: 8.461251127300784e-05\n",
      "  batch 60 loss: 0.0001443404471501708\n",
      "  batch 61 loss: 5.702046109945513e-05\n",
      "  batch 62 loss: 0.00013743212912231684\n",
      "  batch 63 loss: 8.566306496504694e-05\n",
      "  batch 64 loss: 7.400654430966824e-05\n",
      "  batch 65 loss: 7.359177834587172e-05\n",
      "  batch 66 loss: 7.285992614924908e-05\n",
      "  batch 67 loss: 7.202046981547028e-05\n",
      "  batch 68 loss: 8.288747631013393e-05\n",
      "  batch 69 loss: 8.760442142374814e-05\n",
      "  batch 70 loss: 0.0001511002774350345\n",
      "  batch 71 loss: 8.906112634576857e-05\n",
      "  batch 72 loss: 8.79958679433912e-05\n",
      "  batch 73 loss: 9.200080239679664e-05\n",
      "  batch 74 loss: 0.00011721673945430666\n",
      "  batch 75 loss: 0.00015799007087480277\n",
      "  batch 76 loss: 0.00010679266415536404\n",
      "  batch 77 loss: 0.00010710825154092163\n",
      "  batch 78 loss: 9.571095870342106e-05\n",
      "  batch 79 loss: 7.518186612287536e-05\n",
      "  batch 80 loss: 9.714988118503243e-05\n",
      "  batch 81 loss: 0.00011157729750266299\n",
      "  batch 82 loss: 0.00012656988110393286\n",
      "  batch 83 loss: 0.00013192677579354495\n",
      "  batch 84 loss: 9.209780546370894e-05\n",
      "  batch 85 loss: 0.00010133185423910618\n",
      "  batch 86 loss: 0.0001198973914142698\n",
      "  batch 87 loss: 0.00011125286982860416\n",
      "  batch 88 loss: 0.00017960567492991686\n",
      "  batch 89 loss: 0.00012021274596918374\n",
      "  batch 90 loss: 0.00017487634613644332\n",
      "  batch 91 loss: 0.00011035584611818194\n",
      "  batch 92 loss: 0.00012175480514997616\n",
      "  batch 93 loss: 0.0001235554664162919\n",
      "  batch 94 loss: 0.00011938719399040565\n",
      "  batch 95 loss: 0.00019811785023193806\n",
      "LOSS train 0.00019811785023193806 valid 0.0011693384731188416\n",
      "LOSS train 0.00019811785023193806 valid 0.0012873022351413965\n",
      "LOSS train 0.00019811785023193806 valid 0.0012072570389136672\n",
      "LOSS train 0.00019811785023193806 valid 0.0011602016165852547\n",
      "LOSS train 0.00019811785023193806 valid 0.001160710002295673\n",
      "LOSS train 0.00019811785023193806 valid 0.0010676871752366424\n",
      "LOSS train 0.00019811785023193806 valid 0.001126095070503652\n",
      "LOSS train 0.00019811785023193806 valid 0.001178956707008183\n",
      "LOSS train 0.00019811785023193806 valid 0.001139690401032567\n",
      "LOSS train 0.00019811785023193806 valid 0.001166553352959454\n",
      "LOSS train 0.00019811785023193806 valid 0.0012121559120714664\n",
      "LOSS train 0.00019811785023193806 valid 0.0012234030291438103\n",
      "LOSS train 0.00019811785023193806 valid 0.0012277149362489581\n",
      "LOSS train 0.00019811785023193806 valid 0.001250107423402369\n",
      "LOSS train 0.00019811785023193806 valid 0.0013406930956989527\n",
      "LOSS train 0.00019811785023193806 valid 0.0013906920794397593\n",
      "LOSS train 0.00019811785023193806 valid 0.0013973481254652143\n",
      "LOSS train 0.00019811785023193806 valid 0.001414465019479394\n",
      "LOSS train 0.00019811785023193806 valid 0.0014217952266335487\n",
      "LOSS train 0.00019811785023193806 valid 0.001416164101101458\n",
      "LOSS train 0.00019811785023193806 valid 0.001396814128383994\n",
      "LOSS train 0.00019811785023193806 valid 0.0014113987563177943\n",
      "LOSS train 0.00019811785023193806 valid 0.001404817565344274\n",
      "LOSS train 0.00019811785023193806 valid 0.001403326983563602\n",
      "EPOCH 83:\n",
      "  batch 1 loss: 0.00016216631047427654\n",
      "  batch 2 loss: 0.00010459087934577838\n",
      "  batch 3 loss: 0.00016620397218503058\n",
      "  batch 4 loss: 0.00014058260421734303\n",
      "  batch 5 loss: 0.00010798625589814037\n",
      "  batch 6 loss: 0.00011749184341169894\n",
      "  batch 7 loss: 7.82117058406584e-05\n",
      "  batch 8 loss: 9.145441435975954e-05\n",
      "  batch 9 loss: 9.8309428722132e-05\n",
      "  batch 10 loss: 6.813091749791056e-05\n",
      "  batch 11 loss: 0.00011547353642527014\n",
      "  batch 12 loss: 0.00014203021419234574\n",
      "  batch 13 loss: 0.00018079124856740236\n",
      "  batch 14 loss: 0.00027358336956240237\n",
      "  batch 15 loss: 0.00018828909378498793\n",
      "  batch 16 loss: 0.00010678497346816584\n",
      "  batch 17 loss: 0.00010110775474458933\n",
      "  batch 18 loss: 0.00010783172183437273\n",
      "  batch 19 loss: 0.00021874146477784961\n",
      "  batch 20 loss: 0.00015573407290503383\n",
      "  batch 21 loss: 0.00021075550466775894\n",
      "  batch 22 loss: 0.00014571908104699105\n",
      "  batch 23 loss: 0.00021018173720221967\n",
      "  batch 24 loss: 0.0001917974732350558\n",
      "  batch 25 loss: 0.00023085334396455437\n",
      "  batch 26 loss: 0.00022791378432884812\n",
      "  batch 27 loss: 9.809849871089682e-05\n",
      "  batch 28 loss: 0.00015114656707737595\n",
      "  batch 29 loss: 8.619365689810365e-05\n",
      "  batch 30 loss: 7.615657523274422e-05\n",
      "  batch 31 loss: 6.540109461639076e-05\n",
      "  batch 32 loss: 0.0001165859357570298\n",
      "  batch 33 loss: 0.00012329962919466197\n",
      "  batch 34 loss: 0.0001493195741204545\n",
      "  batch 35 loss: 7.883344369474798e-05\n",
      "  batch 36 loss: 8.795173198450357e-05\n",
      "  batch 37 loss: 8.861662354320288e-05\n",
      "  batch 38 loss: 9.31572649278678e-05\n",
      "  batch 39 loss: 0.00011594016541494057\n",
      "  batch 40 loss: 0.00013006548397243023\n",
      "  batch 41 loss: 6.809216574765742e-05\n",
      "  batch 42 loss: 0.00010145221312996\n",
      "  batch 43 loss: 7.693653606111184e-05\n",
      "  batch 44 loss: 5.614825204247609e-05\n",
      "  batch 45 loss: 6.567015952896327e-05\n",
      "  batch 46 loss: 6.72445457894355e-05\n",
      "  batch 47 loss: 9.612359281163663e-05\n",
      "  batch 48 loss: 9.876250987872481e-05\n",
      "  batch 49 loss: 0.00010960453073494136\n",
      "  batch 50 loss: 0.00012610855628736317\n",
      "  batch 51 loss: 0.00012198348122183233\n",
      "  batch 52 loss: 0.00010247738100588322\n",
      "  batch 53 loss: 0.0001207047316711396\n",
      "  batch 54 loss: 0.00010778151045087725\n",
      "  batch 55 loss: 0.0001407387899234891\n",
      "  batch 56 loss: 8.661563333589584e-05\n",
      "  batch 57 loss: 8.45596005092375e-05\n",
      "  batch 58 loss: 0.00011196613922948018\n",
      "  batch 59 loss: 0.0001143194385804236\n",
      "  batch 60 loss: 0.0001360922324238345\n",
      "  batch 61 loss: 5.4793505114503205e-05\n",
      "  batch 62 loss: 0.00013114960165694356\n",
      "  batch 63 loss: 9.002792648971081e-05\n",
      "  batch 64 loss: 8.79121434991248e-05\n",
      "  batch 65 loss: 8.593297388870269e-05\n",
      "  batch 66 loss: 7.719642599113286e-05\n",
      "  batch 67 loss: 7.355122943408787e-05\n",
      "  batch 68 loss: 8.632080425741151e-05\n",
      "  batch 69 loss: 8.932264609029517e-05\n",
      "  batch 70 loss: 0.00015050866932142526\n",
      "  batch 71 loss: 9.178282925859094e-05\n",
      "  batch 72 loss: 9.578427125234157e-05\n",
      "  batch 73 loss: 9.495112317381427e-05\n",
      "  batch 74 loss: 0.00011949091276619583\n",
      "  batch 75 loss: 0.00015395550872199237\n",
      "  batch 76 loss: 0.00010228114115307108\n",
      "  batch 77 loss: 0.00010940192441921681\n",
      "  batch 78 loss: 9.91422202787362e-05\n",
      "  batch 79 loss: 7.573938637506217e-05\n",
      "  batch 80 loss: 9.722728282213211e-05\n",
      "  batch 81 loss: 0.00011195271508768201\n",
      "  batch 82 loss: 0.00012079212319804356\n",
      "  batch 83 loss: 0.0001259466225747019\n",
      "  batch 84 loss: 9.356143709737808e-05\n",
      "  batch 85 loss: 0.00010287360055372119\n",
      "  batch 86 loss: 0.00011295393778709695\n",
      "  batch 87 loss: 0.00011093410284956917\n",
      "  batch 88 loss: 0.00017325884255114943\n",
      "  batch 89 loss: 0.0001125170529121533\n",
      "  batch 90 loss: 0.00017234343977179378\n",
      "  batch 91 loss: 0.00010544934775680304\n",
      "  batch 92 loss: 0.00011495706712594256\n",
      "  batch 93 loss: 0.00012122650514356792\n",
      "  batch 94 loss: 0.00012161541962996125\n",
      "  batch 95 loss: 0.00019237039668951184\n",
      "LOSS train 0.00019237039668951184 valid 0.0011759484186768532\n",
      "LOSS train 0.00019237039668951184 valid 0.0012773353373631835\n",
      "LOSS train 0.00019237039668951184 valid 0.0012106051435694098\n",
      "LOSS train 0.00019237039668951184 valid 0.0011716708540916443\n",
      "LOSS train 0.00019237039668951184 valid 0.00117438321467489\n",
      "LOSS train 0.00019237039668951184 valid 0.0010873455321416259\n",
      "LOSS train 0.00019237039668951184 valid 0.0011308123357594013\n",
      "LOSS train 0.00019237039668951184 valid 0.0011856561759486794\n",
      "LOSS train 0.00019237039668951184 valid 0.0011484299320727587\n",
      "LOSS train 0.00019237039668951184 valid 0.0011729205725714564\n",
      "LOSS train 0.00019237039668951184 valid 0.0012202125508338213\n",
      "LOSS train 0.00019237039668951184 valid 0.0012287445133551955\n",
      "LOSS train 0.00019237039668951184 valid 0.001233142102137208\n",
      "LOSS train 0.00019237039668951184 valid 0.0012566869845613837\n",
      "LOSS train 0.00019237039668951184 valid 0.0013396553695201874\n",
      "LOSS train 0.00019237039668951184 valid 0.0013935184106230736\n",
      "LOSS train 0.00019237039668951184 valid 0.0014011268503963947\n",
      "LOSS train 0.00019237039668951184 valid 0.001416221959516406\n",
      "LOSS train 0.00019237039668951184 valid 0.0014178827404975891\n",
      "LOSS train 0.00019237039668951184 valid 0.0014141419669613242\n",
      "LOSS train 0.00019237039668951184 valid 0.0013996619964018464\n",
      "LOSS train 0.00019237039668951184 valid 0.001412474550306797\n",
      "LOSS train 0.00019237039668951184 valid 0.0014045708812773228\n",
      "LOSS train 0.00019237039668951184 valid 0.001405489630997181\n",
      "EPOCH 84:\n",
      "  batch 1 loss: 0.00016008756938390434\n",
      "  batch 2 loss: 9.015384421218187e-05\n",
      "  batch 3 loss: 0.0001458169281249866\n",
      "  batch 4 loss: 0.00012538500595837831\n",
      "  batch 5 loss: 0.00010322732850909233\n",
      "  batch 6 loss: 0.00010856176959350705\n",
      "  batch 7 loss: 7.918531628092751e-05\n",
      "  batch 8 loss: 9.817139653023332e-05\n",
      "  batch 9 loss: 0.00010380998719483614\n",
      "  batch 10 loss: 6.661655061179772e-05\n",
      "  batch 11 loss: 0.00012081873865099624\n",
      "  batch 12 loss: 0.0001454551238566637\n",
      "  batch 13 loss: 0.00017181076691485941\n",
      "  batch 14 loss: 0.00029201401048339903\n",
      "  batch 15 loss: 0.0001814630231820047\n",
      "  batch 16 loss: 0.00011014440678991377\n",
      "  batch 17 loss: 0.00010759074211819097\n",
      "  batch 18 loss: 0.00012344479910098016\n",
      "  batch 19 loss: 0.000244057213421911\n",
      "  batch 20 loss: 0.00017902690160553902\n",
      "  batch 21 loss: 0.00024100957671180367\n",
      "  batch 22 loss: 0.00013923828373663127\n",
      "  batch 23 loss: 0.0002217948785983026\n",
      "  batch 24 loss: 0.0001960367662832141\n",
      "  batch 25 loss: 0.00026284423074685037\n",
      "  batch 26 loss: 0.00024238089099526405\n",
      "  batch 27 loss: 0.0001020849886117503\n",
      "  batch 28 loss: 0.00015692191664129496\n",
      "  batch 29 loss: 8.810768486000597e-05\n",
      "  batch 30 loss: 8.067511953413486e-05\n",
      "  batch 31 loss: 7.690239726798609e-05\n",
      "  batch 32 loss: 0.0001270528300665319\n",
      "  batch 33 loss: 0.00011624558828771114\n",
      "  batch 34 loss: 0.00014690496027469635\n",
      "  batch 35 loss: 7.365526107605547e-05\n",
      "  batch 36 loss: 7.471468416042626e-05\n",
      "  batch 37 loss: 8.962904394138604e-05\n",
      "  batch 38 loss: 8.854445331962779e-05\n",
      "  batch 39 loss: 0.00011299774632789195\n",
      "  batch 40 loss: 0.00014091128832660615\n",
      "  batch 41 loss: 6.854935782030225e-05\n",
      "  batch 42 loss: 9.995992877520621e-05\n",
      "  batch 43 loss: 7.55206638132222e-05\n",
      "  batch 44 loss: 5.604705074802041e-05\n",
      "  batch 45 loss: 6.751298496965319e-05\n",
      "  batch 46 loss: 6.508053047582507e-05\n",
      "  batch 47 loss: 9.861706348601729e-05\n",
      "  batch 48 loss: 0.00010311993537470698\n",
      "  batch 49 loss: 0.00011337914475006983\n",
      "  batch 50 loss: 0.00012972508557140827\n",
      "  batch 51 loss: 0.0001249650085810572\n",
      "  batch 52 loss: 9.827090980252251e-05\n",
      "  batch 53 loss: 0.00010586967982817441\n",
      "  batch 54 loss: 9.892898378893733e-05\n",
      "  batch 55 loss: 0.00013935068272985518\n",
      "  batch 56 loss: 9.710222366265953e-05\n",
      "  batch 57 loss: 9.71734116319567e-05\n",
      "  batch 58 loss: 0.00011827075650217012\n",
      "  batch 59 loss: 0.00013848094386048615\n",
      "  batch 60 loss: 0.00014246409409679472\n",
      "  batch 61 loss: 6.193722219904885e-05\n",
      "  batch 62 loss: 0.00012079025327693671\n",
      "  batch 63 loss: 8.902738045435399e-05\n",
      "  batch 64 loss: 7.929506682557985e-05\n",
      "  batch 65 loss: 9.519196464680135e-05\n",
      "  batch 66 loss: 0.00010034724255092442\n",
      "  batch 67 loss: 9.358862735098228e-05\n",
      "  batch 68 loss: 0.00010106217814609408\n",
      "  batch 69 loss: 9.587050590198487e-05\n",
      "  batch 70 loss: 0.0001489096030127257\n",
      "  batch 71 loss: 0.00010420613398309797\n",
      "  batch 72 loss: 9.937182039720938e-05\n",
      "  batch 73 loss: 0.00010205961734754965\n",
      "  batch 74 loss: 0.0001238748081959784\n",
      "  batch 75 loss: 0.0001603619020897895\n",
      "  batch 76 loss: 0.00010554133768891916\n",
      "  batch 77 loss: 0.00011369300773367286\n",
      "  batch 78 loss: 0.0001082811868400313\n",
      "  batch 79 loss: 7.866455416660756e-05\n",
      "  batch 80 loss: 9.850804053712636e-05\n",
      "  batch 81 loss: 0.00011317970347590744\n",
      "  batch 82 loss: 0.0001164866698672995\n",
      "  batch 83 loss: 0.00012609003169927746\n",
      "  batch 84 loss: 9.718175715534016e-05\n",
      "  batch 85 loss: 0.00010823590855579823\n",
      "  batch 86 loss: 0.0001223539002239704\n",
      "  batch 87 loss: 0.00011297526361886412\n",
      "  batch 88 loss: 0.00017496987129561603\n",
      "  batch 89 loss: 0.00011829518189188093\n",
      "  batch 90 loss: 0.0001655888045206666\n",
      "  batch 91 loss: 0.00010359958105254918\n",
      "  batch 92 loss: 0.00011823386012110859\n",
      "  batch 93 loss: 0.00013187012518756092\n",
      "  batch 94 loss: 0.00012959481682628393\n",
      "  batch 95 loss: 0.00019821127352770418\n",
      "LOSS train 0.00019821127352770418 valid 0.0011863091494888067\n",
      "LOSS train 0.00019821127352770418 valid 0.001318295719102025\n",
      "LOSS train 0.00019821127352770418 valid 0.0011835091281682253\n",
      "LOSS train 0.00019821127352770418 valid 0.0011306044179946184\n",
      "LOSS train 0.00019821127352770418 valid 0.0011271762195974588\n",
      "LOSS train 0.00019821127352770418 valid 0.0010530251311138272\n",
      "LOSS train 0.00019821127352770418 valid 0.0011180572910234332\n",
      "LOSS train 0.00019821127352770418 valid 0.001177505706436932\n",
      "LOSS train 0.00019821127352770418 valid 0.0011342819780111313\n",
      "LOSS train 0.00019821127352770418 valid 0.0011611528461799026\n",
      "LOSS train 0.00019821127352770418 valid 0.0012282035313546658\n",
      "LOSS train 0.00019821127352770418 valid 0.001236699870787561\n",
      "LOSS train 0.00019821127352770418 valid 0.0012254944304004312\n",
      "LOSS train 0.00019821127352770418 valid 0.001250825822353363\n",
      "LOSS train 0.00019821127352770418 valid 0.0013433932326734066\n",
      "LOSS train 0.00019821127352770418 valid 0.0013955449685454369\n",
      "LOSS train 0.00019821127352770418 valid 0.0013985413825139403\n",
      "LOSS train 0.00019821127352770418 valid 0.0014166852924972773\n",
      "LOSS train 0.00019821127352770418 valid 0.0014260659227147698\n",
      "LOSS train 0.00019821127352770418 valid 0.0014196103438735008\n",
      "LOSS train 0.00019821127352770418 valid 0.0013990425504744053\n",
      "LOSS train 0.00019821127352770418 valid 0.001416143262758851\n",
      "LOSS train 0.00019821127352770418 valid 0.0014122753636911511\n",
      "LOSS train 0.00019821127352770418 valid 0.0014143816661089659\n",
      "EPOCH 85:\n",
      "  batch 1 loss: 0.00016057793982326984\n",
      "  batch 2 loss: 8.785795944277197e-05\n",
      "  batch 3 loss: 0.00014515849761664867\n",
      "  batch 4 loss: 0.00012316598440520465\n",
      "  batch 5 loss: 0.00011019206431228667\n",
      "  batch 6 loss: 0.0001178062884719111\n",
      "  batch 7 loss: 7.705210009589791e-05\n",
      "  batch 8 loss: 9.719707304611802e-05\n",
      "  batch 9 loss: 0.00010833804844878614\n",
      "  batch 10 loss: 7.012332935119048e-05\n",
      "  batch 11 loss: 0.00011805102985817939\n",
      "  batch 12 loss: 0.00014112243661656976\n",
      "  batch 13 loss: 0.0001719645515549928\n",
      "  batch 14 loss: 0.00029348264797590673\n",
      "  batch 15 loss: 0.00017848180141299963\n",
      "  batch 16 loss: 0.00010858618770726025\n",
      "  batch 17 loss: 9.89158870652318e-05\n",
      "  batch 18 loss: 0.00011082159471698105\n",
      "  batch 19 loss: 0.00026519145467318594\n",
      "  batch 20 loss: 0.000171033272636123\n",
      "  batch 21 loss: 0.0002266090305056423\n",
      "  batch 22 loss: 0.00013164899428375065\n",
      "  batch 23 loss: 0.00019796044216491282\n",
      "  batch 24 loss: 0.00019339121354278177\n",
      "  batch 25 loss: 0.0002294703444931656\n",
      "  batch 26 loss: 0.0002409783483017236\n",
      "  batch 27 loss: 0.00010066869435831904\n",
      "  batch 28 loss: 0.0001642721617827192\n",
      "  batch 29 loss: 8.869988960213959e-05\n",
      "  batch 30 loss: 7.915111928014085e-05\n",
      "  batch 31 loss: 6.667785783065483e-05\n",
      "  batch 32 loss: 0.00011946178710786626\n",
      "  batch 33 loss: 0.00011836490739369765\n",
      "  batch 34 loss: 0.00013829543604515493\n",
      "  batch 35 loss: 6.865976320113987e-05\n",
      "  batch 36 loss: 7.694609666941687e-05\n",
      "  batch 37 loss: 8.551310747861862e-05\n",
      "  batch 38 loss: 9.310922177974135e-05\n",
      "  batch 39 loss: 0.00011473682388896123\n",
      "  batch 40 loss: 0.00014367737458087504\n",
      "  batch 41 loss: 6.7646469688043e-05\n",
      "  batch 42 loss: 9.809405310079455e-05\n",
      "  batch 43 loss: 7.173148333095014e-05\n",
      "  batch 44 loss: 5.220688399276696e-05\n",
      "  batch 45 loss: 6.055177436792292e-05\n",
      "  batch 46 loss: 5.788207636214793e-05\n",
      "  batch 47 loss: 8.91276795300655e-05\n",
      "  batch 48 loss: 9.416400280315429e-05\n",
      "  batch 49 loss: 0.0001093925311579369\n",
      "  batch 50 loss: 0.00012696169142145663\n",
      "  batch 51 loss: 0.00012800621334463358\n",
      "  batch 52 loss: 9.820115519687533e-05\n",
      "  batch 53 loss: 0.00010296815889887512\n",
      "  batch 54 loss: 9.660310752224177e-05\n",
      "  batch 55 loss: 0.0001378211163682863\n",
      "  batch 56 loss: 8.809904102236032e-05\n",
      "  batch 57 loss: 8.222588803619146e-05\n",
      "  batch 58 loss: 0.00010208020830759779\n",
      "  batch 59 loss: 0.00010678989929147065\n",
      "  batch 60 loss: 0.00015305672422982752\n",
      "  batch 61 loss: 6.06278408668004e-05\n",
      "  batch 62 loss: 0.00013616889191325754\n",
      "  batch 63 loss: 7.610186003148556e-05\n",
      "  batch 64 loss: 7.034195004962385e-05\n",
      "  batch 65 loss: 7.812739204382524e-05\n",
      "  batch 66 loss: 8.590760990045965e-05\n",
      "  batch 67 loss: 7.724665920250118e-05\n",
      "  batch 68 loss: 9.215607860824093e-05\n",
      "  batch 69 loss: 9.874435636447743e-05\n",
      "  batch 70 loss: 0.00015209373668767512\n",
      "  batch 71 loss: 0.00010102969827130437\n",
      "  batch 72 loss: 9.106549987336621e-05\n",
      "  batch 73 loss: 9.635162859922275e-05\n",
      "  batch 74 loss: 0.0001245266175828874\n",
      "  batch 75 loss: 0.000160272786160931\n",
      "  batch 76 loss: 0.0001019608971546404\n",
      "  batch 77 loss: 0.0001079847861547023\n",
      "  batch 78 loss: 0.00010446321539347991\n",
      "  batch 79 loss: 8.074119978118688e-05\n",
      "  batch 80 loss: 0.00010356439452152699\n",
      "  batch 81 loss: 0.00012045914627378806\n",
      "  batch 82 loss: 0.000116269729915075\n",
      "  batch 83 loss: 0.00013197935186326504\n",
      "  batch 84 loss: 9.536094876239076e-05\n",
      "  batch 85 loss: 0.00011203000030945987\n",
      "  batch 86 loss: 0.00012497827992774546\n",
      "  batch 87 loss: 0.00012147581583121791\n",
      "  batch 88 loss: 0.00018422852735966444\n",
      "  batch 89 loss: 0.00012932144454680383\n",
      "  batch 90 loss: 0.0001671044301474467\n",
      "  batch 91 loss: 0.0001031986903399229\n",
      "  batch 92 loss: 0.00012064047041349113\n",
      "  batch 93 loss: 0.00012663533561863005\n",
      "  batch 94 loss: 0.0001296151604037732\n",
      "  batch 95 loss: 0.0001998606458073482\n",
      "LOSS train 0.0001998606458073482 valid 0.0011538793332874775\n",
      "LOSS train 0.0001998606458073482 valid 0.001303599332459271\n",
      "LOSS train 0.0001998606458073482 valid 0.0011623612372204661\n",
      "LOSS train 0.0001998606458073482 valid 0.0011330483248457313\n",
      "LOSS train 0.0001998606458073482 valid 0.001133484998717904\n",
      "LOSS train 0.0001998606458073482 valid 0.0010566310957074165\n",
      "LOSS train 0.0001998606458073482 valid 0.0011287902016192675\n",
      "LOSS train 0.0001998606458073482 valid 0.0011834120377898216\n",
      "LOSS train 0.0001998606458073482 valid 0.001143302652053535\n",
      "LOSS train 0.0001998606458073482 valid 0.0011760657653212547\n",
      "LOSS train 0.0001998606458073482 valid 0.001237885095179081\n",
      "LOSS train 0.0001998606458073482 valid 0.0012475778348743916\n",
      "LOSS train 0.0001998606458073482 valid 0.0012351464247331023\n",
      "LOSS train 0.0001998606458073482 valid 0.0012578496243804693\n",
      "LOSS train 0.0001998606458073482 valid 0.0013480859342962503\n",
      "LOSS train 0.0001998606458073482 valid 0.0013945812825113535\n",
      "LOSS train 0.0001998606458073482 valid 0.0014010383747518063\n",
      "LOSS train 0.0001998606458073482 valid 0.0014140541898086667\n",
      "LOSS train 0.0001998606458073482 valid 0.001418732455931604\n",
      "LOSS train 0.0001998606458073482 valid 0.001414880040101707\n",
      "LOSS train 0.0001998606458073482 valid 0.0013955737231299281\n",
      "LOSS train 0.0001998606458073482 valid 0.0014081535628065467\n",
      "LOSS train 0.0001998606458073482 valid 0.0014058462111279368\n",
      "LOSS train 0.0001998606458073482 valid 0.0014162149745970964\n",
      "EPOCH 86:\n",
      "  batch 1 loss: 0.00016920265625230968\n",
      "  batch 2 loss: 9.829933696892112e-05\n",
      "  batch 3 loss: 0.0001534138573333621\n",
      "  batch 4 loss: 0.00012734088522847742\n",
      "  batch 5 loss: 0.00010414107964606956\n",
      "  batch 6 loss: 0.00010857900633709505\n",
      "  batch 7 loss: 7.727830961812288e-05\n",
      "  batch 8 loss: 9.459316788706928e-05\n",
      "  batch 9 loss: 0.00011171679943799973\n",
      "  batch 10 loss: 7.377935980912298e-05\n",
      "  batch 11 loss: 0.00012623559450730681\n",
      "  batch 12 loss: 0.00014480040408670902\n",
      "  batch 13 loss: 0.00017283507622778416\n",
      "  batch 14 loss: 0.0002580310101620853\n",
      "  batch 15 loss: 0.00016869166574906558\n",
      "  batch 16 loss: 0.00010598805965855718\n",
      "  batch 17 loss: 0.00010330147051718086\n",
      "  batch 18 loss: 0.00010487849795026705\n",
      "  batch 19 loss: 0.0002368892019148916\n",
      "  batch 20 loss: 0.00016176403732970357\n",
      "  batch 21 loss: 0.000200898532057181\n",
      "  batch 22 loss: 0.0001238305412698537\n",
      "  batch 23 loss: 0.00019645030261017382\n",
      "  batch 24 loss: 0.00018339786038268358\n",
      "  batch 25 loss: 0.00023705656349193305\n",
      "  batch 26 loss: 0.00023507933656219393\n",
      "  batch 27 loss: 9.046460036188364e-05\n",
      "  batch 28 loss: 0.00016535809845663607\n",
      "  batch 29 loss: 8.393846655962989e-05\n",
      "  batch 30 loss: 7.661330164410174e-05\n",
      "  batch 31 loss: 7.771543459966779e-05\n",
      "  batch 32 loss: 0.00014458713121712208\n",
      "  batch 33 loss: 0.0001457264879718423\n",
      "  batch 34 loss: 0.00015859397535678\n",
      "  batch 35 loss: 6.814048538217321e-05\n",
      "  batch 36 loss: 7.075632311170921e-05\n",
      "  batch 37 loss: 8.393256575800478e-05\n",
      "  batch 38 loss: 8.966264431364834e-05\n",
      "  batch 39 loss: 0.00011334267037454993\n",
      "  batch 40 loss: 0.00013262675201985985\n",
      "  batch 41 loss: 7.216879748739302e-05\n",
      "  batch 42 loss: 0.00010158250370295718\n",
      "  batch 43 loss: 7.979567453730851e-05\n",
      "  batch 44 loss: 5.555891402764246e-05\n",
      "  batch 45 loss: 6.502280302811414e-05\n",
      "  batch 46 loss: 5.807096022181213e-05\n",
      "  batch 47 loss: 8.43860034365207e-05\n",
      "  batch 48 loss: 8.792006701696664e-05\n",
      "  batch 49 loss: 0.00010809318337123841\n",
      "  batch 50 loss: 0.00012372767378110439\n",
      "  batch 51 loss: 0.00013907879474572837\n",
      "  batch 52 loss: 0.00010978952923323959\n",
      "  batch 53 loss: 0.00010566799028310925\n",
      "  batch 54 loss: 0.00010175931674893945\n",
      "  batch 55 loss: 0.00012640783097594976\n",
      "  batch 56 loss: 8.58758285176009e-05\n",
      "  batch 57 loss: 7.112010644050315e-05\n",
      "  batch 58 loss: 0.00010020178160630167\n",
      "  batch 59 loss: 9.617755131330341e-05\n",
      "  batch 60 loss: 0.00016457244055345654\n",
      "  batch 61 loss: 6.501076859422028e-05\n",
      "  batch 62 loss: 0.0001262681616935879\n",
      "  batch 63 loss: 8.032804180402309e-05\n",
      "  batch 64 loss: 7.201291737146676e-05\n",
      "  batch 65 loss: 9.294258779846132e-05\n",
      "  batch 66 loss: 8.99086007848382e-05\n",
      "  batch 67 loss: 8.205085759982467e-05\n",
      "  batch 68 loss: 9.161404159385711e-05\n",
      "  batch 69 loss: 9.634492744226009e-05\n",
      "  batch 70 loss: 0.0001525472616776824\n",
      "  batch 71 loss: 9.757093357620761e-05\n",
      "  batch 72 loss: 8.334354788530618e-05\n",
      "  batch 73 loss: 8.616453851573169e-05\n",
      "  batch 74 loss: 0.0001110051671275869\n",
      "  batch 75 loss: 0.00015309039736166596\n",
      "  batch 76 loss: 9.928560757543892e-05\n",
      "  batch 77 loss: 0.00011375611939001828\n",
      "  batch 78 loss: 0.00011802140943473205\n",
      "  batch 79 loss: 9.705583215691149e-05\n",
      "  batch 80 loss: 0.00012279230577405542\n",
      "  batch 81 loss: 0.00012196054740343243\n",
      "  batch 82 loss: 0.0001246094034286216\n",
      "  batch 83 loss: 0.00012379280815366656\n",
      "  batch 84 loss: 8.94143886398524e-05\n",
      "  batch 85 loss: 0.00010432797716930509\n",
      "  batch 86 loss: 0.00013178230437915772\n",
      "  batch 87 loss: 0.00013240514090284705\n",
      "  batch 88 loss: 0.0002206354110967368\n",
      "  batch 89 loss: 0.00013399632007349283\n",
      "  batch 90 loss: 0.0001842551864683628\n",
      "  batch 91 loss: 0.00011497960076667368\n",
      "  batch 92 loss: 0.00012212088040541857\n",
      "  batch 93 loss: 0.00012999364116694778\n",
      "  batch 94 loss: 0.0001340505841653794\n",
      "  batch 95 loss: 0.00021028694754932076\n",
      "LOSS train 0.00021028694754932076 valid 0.0013390583917498589\n",
      "LOSS train 0.00021028694754932076 valid 0.001450341776944697\n",
      "LOSS train 0.00021028694754932076 valid 0.0012609328841790557\n",
      "LOSS train 0.00021028694754932076 valid 0.0012064981274306774\n",
      "LOSS train 0.00021028694754932076 valid 0.0012058174470439553\n",
      "LOSS train 0.00021028694754932076 valid 0.0011426827404648066\n",
      "LOSS train 0.00021028694754932076 valid 0.001218708697706461\n",
      "LOSS train 0.00021028694754932076 valid 0.0012616507010534406\n",
      "LOSS train 0.00021028694754932076 valid 0.0012190229026600718\n",
      "LOSS train 0.00021028694754932076 valid 0.0012554081622511148\n",
      "LOSS train 0.00021028694754932076 valid 0.0013424679636955261\n",
      "LOSS train 0.00021028694754932076 valid 0.0013573787873610854\n",
      "LOSS train 0.00021028694754932076 valid 0.001338681671768427\n",
      "LOSS train 0.00021028694754932076 valid 0.001364985597319901\n",
      "LOSS train 0.00021028694754932076 valid 0.0014723252970725298\n",
      "LOSS train 0.00021028694754932076 valid 0.0015174022410064936\n",
      "LOSS train 0.00021028694754932076 valid 0.0015180202899500728\n",
      "LOSS train 0.00021028694754932076 valid 0.0015366130974143744\n",
      "LOSS train 0.00021028694754932076 valid 0.0015479113208130002\n",
      "LOSS train 0.00021028694754932076 valid 0.0015450076898559928\n",
      "LOSS train 0.00021028694754932076 valid 0.0015253511955961585\n",
      "LOSS train 0.00021028694754932076 valid 0.0015365059953182936\n",
      "LOSS train 0.00021028694754932076 valid 0.0015375390648841858\n",
      "LOSS train 0.00021028694754932076 valid 0.0015568840317428112\n",
      "EPOCH 87:\n",
      "  batch 1 loss: 0.00017900188686326146\n",
      "  batch 2 loss: 0.00010767846833914518\n",
      "  batch 3 loss: 0.00016019627219066024\n",
      "  batch 4 loss: 0.00014214692055247724\n",
      "  batch 5 loss: 0.00011210078082513064\n",
      "  batch 6 loss: 0.00011720523616531864\n",
      "  batch 7 loss: 7.970583828864619e-05\n",
      "  batch 8 loss: 9.612430585548282e-05\n",
      "  batch 9 loss: 0.00010544493852648884\n",
      "  batch 10 loss: 7.128518336685374e-05\n",
      "  batch 11 loss: 0.0001505874388385564\n",
      "  batch 12 loss: 0.0001666551106609404\n",
      "  batch 13 loss: 0.0001851843553595245\n",
      "  batch 14 loss: 0.00027849694015458226\n",
      "  batch 15 loss: 0.00017753281281329691\n",
      "  batch 16 loss: 0.0001144034176832065\n",
      "  batch 17 loss: 9.85471997410059e-05\n",
      "  batch 18 loss: 0.00010726821346906945\n",
      "  batch 19 loss: 0.00022557436022907495\n",
      "  batch 20 loss: 0.0001699534768704325\n",
      "  batch 21 loss: 0.0002116614778060466\n",
      "  batch 22 loss: 0.0001378092565573752\n",
      "  batch 23 loss: 0.00023702142061665654\n",
      "  batch 24 loss: 0.0001829699904192239\n",
      "  batch 25 loss: 0.00024077057605609298\n",
      "  batch 26 loss: 0.0002314718731213361\n",
      "  batch 27 loss: 9.236519690603018e-05\n",
      "  batch 28 loss: 0.00014545966405421495\n",
      "  batch 29 loss: 8.91542003955692e-05\n",
      "  batch 30 loss: 8.317039464600384e-05\n",
      "  batch 31 loss: 7.194180216174573e-05\n",
      "  batch 32 loss: 0.0001321151212323457\n",
      "  batch 33 loss: 0.0001282219891436398\n",
      "  batch 34 loss: 0.0001539770746603608\n",
      "  batch 35 loss: 7.995962369022891e-05\n",
      "  batch 36 loss: 7.78541507315822e-05\n",
      "  batch 37 loss: 9.961570322047919e-05\n",
      "  batch 38 loss: 9.336633956991136e-05\n",
      "  batch 39 loss: 0.0001236550451721996\n",
      "  batch 40 loss: 0.0001416581799276173\n",
      "  batch 41 loss: 8.201408491004258e-05\n",
      "  batch 42 loss: 0.000101970843388699\n",
      "  batch 43 loss: 9.041406156029552e-05\n",
      "  batch 44 loss: 6.745012069586664e-05\n",
      "  batch 45 loss: 8.06028037914075e-05\n",
      "  batch 46 loss: 7.214948709588498e-05\n",
      "  batch 47 loss: 8.876274659996852e-05\n",
      "  batch 48 loss: 9.259417856810614e-05\n",
      "  batch 49 loss: 0.00010492657747818157\n",
      "  batch 50 loss: 0.0001253976224688813\n",
      "  batch 51 loss: 0.0001735470723360777\n",
      "  batch 52 loss: 0.00011269445531070232\n",
      "  batch 53 loss: 0.00011590644135139883\n",
      "  batch 54 loss: 0.00010960493818856776\n",
      "  batch 55 loss: 0.0001345306955045089\n",
      "  batch 56 loss: 9.472994861425832e-05\n",
      "  batch 57 loss: 7.720708526903763e-05\n",
      "  batch 58 loss: 8.731199341127649e-05\n",
      "  batch 59 loss: 9.06176064745523e-05\n",
      "  batch 60 loss: 0.00015034707030281425\n",
      "  batch 61 loss: 6.600480992347002e-05\n",
      "  batch 62 loss: 0.0001505285908933729\n",
      "  batch 63 loss: 9.900039003696293e-05\n",
      "  batch 64 loss: 8.26823670649901e-05\n",
      "  batch 65 loss: 7.602563709951937e-05\n",
      "  batch 66 loss: 7.835020369384438e-05\n",
      "  batch 67 loss: 7.748133793938905e-05\n",
      "  batch 68 loss: 8.492570486851037e-05\n",
      "  batch 69 loss: 0.00010345054761273786\n",
      "  batch 70 loss: 0.00016697935643605888\n",
      "  batch 71 loss: 0.00011177718988619745\n",
      "  batch 72 loss: 9.952179243555292e-05\n",
      "  batch 73 loss: 9.823101572692394e-05\n",
      "  batch 74 loss: 0.0001404154027113691\n",
      "  batch 75 loss: 0.00018693969468586147\n",
      "  batch 76 loss: 0.00011092251224908978\n",
      "  batch 77 loss: 0.00010345491318730637\n",
      "  batch 78 loss: 0.00010748137719929218\n",
      "  batch 79 loss: 9.530887473374605e-05\n",
      "  batch 80 loss: 0.00013236809172667563\n",
      "  batch 81 loss: 0.00014577062393072993\n",
      "  batch 82 loss: 0.00014523824211210012\n",
      "  batch 83 loss: 0.00013273376680444926\n",
      "  batch 84 loss: 9.782440611161292e-05\n",
      "  batch 85 loss: 0.00010828187805600464\n",
      "  batch 86 loss: 0.00012179187615402043\n",
      "  batch 87 loss: 0.00012578140012919903\n",
      "  batch 88 loss: 0.00019599887309595942\n",
      "  batch 89 loss: 0.00013796745042782277\n",
      "  batch 90 loss: 0.00020029355073347688\n",
      "  batch 91 loss: 0.00012986015644855797\n",
      "  batch 92 loss: 0.00014264252968132496\n",
      "  batch 93 loss: 0.0001426326052751392\n",
      "  batch 94 loss: 0.00013878392928745598\n",
      "  batch 95 loss: 0.0002054534270428121\n",
      "LOSS train 0.0002054534270428121 valid 0.0012533828848972917\n",
      "LOSS train 0.0002054534270428121 valid 0.0013786142226308584\n",
      "LOSS train 0.0002054534270428121 valid 0.001231066882610321\n",
      "LOSS train 0.0002054534270428121 valid 0.0011893135961145163\n",
      "LOSS train 0.0002054534270428121 valid 0.0011780274799093604\n",
      "LOSS train 0.0002054534270428121 valid 0.0011141125578433275\n",
      "LOSS train 0.0002054534270428121 valid 0.0011568873887881637\n",
      "LOSS train 0.0002054534270428121 valid 0.001202368875965476\n",
      "LOSS train 0.0002054534270428121 valid 0.0011688658269122243\n",
      "LOSS train 0.0002054534270428121 valid 0.001196841592900455\n",
      "LOSS train 0.0002054534270428121 valid 0.0012648619012907147\n",
      "LOSS train 0.0002054534270428121 valid 0.0012699587969109416\n",
      "LOSS train 0.0002054534270428121 valid 0.001265169121325016\n",
      "LOSS train 0.0002054534270428121 valid 0.0012898952700197697\n",
      "LOSS train 0.0002054534270428121 valid 0.0013867449015378952\n",
      "LOSS train 0.0002054534270428121 valid 0.001431760611012578\n",
      "LOSS train 0.0002054534270428121 valid 0.0014356820611283183\n",
      "LOSS train 0.0002054534270428121 valid 0.0014486381551250815\n",
      "LOSS train 0.0002054534270428121 valid 0.0014505910221487284\n",
      "LOSS train 0.0002054534270428121 valid 0.0014407484559342265\n",
      "LOSS train 0.0002054534270428121 valid 0.0014209577348083258\n",
      "LOSS train 0.0002054534270428121 valid 0.0014326367527246475\n",
      "LOSS train 0.0002054534270428121 valid 0.0014295821310952306\n",
      "LOSS train 0.0002054534270428121 valid 0.0014362586662173271\n",
      "EPOCH 88:\n",
      "  batch 1 loss: 0.00017369577835779637\n",
      "  batch 2 loss: 0.00010786350321723148\n",
      "  batch 3 loss: 0.0001647075405344367\n",
      "  batch 4 loss: 0.0001560472883284092\n",
      "  batch 5 loss: 0.00012620104826055467\n",
      "  batch 6 loss: 0.00013579953520093113\n",
      "  batch 7 loss: 9.381509880768135e-05\n",
      "  batch 8 loss: 9.821107232710347e-05\n",
      "  batch 9 loss: 0.00010728603228926659\n",
      "  batch 10 loss: 7.803897460689768e-05\n",
      "  batch 11 loss: 0.00012646600953303277\n",
      "  batch 12 loss: 0.000148657156387344\n",
      "  batch 13 loss: 0.00018521066522225738\n",
      "  batch 14 loss: 0.0003103835042566061\n",
      "  batch 15 loss: 0.00018317073408979923\n",
      "  batch 16 loss: 0.00012075062113581225\n",
      "  batch 17 loss: 0.0001079145004041493\n",
      "  batch 18 loss: 0.0001185899309348315\n",
      "  batch 19 loss: 0.00022410787642002106\n",
      "  batch 20 loss: 0.00017550535267218947\n",
      "  batch 21 loss: 0.00021709976135753095\n",
      "  batch 22 loss: 0.00014133137301541865\n",
      "  batch 23 loss: 0.00024254746676888317\n",
      "  batch 24 loss: 0.00020048489386681467\n",
      "  batch 25 loss: 0.00024236948229372501\n",
      "  batch 26 loss: 0.00024081027368083596\n",
      "  batch 27 loss: 0.00011057751544285566\n",
      "  batch 28 loss: 0.0001566530263517052\n",
      "  batch 29 loss: 0.00010453059076098725\n",
      "  batch 30 loss: 9.148468961939216e-05\n",
      "  batch 31 loss: 7.720749999862164e-05\n",
      "  batch 32 loss: 0.00012861299910582602\n",
      "  batch 33 loss: 0.00013171475438866764\n",
      "  batch 34 loss: 0.00016723285079933703\n",
      "  batch 35 loss: 7.959317008499056e-05\n",
      "  batch 36 loss: 8.863091352395713e-05\n",
      "  batch 37 loss: 0.00010761502926470712\n",
      "  batch 38 loss: 9.848001354839653e-05\n",
      "  batch 39 loss: 0.0001253055816050619\n",
      "  batch 40 loss: 0.00013958994532004\n",
      "  batch 41 loss: 7.751841621939093e-05\n",
      "  batch 42 loss: 0.00010767903586383909\n",
      "  batch 43 loss: 8.680725295562297e-05\n",
      "  batch 44 loss: 6.378462421707809e-05\n",
      "  batch 45 loss: 7.762468158034608e-05\n",
      "  batch 46 loss: 7.215723599074408e-05\n",
      "  batch 47 loss: 0.00010566510900389403\n",
      "  batch 48 loss: 0.00010210819891653955\n",
      "  batch 49 loss: 0.00010861080954782665\n",
      "  batch 50 loss: 0.0001266578765353188\n",
      "  batch 51 loss: 0.00013195897918194532\n",
      "  batch 52 loss: 0.00010766911873361096\n",
      "  batch 53 loss: 0.00012205004168208688\n",
      "  batch 54 loss: 0.0001294187968596816\n",
      "  batch 55 loss: 0.00015279831131920218\n",
      "  batch 56 loss: 9.325728024123237e-05\n",
      "  batch 57 loss: 7.935451867524534e-05\n",
      "  batch 58 loss: 0.00010345582268200815\n",
      "  batch 59 loss: 9.460640285396948e-05\n",
      "  batch 60 loss: 0.00013991068408358842\n",
      "  batch 61 loss: 6.119245517766103e-05\n",
      "  batch 62 loss: 0.00015721636009402573\n",
      "  batch 63 loss: 0.00010507767728995532\n",
      "  batch 64 loss: 9.988014062400907e-05\n",
      "  batch 65 loss: 8.806244295556098e-05\n",
      "  batch 66 loss: 8.450114546576515e-05\n",
      "  batch 67 loss: 7.957422349136323e-05\n",
      "  batch 68 loss: 8.895307837519795e-05\n",
      "  batch 69 loss: 9.099237649934366e-05\n",
      "  batch 70 loss: 0.00014857188216410577\n",
      "  batch 71 loss: 9.276428318116814e-05\n",
      "  batch 72 loss: 9.560122271068394e-05\n",
      "  batch 73 loss: 0.00010419637692393735\n",
      "  batch 74 loss: 0.00013341123121790588\n",
      "  batch 75 loss: 0.0001903826487250626\n",
      "  batch 76 loss: 0.00012592808343470097\n",
      "  batch 77 loss: 0.00011253168486291543\n",
      "  batch 78 loss: 0.00010856785957003012\n",
      "  batch 79 loss: 8.777307812124491e-05\n",
      "  batch 80 loss: 0.00011715115397237241\n",
      "  batch 81 loss: 0.00013805419439449906\n",
      "  batch 82 loss: 0.00013280757411848754\n",
      "  batch 83 loss: 0.00013664003927260637\n",
      "  batch 84 loss: 0.00010223538265563548\n",
      "  batch 85 loss: 0.0001201251070597209\n",
      "  batch 86 loss: 0.00012579650501720607\n",
      "  batch 87 loss: 0.00011667108628898859\n",
      "  batch 88 loss: 0.00017870753072202206\n",
      "  batch 89 loss: 0.00013014012074563652\n",
      "  batch 90 loss: 0.00018136005382984877\n",
      "  batch 91 loss: 0.00012408191105350852\n",
      "  batch 92 loss: 0.0001426360395271331\n",
      "  batch 93 loss: 0.00013567379210144281\n",
      "  batch 94 loss: 0.00013796055282000452\n",
      "  batch 95 loss: 0.00021478903363458812\n",
      "LOSS train 0.00021478903363458812 valid 0.0011089758481830359\n",
      "LOSS train 0.00021478903363458812 valid 0.001300709554925561\n",
      "LOSS train 0.00021478903363458812 valid 0.001148503040894866\n",
      "LOSS train 0.00021478903363458812 valid 0.0010942728258669376\n",
      "LOSS train 0.00021478903363458812 valid 0.0010865334188565612\n",
      "LOSS train 0.00021478903363458812 valid 0.0010146398562937975\n",
      "LOSS train 0.00021478903363458812 valid 0.0010727099142968655\n",
      "LOSS train 0.00021478903363458812 valid 0.0011270706309005618\n",
      "LOSS train 0.00021478903363458812 valid 0.0010826247744262218\n",
      "LOSS train 0.00021478903363458812 valid 0.0011024711420759559\n",
      "LOSS train 0.00021478903363458812 valid 0.0011637195711955428\n",
      "LOSS train 0.00021478903363458812 valid 0.0011666627833619714\n",
      "LOSS train 0.00021478903363458812 valid 0.0011484160786494613\n",
      "LOSS train 0.00021478903363458812 valid 0.0011787504190579057\n",
      "LOSS train 0.00021478903363458812 valid 0.0012740520760416985\n",
      "LOSS train 0.00021478903363458812 valid 0.00132364418823272\n",
      "LOSS train 0.00021478903363458812 valid 0.0013256411766633391\n",
      "LOSS train 0.00021478903363458812 valid 0.001349512254819274\n",
      "LOSS train 0.00021478903363458812 valid 0.0013615139760077\n",
      "LOSS train 0.00021478903363458812 valid 0.0013522120425477624\n",
      "LOSS train 0.00021478903363458812 valid 0.0013307107146829367\n",
      "LOSS train 0.00021478903363458812 valid 0.0013522832887247205\n",
      "LOSS train 0.00021478903363458812 valid 0.0013478432083502412\n",
      "LOSS train 0.00021478903363458812 valid 0.0013536380138248205\n",
      "EPOCH 89:\n",
      "  batch 1 loss: 0.00016473988944198936\n",
      "  batch 2 loss: 0.00010888557881116867\n",
      "  batch 3 loss: 0.00017126990132965147\n",
      "  batch 4 loss: 0.00014138201368041337\n",
      "  batch 5 loss: 0.0001195697914226912\n",
      "  batch 6 loss: 0.00013072596630081534\n",
      "  batch 7 loss: 9.930973465088755e-05\n",
      "  batch 8 loss: 0.00010593237675493583\n",
      "  batch 9 loss: 0.0001211840208270587\n",
      "  batch 10 loss: 9.497511200606823e-05\n",
      "  batch 11 loss: 0.00013588239380624145\n",
      "  batch 12 loss: 0.00014809289132244885\n",
      "  batch 13 loss: 0.0001850808912422508\n",
      "  batch 14 loss: 0.0003107329539489001\n",
      "  batch 15 loss: 0.000173047068528831\n",
      "  batch 16 loss: 0.00010865709919016808\n",
      "  batch 17 loss: 0.00013895949814468622\n",
      "  batch 18 loss: 0.00012517711729742587\n",
      "  batch 19 loss: 0.00027638135361485183\n",
      "  batch 20 loss: 0.00021155639842618257\n",
      "  batch 21 loss: 0.00022649692255072296\n",
      "  batch 22 loss: 0.00013762970047537237\n",
      "  batch 23 loss: 0.00021426132298074663\n",
      "  batch 24 loss: 0.00019811584206763655\n",
      "  batch 25 loss: 0.0002360146027058363\n",
      "  batch 26 loss: 0.0002426547434879467\n",
      "  batch 27 loss: 0.00010928975825663656\n",
      "  batch 28 loss: 0.00015035631076898426\n",
      "  batch 29 loss: 0.00010084673704113811\n",
      "  batch 30 loss: 8.343806985067204e-05\n",
      "  batch 31 loss: 7.557158824056387e-05\n",
      "  batch 32 loss: 0.00013243478315416723\n",
      "  batch 33 loss: 0.00013181104441173375\n",
      "  batch 34 loss: 0.0001522317179478705\n",
      "  batch 35 loss: 7.279482088051736e-05\n",
      "  batch 36 loss: 7.454057049471885e-05\n",
      "  batch 37 loss: 9.050486551132053e-05\n",
      "  batch 38 loss: 9.438858251087368e-05\n",
      "  batch 39 loss: 0.00012233556481078267\n",
      "  batch 40 loss: 0.00014714724966324866\n",
      "  batch 41 loss: 8.492499182466418e-05\n",
      "  batch 42 loss: 0.00010230518091702834\n",
      "  batch 43 loss: 8.309647819260135e-05\n",
      "  batch 44 loss: 5.427434007287957e-05\n",
      "  batch 45 loss: 7.268956687767059e-05\n",
      "  batch 46 loss: 6.236587068997324e-05\n",
      "  batch 47 loss: 9.996019070968032e-05\n",
      "  batch 48 loss: 0.00010027130338130519\n",
      "  batch 49 loss: 0.00010668130562407896\n",
      "  batch 50 loss: 0.00012132249685237184\n",
      "  batch 51 loss: 0.000120820157462731\n",
      "  batch 52 loss: 0.00011062294652219862\n",
      "  batch 53 loss: 0.00012413115473464131\n",
      "  batch 54 loss: 0.00011172637459821999\n",
      "  batch 55 loss: 0.00013586902059614658\n",
      "  batch 56 loss: 8.202823664760217e-05\n",
      "  batch 57 loss: 8.66422924445942e-05\n",
      "  batch 58 loss: 0.00010344591282773763\n",
      "  batch 59 loss: 9.90579865174368e-05\n",
      "  batch 60 loss: 0.00018600770272314548\n",
      "  batch 61 loss: 5.649897502735257e-05\n",
      "  batch 62 loss: 0.00015810241166036576\n",
      "  batch 63 loss: 8.990056812763214e-05\n",
      "  batch 64 loss: 7.992457540240139e-05\n",
      "  batch 65 loss: 8.989057096187025e-05\n",
      "  batch 66 loss: 7.675599044887349e-05\n",
      "  batch 67 loss: 6.82883255649358e-05\n",
      "  batch 68 loss: 8.185977640096098e-05\n",
      "  batch 69 loss: 8.843260002322495e-05\n",
      "  batch 70 loss: 0.0001534024631837383\n",
      "  batch 71 loss: 0.00010116468911292031\n",
      "  batch 72 loss: 9.948982915375382e-05\n",
      "  batch 73 loss: 9.751821198733523e-05\n",
      "  batch 74 loss: 0.00011604681640164927\n",
      "  batch 75 loss: 0.00016704469453543425\n",
      "  batch 76 loss: 0.00010365185153204948\n",
      "  batch 77 loss: 0.00010568953439360484\n",
      "  batch 78 loss: 9.805323497857898e-05\n",
      "  batch 79 loss: 8.746436651563272e-05\n",
      "  batch 80 loss: 0.00010349490912631154\n",
      "  batch 81 loss: 0.00011988574988208711\n",
      "  batch 82 loss: 0.00011782132787629962\n",
      "  batch 83 loss: 0.0001277621486224234\n",
      "  batch 84 loss: 9.110270184464753e-05\n",
      "  batch 85 loss: 0.00010599508823361248\n",
      "  batch 86 loss: 0.00012032744416501373\n",
      "  batch 87 loss: 0.00010925301467068493\n",
      "  batch 88 loss: 0.00017714389832690358\n",
      "  batch 89 loss: 0.00012625104864127934\n",
      "  batch 90 loss: 0.0001695376558927819\n",
      "  batch 91 loss: 0.00011116703535662964\n",
      "  batch 92 loss: 0.00012834405060857534\n",
      "  batch 93 loss: 0.00012076004350092262\n",
      "  batch 94 loss: 0.00012023147428408265\n",
      "  batch 95 loss: 0.00018250699213240296\n",
      "LOSS train 0.00018250699213240296 valid 0.0010645778384059668\n",
      "LOSS train 0.00018250699213240296 valid 0.0012545714853331447\n",
      "LOSS train 0.00018250699213240296 valid 0.0011267161462455988\n",
      "LOSS train 0.00018250699213240296 valid 0.0010749998036772013\n",
      "LOSS train 0.00018250699213240296 valid 0.001077348249964416\n",
      "LOSS train 0.00018250699213240296 valid 0.0009957549627870321\n",
      "LOSS train 0.00018250699213240296 valid 0.0010647326707839966\n",
      "LOSS train 0.00018250699213240296 valid 0.00111433956772089\n",
      "LOSS train 0.00018250699213240296 valid 0.0010761348530650139\n",
      "LOSS train 0.00018250699213240296 valid 0.0011026732390746474\n",
      "LOSS train 0.00018250699213240296 valid 0.0011610375950112939\n",
      "LOSS train 0.00018250699213240296 valid 0.0011660302989184856\n",
      "LOSS train 0.00018250699213240296 valid 0.001159505918622017\n",
      "LOSS train 0.00018250699213240296 valid 0.0011853601317852736\n",
      "LOSS train 0.00018250699213240296 valid 0.001272842288017273\n",
      "LOSS train 0.00018250699213240296 valid 0.0013138337526470423\n",
      "LOSS train 0.00018250699213240296 valid 0.0013177581131458282\n",
      "LOSS train 0.00018250699213240296 valid 0.0013408609665930271\n",
      "LOSS train 0.00018250699213240296 valid 0.0013515283353626728\n",
      "LOSS train 0.00018250699213240296 valid 0.0013492332072928548\n",
      "LOSS train 0.00018250699213240296 valid 0.001328497426584363\n",
      "LOSS train 0.00018250699213240296 valid 0.0013449355028569698\n",
      "LOSS train 0.00018250699213240296 valid 0.0013400395400822163\n",
      "LOSS train 0.00018250699213240296 valid 0.0013534696772694588\n",
      "EPOCH 90:\n",
      "  batch 1 loss: 0.00016208393208216876\n",
      "  batch 2 loss: 0.00010253117943648249\n",
      "  batch 3 loss: 0.00015557146980427206\n",
      "  batch 4 loss: 0.00013509709970094264\n",
      "  batch 5 loss: 0.00010968466085614637\n",
      "  batch 6 loss: 0.00011038954835385084\n",
      "  batch 7 loss: 8.37021361803636e-05\n",
      "  batch 8 loss: 9.253386815544218e-05\n",
      "  batch 9 loss: 0.0001053780724760145\n",
      "  batch 10 loss: 6.499100709334016e-05\n",
      "  batch 11 loss: 0.00011340851779095829\n",
      "  batch 12 loss: 0.00013316591503098607\n",
      "  batch 13 loss: 0.0001741638989187777\n",
      "  batch 14 loss: 0.00025721057318150997\n",
      "  batch 15 loss: 0.00017290556570515037\n",
      "  batch 16 loss: 0.00010617551015457138\n",
      "  batch 17 loss: 9.306701394962147e-05\n",
      "  batch 18 loss: 0.00010657998063834384\n",
      "  batch 19 loss: 0.00021492730593308806\n",
      "  batch 20 loss: 0.00014680380991194397\n",
      "  batch 21 loss: 0.0001985724811675027\n",
      "  batch 22 loss: 0.0001406148076057434\n",
      "  batch 23 loss: 0.0002179386210627854\n",
      "  batch 24 loss: 0.00018716868362389505\n",
      "  batch 25 loss: 0.00023109505127649754\n",
      "  batch 26 loss: 0.00021550420206040144\n",
      "  batch 27 loss: 8.319511107401922e-05\n",
      "  batch 28 loss: 0.00013180852693039924\n",
      "  batch 29 loss: 8.181965677067637e-05\n",
      "  batch 30 loss: 7.270889182109386e-05\n",
      "  batch 31 loss: 6.970821414142847e-05\n",
      "  batch 32 loss: 0.00012229855929035693\n",
      "  batch 33 loss: 0.00011922305566258729\n",
      "  batch 34 loss: 0.000143747020047158\n",
      "  batch 35 loss: 6.877671694383025e-05\n",
      "  batch 36 loss: 6.478462455561385e-05\n",
      "  batch 37 loss: 8.300357148982584e-05\n",
      "  batch 38 loss: 8.569755300413817e-05\n",
      "  batch 39 loss: 0.00010956076584989205\n",
      "  batch 40 loss: 0.0001198629688587971\n",
      "  batch 41 loss: 6.979799218242988e-05\n",
      "  batch 42 loss: 9.750873869052157e-05\n",
      "  batch 43 loss: 7.18748924555257e-05\n",
      "  batch 44 loss: 4.800413444172591e-05\n",
      "  batch 45 loss: 6.119825411587954e-05\n",
      "  batch 46 loss: 5.614792462438345e-05\n",
      "  batch 47 loss: 8.859574154485017e-05\n",
      "  batch 48 loss: 8.571111538913101e-05\n",
      "  batch 49 loss: 9.489723743172362e-05\n",
      "  batch 50 loss: 0.00011050868488382548\n",
      "  batch 51 loss: 0.00011559089762158692\n",
      "  batch 52 loss: 9.563226194586605e-05\n",
      "  batch 53 loss: 9.871541988104582e-05\n",
      "  batch 54 loss: 9.947751823347062e-05\n",
      "  batch 55 loss: 0.00012370242620818317\n",
      "  batch 56 loss: 7.814507989678532e-05\n",
      "  batch 57 loss: 6.607519026147202e-05\n",
      "  batch 58 loss: 8.516103844158351e-05\n",
      "  batch 59 loss: 7.777731661917642e-05\n",
      "  batch 60 loss: 0.00014127967006061226\n",
      "  batch 61 loss: 5.746351962443441e-05\n",
      "  batch 62 loss: 0.00011915171489818022\n",
      "  batch 63 loss: 8.338042243849486e-05\n",
      "  batch 64 loss: 7.525325781898573e-05\n",
      "  batch 65 loss: 9.657070768298581e-05\n",
      "  batch 66 loss: 7.890332926763222e-05\n",
      "  batch 67 loss: 7.189717871369794e-05\n",
      "  batch 68 loss: 8.575367246521637e-05\n",
      "  batch 69 loss: 8.081781561486423e-05\n",
      "  batch 70 loss: 0.0001338350703008473\n",
      "  batch 71 loss: 8.74680990818888e-05\n",
      "  batch 72 loss: 8.107447501970455e-05\n",
      "  batch 73 loss: 9.078892617253587e-05\n",
      "  batch 74 loss: 0.000107973552076146\n",
      "  batch 75 loss: 0.00013892431161366403\n",
      "  batch 76 loss: 8.805107790976763e-05\n",
      "  batch 77 loss: 0.00010257969552185386\n",
      "  batch 78 loss: 9.612385474611074e-05\n",
      "  batch 79 loss: 7.506772817578167e-05\n",
      "  batch 80 loss: 0.0001010695705190301\n",
      "  batch 81 loss: 0.00011533422366483137\n",
      "  batch 82 loss: 0.0001167700975202024\n",
      "  batch 83 loss: 0.00012508657528087497\n",
      "  batch 84 loss: 9.114088607020676e-05\n",
      "  batch 85 loss: 9.782483539311215e-05\n",
      "  batch 86 loss: 0.00011229945084778592\n",
      "  batch 87 loss: 0.00010256501263938844\n",
      "  batch 88 loss: 0.00017645920161157846\n",
      "  batch 89 loss: 0.00011303904466331005\n",
      "  batch 90 loss: 0.00015702657401561737\n",
      "  batch 91 loss: 9.992518607759848e-05\n",
      "  batch 92 loss: 0.0001184871289297007\n",
      "  batch 93 loss: 0.00012141025945311412\n",
      "  batch 94 loss: 0.00012159722245996818\n",
      "  batch 95 loss: 0.00019171855819877237\n",
      "LOSS train 0.00019171855819877237 valid 0.0012204714585095644\n",
      "LOSS train 0.00019171855819877237 valid 0.0013983557000756264\n",
      "LOSS train 0.00019171855819877237 valid 0.001229610643349588\n",
      "LOSS train 0.00019171855819877237 valid 0.0011768060503527522\n",
      "LOSS train 0.00019171855819877237 valid 0.0011905457358807325\n",
      "LOSS train 0.00019171855819877237 valid 0.001107513322494924\n",
      "LOSS train 0.00019171855819877237 valid 0.0011735304724425077\n",
      "LOSS train 0.00019171855819877237 valid 0.0012314682826399803\n",
      "LOSS train 0.00019171855819877237 valid 0.0011893686605617404\n",
      "LOSS train 0.00019171855819877237 valid 0.0012195141753181815\n",
      "LOSS train 0.00019171855819877237 valid 0.0013028956018388271\n",
      "LOSS train 0.00019171855819877237 valid 0.0013088423293083906\n",
      "LOSS train 0.00019171855819877237 valid 0.0013003842905163765\n",
      "LOSS train 0.00019171855819877237 valid 0.0013326004846021533\n",
      "LOSS train 0.00019171855819877237 valid 0.0014452285831794143\n",
      "LOSS train 0.00019171855819877237 valid 0.0014955427031964064\n",
      "LOSS train 0.00019171855819877237 valid 0.0014968127943575382\n",
      "LOSS train 0.00019171855819877237 valid 0.0015226660761982203\n",
      "LOSS train 0.00019171855819877237 valid 0.0015332329785451293\n",
      "LOSS train 0.00019171855819877237 valid 0.001537181087769568\n",
      "LOSS train 0.00019171855819877237 valid 0.001514166477136314\n",
      "LOSS train 0.00019171855819877237 valid 0.0015320507809519768\n",
      "LOSS train 0.00019171855819877237 valid 0.001526147359982133\n",
      "LOSS train 0.00019171855819877237 valid 0.0015479850117117167\n",
      "EPOCH 91:\n",
      "  batch 1 loss: 0.00017088654567487538\n",
      "  batch 2 loss: 8.851179882185534e-05\n",
      "  batch 3 loss: 0.00014625093899667263\n",
      "  batch 4 loss: 0.0001249463384738192\n",
      "  batch 5 loss: 0.00010066263348562643\n",
      "  batch 6 loss: 0.00010329319775337353\n",
      "  batch 7 loss: 7.751159864710644e-05\n",
      "  batch 8 loss: 9.110764949582517e-05\n",
      "  batch 9 loss: 0.00012027857883367687\n",
      "  batch 10 loss: 6.96258939569816e-05\n",
      "  batch 11 loss: 0.0001284140016650781\n",
      "  batch 12 loss: 0.0001432367425877601\n",
      "  batch 13 loss: 0.00016802881145849824\n",
      "  batch 14 loss: 0.0002496548695489764\n",
      "  batch 15 loss: 0.0001603116252226755\n",
      "  batch 16 loss: 0.00010259896225761622\n",
      "  batch 17 loss: 9.434089588467032e-05\n",
      "  batch 18 loss: 9.925648191710934e-05\n",
      "  batch 19 loss: 0.0002024620189331472\n",
      "  batch 20 loss: 0.00014419977378565818\n",
      "  batch 21 loss: 0.00018894646200351417\n",
      "  batch 22 loss: 0.0001135256898123771\n",
      "  batch 23 loss: 0.0001786545035429299\n",
      "  batch 24 loss: 0.0001661865971982479\n",
      "  batch 25 loss: 0.0002110133646056056\n",
      "  batch 26 loss: 0.00020818959455937147\n",
      "  batch 27 loss: 8.129417983582243e-05\n",
      "  batch 28 loss: 0.00012306636199355125\n",
      "  batch 29 loss: 7.789135270286351e-05\n",
      "  batch 30 loss: 6.889548967592418e-05\n",
      "  batch 31 loss: 6.222803494893014e-05\n",
      "  batch 32 loss: 0.0001106424315366894\n",
      "  batch 33 loss: 0.0001080500878742896\n",
      "  batch 34 loss: 0.00013704229786526412\n",
      "  batch 35 loss: 6.70970039209351e-05\n",
      "  batch 36 loss: 5.527225221158005e-05\n",
      "  batch 37 loss: 8.45661197672598e-05\n",
      "  batch 38 loss: 8.47027258714661e-05\n",
      "  batch 39 loss: 0.0001038648551912047\n",
      "  batch 40 loss: 0.0001210925547638908\n",
      "  batch 41 loss: 7.481841021217406e-05\n",
      "  batch 42 loss: 9.478038555243984e-05\n",
      "  batch 43 loss: 7.211407501017675e-05\n",
      "  batch 44 loss: 5.028393934480846e-05\n",
      "  batch 45 loss: 5.8158715546596795e-05\n",
      "  batch 46 loss: 5.332432556315325e-05\n",
      "  batch 47 loss: 7.663360884180292e-05\n",
      "  batch 48 loss: 7.770245429128408e-05\n",
      "  batch 49 loss: 8.789436833467335e-05\n",
      "  batch 50 loss: 0.00010790821397677064\n",
      "  batch 51 loss: 0.00010372878750786185\n",
      "  batch 52 loss: 8.615699334768578e-05\n",
      "  batch 53 loss: 8.610803342889994e-05\n",
      "  batch 54 loss: 8.736614836379886e-05\n",
      "  batch 55 loss: 0.00011384530807845294\n",
      "  batch 56 loss: 7.196694787126034e-05\n",
      "  batch 57 loss: 6.593059515580535e-05\n",
      "  batch 58 loss: 8.262526534963399e-05\n",
      "  batch 59 loss: 7.487664697691798e-05\n",
      "  batch 60 loss: 0.00011797702609328553\n",
      "  batch 61 loss: 5.023477933718823e-05\n",
      "  batch 62 loss: 0.00010548721911618486\n",
      "  batch 63 loss: 7.293846283573657e-05\n",
      "  batch 64 loss: 6.292136094998568e-05\n",
      "  batch 65 loss: 6.67856729705818e-05\n",
      "  batch 66 loss: 7.166757859522477e-05\n",
      "  batch 67 loss: 6.833708175690845e-05\n",
      "  batch 68 loss: 7.355789421126246e-05\n",
      "  batch 69 loss: 8.139281999319792e-05\n",
      "  batch 70 loss: 0.0001443623041268438\n",
      "  batch 71 loss: 9.357529052067548e-05\n",
      "  batch 72 loss: 7.704387826379389e-05\n",
      "  batch 73 loss: 8.222219184972346e-05\n",
      "  batch 74 loss: 0.00010111381561728194\n",
      "  batch 75 loss: 0.0001315792032983154\n",
      "  batch 76 loss: 7.804183405824006e-05\n",
      "  batch 77 loss: 8.92956304596737e-05\n",
      "  batch 78 loss: 8.767331746639684e-05\n",
      "  batch 79 loss: 6.765864964108914e-05\n",
      "  batch 80 loss: 8.727193926461041e-05\n",
      "  batch 81 loss: 0.00010643518908182159\n",
      "  batch 82 loss: 0.00011640076991170645\n",
      "  batch 83 loss: 0.00011899587843799964\n",
      "  batch 84 loss: 8.245066419476643e-05\n",
      "  batch 85 loss: 9.258747741114348e-05\n",
      "  batch 86 loss: 0.00010342444147681817\n",
      "  batch 87 loss: 0.00010203532292507589\n",
      "  batch 88 loss: 0.00017153409135062248\n",
      "  batch 89 loss: 0.00010797247523441911\n",
      "  batch 90 loss: 0.0001612628111615777\n",
      "  batch 91 loss: 9.947203943738714e-05\n",
      "  batch 92 loss: 0.0001073372332029976\n",
      "  batch 93 loss: 0.00010665907029761001\n",
      "  batch 94 loss: 0.00010643343557603657\n",
      "  batch 95 loss: 0.00017935682262759656\n",
      "LOSS train 0.00017935682262759656 valid 0.0014134470839053392\n",
      "LOSS train 0.00017935682262759656 valid 0.0014943337300792336\n",
      "LOSS train 0.00017935682262759656 valid 0.001401584013365209\n",
      "LOSS train 0.00017935682262759656 valid 0.0013603393454104662\n",
      "LOSS train 0.00017935682262759656 valid 0.001387959928251803\n",
      "LOSS train 0.00017935682262759656 valid 0.0013096871552988887\n",
      "LOSS train 0.00017935682262759656 valid 0.0013411673717200756\n",
      "LOSS train 0.00017935682262759656 valid 0.0014073688071221113\n",
      "LOSS train 0.00017935682262759656 valid 0.001372258528135717\n",
      "LOSS train 0.00017935682262759656 valid 0.0013975145993754268\n",
      "LOSS train 0.00017935682262759656 valid 0.0014503983547911048\n",
      "LOSS train 0.00017935682262759656 valid 0.0014499642420560122\n",
      "LOSS train 0.00017935682262759656 valid 0.0014620600268244743\n",
      "LOSS train 0.00017935682262759656 valid 0.0014947563176974654\n",
      "LOSS train 0.00017935682262759656 valid 0.0015809780452400446\n",
      "LOSS train 0.00017935682262759656 valid 0.0016402483452111483\n",
      "LOSS train 0.00017935682262759656 valid 0.0016452271956950426\n",
      "LOSS train 0.00017935682262759656 valid 0.001652767532505095\n",
      "LOSS train 0.00017935682262759656 valid 0.0016482804203405976\n",
      "LOSS train 0.00017935682262759656 valid 0.0016415208810940385\n",
      "LOSS train 0.00017935682262759656 valid 0.0016195852076634765\n",
      "LOSS train 0.00017935682262759656 valid 0.0016336159314960241\n",
      "LOSS train 0.00017935682262759656 valid 0.0016211677575483918\n",
      "LOSS train 0.00017935682262759656 valid 0.001616908935829997\n",
      "EPOCH 92:\n",
      "  batch 1 loss: 0.00016095725004561245\n",
      "  batch 2 loss: 9.262736421078444e-05\n",
      "  batch 3 loss: 0.00015007532783783972\n",
      "  batch 4 loss: 0.00013817321450915188\n",
      "  batch 5 loss: 0.00010396283323643729\n",
      "  batch 6 loss: 0.00010822636249940842\n",
      "  batch 7 loss: 7.468850526493043e-05\n",
      "  batch 8 loss: 8.311439160024747e-05\n",
      "  batch 9 loss: 0.00010119054059032351\n",
      "  batch 10 loss: 6.024051981512457e-05\n",
      "  batch 11 loss: 0.00011225033813389018\n",
      "  batch 12 loss: 0.00012547659571282566\n",
      "  batch 13 loss: 0.00016665272414684296\n",
      "  batch 14 loss: 0.0002621424209792167\n",
      "  batch 15 loss: 0.00017005080007947981\n",
      "  batch 16 loss: 9.591624257154763e-05\n",
      "  batch 17 loss: 9.006958134705201e-05\n",
      "  batch 18 loss: 9.921138553181663e-05\n",
      "  batch 19 loss: 0.000197514338651672\n",
      "  batch 20 loss: 0.00014285356155596673\n",
      "  batch 21 loss: 0.00018710753647610545\n",
      "  batch 22 loss: 0.00010974308679578826\n",
      "  batch 23 loss: 0.0001775854325387627\n",
      "  batch 24 loss: 0.0001643868163228035\n",
      "  batch 25 loss: 0.00020342344942037016\n",
      "  batch 26 loss: 0.00020733970450237393\n",
      "  batch 27 loss: 8.433495531789958e-05\n",
      "  batch 28 loss: 0.00013399182353168726\n",
      "  batch 29 loss: 7.834075950086117e-05\n",
      "  batch 30 loss: 7.079809438437223e-05\n",
      "  batch 31 loss: 6.788913742639124e-05\n",
      "  batch 32 loss: 0.00011640131560852751\n",
      "  batch 33 loss: 0.00011854807962663472\n",
      "  batch 34 loss: 0.00013716179819311947\n",
      "  batch 35 loss: 6.460504664573818e-05\n",
      "  batch 36 loss: 5.9513429732760414e-05\n",
      "  batch 37 loss: 7.20571115380153e-05\n",
      "  batch 38 loss: 7.804384222254157e-05\n",
      "  batch 39 loss: 0.00010476668830960989\n",
      "  batch 40 loss: 0.00010950709838652983\n",
      "  batch 41 loss: 7.426719093928114e-05\n",
      "  batch 42 loss: 0.0001010765990940854\n",
      "  batch 43 loss: 7.541386730736122e-05\n",
      "  batch 44 loss: 5.342143413145095e-05\n",
      "  batch 45 loss: 6.198363553266972e-05\n",
      "  batch 46 loss: 5.618736395263113e-05\n",
      "  batch 47 loss: 8.313259604619816e-05\n",
      "  batch 48 loss: 8.570175123168156e-05\n",
      "  batch 49 loss: 8.828597492538393e-05\n",
      "  batch 50 loss: 0.00011170614743605256\n",
      "  batch 51 loss: 0.00010199764074059203\n",
      "  batch 52 loss: 9.372159547638148e-05\n",
      "  batch 53 loss: 9.259500802727416e-05\n",
      "  batch 54 loss: 9.848689660429955e-05\n",
      "  batch 55 loss: 0.00012201201752759516\n",
      "  batch 56 loss: 7.511650619562715e-05\n",
      "  batch 57 loss: 5.736903040087782e-05\n",
      "  batch 58 loss: 7.847129018045962e-05\n",
      "  batch 59 loss: 7.146617281250656e-05\n",
      "  batch 60 loss: 0.00012477912241593003\n",
      "  batch 61 loss: 4.7016190364956856e-05\n",
      "  batch 62 loss: 0.00012580861221067607\n",
      "  batch 63 loss: 7.563149847555906e-05\n",
      "  batch 64 loss: 7.315741095226258e-05\n",
      "  batch 65 loss: 6.87665306031704e-05\n",
      "  batch 66 loss: 6.537805893458426e-05\n",
      "  batch 67 loss: 6.331212352961302e-05\n",
      "  batch 68 loss: 8.087814785540104e-05\n",
      "  batch 69 loss: 8.097841055132449e-05\n",
      "  batch 70 loss: 0.00013143738033249974\n",
      "  batch 71 loss: 8.391217852476984e-05\n",
      "  batch 72 loss: 8.013128535822034e-05\n",
      "  batch 73 loss: 9.2214802862145e-05\n",
      "  batch 74 loss: 9.815514204092324e-05\n",
      "  batch 75 loss: 0.00013068145199213177\n",
      "  batch 76 loss: 8.474926289636642e-05\n",
      "  batch 77 loss: 9.752133337315172e-05\n",
      "  batch 78 loss: 9.081095777219161e-05\n",
      "  batch 79 loss: 7.21361575415358e-05\n",
      "  batch 80 loss: 8.1680242146831e-05\n",
      "  batch 81 loss: 9.491178207099438e-05\n",
      "  batch 82 loss: 0.00010158529039472342\n",
      "  batch 83 loss: 0.0001090470832423307\n",
      "  batch 84 loss: 8.170168439392e-05\n",
      "  batch 85 loss: 9.841691644396633e-05\n",
      "  batch 86 loss: 0.0001116488638217561\n",
      "  batch 87 loss: 0.00010520102659938857\n",
      "  batch 88 loss: 0.00016888248501345515\n",
      "  batch 89 loss: 0.00010764086619019508\n",
      "  batch 90 loss: 0.000165215038578026\n",
      "  batch 91 loss: 0.00010105499677592888\n",
      "  batch 92 loss: 0.0001148178125731647\n",
      "  batch 93 loss: 0.0001165616704383865\n",
      "  batch 94 loss: 0.00011413432366680354\n",
      "  batch 95 loss: 0.00018101245223078877\n",
      "LOSS train 0.00018101245223078877 valid 0.001438270090147853\n",
      "LOSS train 0.00018101245223078877 valid 0.001544877770356834\n",
      "LOSS train 0.00018101245223078877 valid 0.0015454258536919951\n",
      "LOSS train 0.00018101245223078877 valid 0.0014747523237019777\n",
      "LOSS train 0.00018101245223078877 valid 0.0014581115683540702\n",
      "LOSS train 0.00018101245223078877 valid 0.001348639139905572\n",
      "LOSS train 0.00018101245223078877 valid 0.0013643772108480334\n",
      "LOSS train 0.00018101245223078877 valid 0.0014271191321313381\n",
      "LOSS train 0.00018101245223078877 valid 0.0013893569121137261\n",
      "LOSS train 0.00018101245223078877 valid 0.0013986433623358607\n",
      "LOSS train 0.00018101245223078877 valid 0.0014264571946114302\n",
      "LOSS train 0.00018101245223078877 valid 0.0014284754870459437\n",
      "LOSS train 0.00018101245223078877 valid 0.001449624658562243\n",
      "LOSS train 0.00018101245223078877 valid 0.0014822963858023286\n",
      "LOSS train 0.00018101245223078877 valid 0.0015595491277053952\n",
      "LOSS train 0.00018101245223078877 valid 0.001620883820578456\n",
      "LOSS train 0.00018101245223078877 valid 0.0016277753747999668\n",
      "LOSS train 0.00018101245223078877 valid 0.0016380168963223696\n",
      "LOSS train 0.00018101245223078877 valid 0.001637871959246695\n",
      "LOSS train 0.00018101245223078877 valid 0.0016347982455044985\n",
      "LOSS train 0.00018101245223078877 valid 0.0016225680010393262\n",
      "LOSS train 0.00018101245223078877 valid 0.0016434593126177788\n",
      "LOSS train 0.00018101245223078877 valid 0.00162975350394845\n",
      "LOSS train 0.00018101245223078877 valid 0.001619206159375608\n",
      "EPOCH 93:\n",
      "  batch 1 loss: 0.00015718431677669287\n",
      "  batch 2 loss: 9.358544048154727e-05\n",
      "  batch 3 loss: 0.00015126782818697393\n",
      "  batch 4 loss: 0.00012610983685590327\n",
      "  batch 5 loss: 0.00010957354970742017\n",
      "  batch 6 loss: 0.00012047850759699941\n",
      "  batch 7 loss: 8.539023110643029e-05\n",
      "  batch 8 loss: 9.632137516746297e-05\n",
      "  batch 9 loss: 9.820685954764485e-05\n",
      "  batch 10 loss: 7.012105197645724e-05\n",
      "  batch 11 loss: 0.00011026174615835771\n",
      "  batch 12 loss: 0.0001373017585137859\n",
      "  batch 13 loss: 0.00017038948135450482\n",
      "  batch 14 loss: 0.0002709643158596009\n",
      "  batch 15 loss: 0.00016474377480335534\n",
      "  batch 16 loss: 0.00010128012218046933\n",
      "  batch 17 loss: 0.0001182049309136346\n",
      "  batch 18 loss: 0.00011437282955739647\n",
      "  batch 19 loss: 0.00022953665757086128\n",
      "  batch 20 loss: 0.0001661930000409484\n",
      "  batch 21 loss: 0.00019726151367649436\n",
      "  batch 22 loss: 0.00011595404794206843\n",
      "  batch 23 loss: 0.00017258018488064408\n",
      "  batch 24 loss: 0.00016154236800502986\n",
      "  batch 25 loss: 0.0002007044677156955\n",
      "  batch 26 loss: 0.00020409013086464256\n",
      "  batch 27 loss: 9.205040987581015e-05\n",
      "  batch 28 loss: 0.00012711765884887427\n",
      "  batch 29 loss: 8.294542203657329e-05\n",
      "  batch 30 loss: 7.057479524519295e-05\n",
      "  batch 31 loss: 5.886657891096547e-05\n",
      "  batch 32 loss: 0.0001067492994479835\n",
      "  batch 33 loss: 0.00010439206380397081\n",
      "  batch 34 loss: 0.0001272388908546418\n",
      "  batch 35 loss: 6.426400068448856e-05\n",
      "  batch 36 loss: 6.810307240812108e-05\n",
      "  batch 37 loss: 7.910875865491107e-05\n",
      "  batch 38 loss: 8.804938988760114e-05\n",
      "  batch 39 loss: 0.0001091825615731068\n",
      "  batch 40 loss: 0.00012029386562062427\n",
      "  batch 41 loss: 6.260705413296819e-05\n",
      "  batch 42 loss: 9.702493844088167e-05\n",
      "  batch 43 loss: 7.154180639190599e-05\n",
      "  batch 44 loss: 5.0059494242304936e-05\n",
      "  batch 45 loss: 5.666660945280455e-05\n",
      "  batch 46 loss: 5.3665855375584215e-05\n",
      "  batch 47 loss: 8.675782737554982e-05\n",
      "  batch 48 loss: 8.58509010868147e-05\n",
      "  batch 49 loss: 0.00010296908294549212\n",
      "  batch 50 loss: 0.00010599866072880104\n",
      "  batch 51 loss: 0.00011132413055747747\n",
      "  batch 52 loss: 8.501028787577525e-05\n",
      "  batch 53 loss: 9.262697130907327e-05\n",
      "  batch 54 loss: 9.250425500795245e-05\n",
      "  batch 55 loss: 0.0001231283531524241\n",
      "  batch 56 loss: 7.698321132920682e-05\n",
      "  batch 57 loss: 6.548356759594753e-05\n",
      "  batch 58 loss: 8.269664976978675e-05\n",
      "  batch 59 loss: 7.981700036907569e-05\n",
      "  batch 60 loss: 0.00012516065908130258\n",
      "  batch 61 loss: 4.609605093719438e-05\n",
      "  batch 62 loss: 0.00011293399438727647\n",
      "  batch 63 loss: 6.942198524484411e-05\n",
      "  batch 64 loss: 6.960852624615654e-05\n",
      "  batch 65 loss: 6.5695981902536e-05\n",
      "  batch 66 loss: 6.572536949533969e-05\n",
      "  batch 67 loss: 5.84255249123089e-05\n",
      "  batch 68 loss: 7.454032311215997e-05\n",
      "  batch 69 loss: 8.00732959760353e-05\n",
      "  batch 70 loss: 0.00012653197336476296\n",
      "  batch 71 loss: 7.926653779577464e-05\n",
      "  batch 72 loss: 7.605225255247205e-05\n",
      "  batch 73 loss: 8.133169467328116e-05\n",
      "  batch 74 loss: 9.728905570227653e-05\n",
      "  batch 75 loss: 0.0001370216195937246\n",
      "  batch 76 loss: 7.537752389907837e-05\n",
      "  batch 77 loss: 9.081795724341646e-05\n",
      "  batch 78 loss: 8.751830318942666e-05\n",
      "  batch 79 loss: 6.822490104241297e-05\n",
      "  batch 80 loss: 9.013881208375096e-05\n",
      "  batch 81 loss: 0.00010912812285823748\n",
      "  batch 82 loss: 0.00011168336641276255\n",
      "  batch 83 loss: 0.00011131084465887398\n",
      "  batch 84 loss: 7.486409595003352e-05\n",
      "  batch 85 loss: 9.142483759205788e-05\n",
      "  batch 86 loss: 0.00010443045175634325\n",
      "  batch 87 loss: 0.0001011723798001185\n",
      "  batch 88 loss: 0.00015642214566469193\n",
      "  batch 89 loss: 0.00010243611177429557\n",
      "  batch 90 loss: 0.00015181308845058084\n",
      "  batch 91 loss: 9.67793312156573e-05\n",
      "  batch 92 loss: 0.00010872864368138835\n",
      "  batch 93 loss: 0.00011798808554885909\n",
      "  batch 94 loss: 0.0001078180648619309\n",
      "  batch 95 loss: 0.00016974308528006077\n",
      "LOSS train 0.00016974308528006077 valid 0.0011434269836172462\n",
      "LOSS train 0.00016974308528006077 valid 0.0012654901947826147\n",
      "LOSS train 0.00016974308528006077 valid 0.0011907548177987337\n",
      "LOSS train 0.00016974308528006077 valid 0.0011458187364041805\n",
      "LOSS train 0.00016974308528006077 valid 0.0011288899695500731\n",
      "LOSS train 0.00016974308528006077 valid 0.0010382439941167831\n",
      "LOSS train 0.00016974308528006077 valid 0.0010864166542887688\n",
      "LOSS train 0.00016974308528006077 valid 0.0011378629133105278\n",
      "LOSS train 0.00016974308528006077 valid 0.0011086969170719385\n",
      "LOSS train 0.00016974308528006077 valid 0.0011328000109642744\n",
      "LOSS train 0.00016974308528006077 valid 0.0011734160361811519\n",
      "LOSS train 0.00016974308528006077 valid 0.0011849763104692101\n",
      "LOSS train 0.00016974308528006077 valid 0.001190183567814529\n",
      "LOSS train 0.00016974308528006077 valid 0.0012135142460465431\n",
      "LOSS train 0.00016974308528006077 valid 0.001281827804632485\n",
      "LOSS train 0.00016974308528006077 valid 0.0013302282895892859\n",
      "LOSS train 0.00016974308528006077 valid 0.0013346459018066525\n",
      "LOSS train 0.00016974308528006077 valid 0.0013459384208545089\n",
      "LOSS train 0.00016974308528006077 valid 0.001349415979348123\n",
      "LOSS train 0.00016974308528006077 valid 0.0013457959285005927\n",
      "LOSS train 0.00016974308528006077 valid 0.0013329496141523123\n",
      "LOSS train 0.00016974308528006077 valid 0.0013489223783835769\n",
      "LOSS train 0.00016974308528006077 valid 0.0013407958904281259\n",
      "LOSS train 0.00016974308528006077 valid 0.0013408506056293845\n",
      "EPOCH 94:\n",
      "  batch 1 loss: 0.00015385470760520548\n",
      "  batch 2 loss: 8.77439379110001e-05\n",
      "  batch 3 loss: 0.00014300084148999304\n",
      "  batch 4 loss: 0.00012170035915914923\n",
      "  batch 5 loss: 0.00010078098421217874\n",
      "  batch 6 loss: 0.00010445938823977485\n",
      "  batch 7 loss: 7.349561201408505e-05\n",
      "  batch 8 loss: 8.725487714400515e-05\n",
      "  batch 9 loss: 9.711486200103536e-05\n",
      "  batch 10 loss: 6.814501830376685e-05\n",
      "  batch 11 loss: 0.00010240739356959239\n",
      "  batch 12 loss: 0.00012746844731736928\n",
      "  batch 13 loss: 0.0001636887463973835\n",
      "  batch 14 loss: 0.0002427259023534134\n",
      "  batch 15 loss: 0.00015602521307300776\n",
      "  batch 16 loss: 9.814313671085984e-05\n",
      "  batch 17 loss: 8.537839312339202e-05\n",
      "  batch 18 loss: 9.423181472811848e-05\n",
      "  batch 19 loss: 0.00020099400717299432\n",
      "  batch 20 loss: 0.00013154561747796834\n",
      "  batch 21 loss: 0.0001827058440539986\n",
      "  batch 22 loss: 0.00011341615754645318\n",
      "  batch 23 loss: 0.00016851435066200793\n",
      "  batch 24 loss: 0.00015325279673561454\n",
      "  batch 25 loss: 0.00019622419495135546\n",
      "  batch 26 loss: 0.0001902255608001724\n",
      "  batch 27 loss: 7.964491669554263e-05\n",
      "  batch 28 loss: 0.00011952201020903885\n",
      "  batch 29 loss: 7.524248212575912e-05\n",
      "  batch 30 loss: 6.502428004750982e-05\n",
      "  batch 31 loss: 5.4904234275454655e-05\n",
      "  batch 32 loss: 0.00010037492756964639\n",
      "  batch 33 loss: 0.00010306857438990846\n",
      "  batch 34 loss: 0.0001293569221161306\n",
      "  batch 35 loss: 6.236757326405495e-05\n",
      "  batch 36 loss: 6.094041600590572e-05\n",
      "  batch 37 loss: 7.232814823510125e-05\n",
      "  batch 38 loss: 7.311826630029827e-05\n",
      "  batch 39 loss: 9.33672854444012e-05\n",
      "  batch 40 loss: 0.00011192858801223338\n",
      "  batch 41 loss: 5.827697168570012e-05\n",
      "  batch 42 loss: 9.697176574263722e-05\n",
      "  batch 43 loss: 6.567724631167948e-05\n",
      "  batch 44 loss: 4.589101445162669e-05\n",
      "  batch 45 loss: 5.828188295708969e-05\n",
      "  batch 46 loss: 4.9792917707236484e-05\n",
      "  batch 47 loss: 7.627423474332318e-05\n",
      "  batch 48 loss: 7.842866762075573e-05\n",
      "  batch 49 loss: 8.755786257097498e-05\n",
      "  batch 50 loss: 0.00010205358557868749\n",
      "  batch 51 loss: 9.816586680244654e-05\n",
      "  batch 52 loss: 9.106163633987308e-05\n",
      "  batch 53 loss: 9.412386862095445e-05\n",
      "  batch 54 loss: 9.252721793018281e-05\n",
      "  batch 55 loss: 0.00012165006046416238\n",
      "  batch 56 loss: 7.482827641069889e-05\n",
      "  batch 57 loss: 5.8048724895343184e-05\n",
      "  batch 58 loss: 7.797519356245175e-05\n",
      "  batch 59 loss: 7.09934756741859e-05\n",
      "  batch 60 loss: 0.00012100266758352518\n",
      "  batch 61 loss: 4.3971736886305735e-05\n",
      "  batch 62 loss: 0.00010523200035095215\n",
      "  batch 63 loss: 6.643298547714949e-05\n",
      "  batch 64 loss: 6.693374598398805e-05\n",
      "  batch 65 loss: 7.480652129743248e-05\n",
      "  batch 66 loss: 6.500134622910991e-05\n",
      "  batch 67 loss: 5.8464036555960774e-05\n",
      "  batch 68 loss: 7.13218905730173e-05\n",
      "  batch 69 loss: 7.728464697720483e-05\n",
      "  batch 70 loss: 0.0001228465116582811\n",
      "  batch 71 loss: 7.064316741889343e-05\n",
      "  batch 72 loss: 6.988001405261457e-05\n",
      "  batch 73 loss: 8.343227818841115e-05\n",
      "  batch 74 loss: 9.615519957151264e-05\n",
      "  batch 75 loss: 0.00012930903176311404\n",
      "  batch 76 loss: 7.218788960017264e-05\n",
      "  batch 77 loss: 8.56775586726144e-05\n",
      "  batch 78 loss: 8.664916822453961e-05\n",
      "  batch 79 loss: 6.397350807674229e-05\n",
      "  batch 80 loss: 8.697759767528623e-05\n",
      "  batch 81 loss: 0.00010411017865408212\n",
      "  batch 82 loss: 0.00011142517905682325\n",
      "  batch 83 loss: 0.00011394099419703707\n",
      "  batch 84 loss: 7.746359915472567e-05\n",
      "  batch 85 loss: 8.673869888298213e-05\n",
      "  batch 86 loss: 0.00010406901128590107\n",
      "  batch 87 loss: 9.894078539218754e-05\n",
      "  batch 88 loss: 0.0001697478728601709\n",
      "  batch 89 loss: 0.0001067705379682593\n",
      "  batch 90 loss: 0.00015155618893913925\n",
      "  batch 91 loss: 8.903277921490371e-05\n",
      "  batch 92 loss: 0.0001048482081387192\n",
      "  batch 93 loss: 0.00010176506475545466\n",
      "  batch 94 loss: 0.0001049507554853335\n",
      "  batch 95 loss: 0.00017568589828442782\n",
      "LOSS train 0.00017568589828442782 valid 0.0012610874837264419\n",
      "LOSS train 0.00017568589828442782 valid 0.0013961198274046183\n",
      "LOSS train 0.00017568589828442782 valid 0.0012534152483567595\n",
      "LOSS train 0.00017568589828442782 valid 0.001208336791023612\n",
      "LOSS train 0.00017568589828442782 valid 0.0012074223486706614\n",
      "LOSS train 0.00017568589828442782 valid 0.0011164791649207473\n",
      "LOSS train 0.00017568589828442782 valid 0.001182100735604763\n",
      "LOSS train 0.00017568589828442782 valid 0.001240984071046114\n",
      "LOSS train 0.00017568589828442782 valid 0.0012054614489898086\n",
      "LOSS train 0.00017568589828442782 valid 0.0012353112688288093\n",
      "LOSS train 0.00017568589828442782 valid 0.0013050887500867248\n",
      "LOSS train 0.00017568589828442782 valid 0.001310892403125763\n",
      "LOSS train 0.00017568589828442782 valid 0.0013046072563156486\n",
      "LOSS train 0.00017568589828442782 valid 0.0013330040965229273\n",
      "LOSS train 0.00017568589828442782 valid 0.001440659398213029\n",
      "LOSS train 0.00017568589828442782 valid 0.0015011561335995793\n",
      "LOSS train 0.00017568589828442782 valid 0.0015020804712548852\n",
      "LOSS train 0.00017568589828442782 valid 0.0015215945895761251\n",
      "LOSS train 0.00017568589828442782 valid 0.0015265516703948379\n",
      "LOSS train 0.00017568589828442782 valid 0.001526624197140336\n",
      "LOSS train 0.00017568589828442782 valid 0.0015054059913381934\n",
      "LOSS train 0.00017568589828442782 valid 0.0015216614119708538\n",
      "LOSS train 0.00017568589828442782 valid 0.0015145146753638983\n",
      "LOSS train 0.00017568589828442782 valid 0.0015319682424888015\n",
      "EPOCH 95:\n",
      "  batch 1 loss: 0.00015221640933305025\n",
      "  batch 2 loss: 8.253610576502979e-05\n",
      "  batch 3 loss: 0.00013383652549237013\n",
      "  batch 4 loss: 0.00011196004197699949\n",
      "  batch 5 loss: 9.524689812678844e-05\n",
      "  batch 6 loss: 9.239718201570213e-05\n",
      "  batch 7 loss: 6.586212111869827e-05\n",
      "  batch 8 loss: 7.912189175840467e-05\n",
      "  batch 9 loss: 9.077099821297452e-05\n",
      "  batch 10 loss: 5.74898294871673e-05\n",
      "  batch 11 loss: 9.854289965005592e-05\n",
      "  batch 12 loss: 0.0001264523307327181\n",
      "  batch 13 loss: 0.00016645636060275137\n",
      "  batch 14 loss: 0.0002450189203955233\n",
      "  batch 15 loss: 0.00015817623352631927\n",
      "  batch 16 loss: 8.976857498055324e-05\n",
      "  batch 17 loss: 7.823870691936463e-05\n",
      "  batch 18 loss: 8.607070049038157e-05\n",
      "  batch 19 loss: 0.00017961373669095337\n",
      "  batch 20 loss: 0.00012570861144922674\n",
      "  batch 21 loss: 0.00017701233446132392\n",
      "  batch 22 loss: 0.00010802842007251456\n",
      "  batch 23 loss: 0.0001590447936905548\n",
      "  batch 24 loss: 0.00014630259829573333\n",
      "  batch 25 loss: 0.00018563833145890385\n",
      "  batch 26 loss: 0.00020145998860243708\n",
      "  batch 27 loss: 7.119735528249294e-05\n",
      "  batch 28 loss: 0.00010835806460818276\n",
      "  batch 29 loss: 6.345157453324646e-05\n",
      "  batch 30 loss: 5.7189630751963705e-05\n",
      "  batch 31 loss: 5.129622149979696e-05\n",
      "  batch 32 loss: 0.00010568297875579447\n",
      "  batch 33 loss: 0.0001022459618980065\n",
      "  batch 34 loss: 0.00013856173609383404\n",
      "  batch 35 loss: 5.345071986084804e-05\n",
      "  batch 36 loss: 5.5851465731393546e-05\n",
      "  batch 37 loss: 7.276183168869466e-05\n",
      "  batch 38 loss: 7.491216820199043e-05\n",
      "  batch 39 loss: 9.463470632908866e-05\n",
      "  batch 40 loss: 0.00010692133218981326\n",
      "  batch 41 loss: 5.7617326092440635e-05\n",
      "  batch 42 loss: 0.00010352483513997868\n",
      "  batch 43 loss: 6.054234836483374e-05\n",
      "  batch 44 loss: 4.27569066232536e-05\n",
      "  batch 45 loss: 5.276558658806607e-05\n",
      "  batch 46 loss: 4.818279921892099e-05\n",
      "  batch 47 loss: 7.253247167682275e-05\n",
      "  batch 48 loss: 7.547246059402823e-05\n",
      "  batch 49 loss: 8.363727829419076e-05\n",
      "  batch 50 loss: 9.547115041641518e-05\n",
      "  batch 51 loss: 9.582492930348963e-05\n",
      "  batch 52 loss: 8.262033952632919e-05\n",
      "  batch 53 loss: 8.222414908232167e-05\n",
      "  batch 54 loss: 8.119927952066064e-05\n",
      "  batch 55 loss: 0.00011059638200094923\n",
      "  batch 56 loss: 7.0490590587724e-05\n",
      "  batch 57 loss: 6.39846984995529e-05\n",
      "  batch 58 loss: 0.00010140104859601706\n",
      "  batch 59 loss: 8.465413702651858e-05\n",
      "  batch 60 loss: 0.00011158739653183147\n",
      "  batch 61 loss: 4.186182195553556e-05\n",
      "  batch 62 loss: 9.476869308855385e-05\n",
      "  batch 63 loss: 6.507980288006365e-05\n",
      "  batch 64 loss: 5.7994697272079065e-05\n",
      "  batch 65 loss: 6.152442074380815e-05\n",
      "  batch 66 loss: 5.750694253947586e-05\n",
      "  batch 67 loss: 5.419445369625464e-05\n",
      "  batch 68 loss: 6.661388761131093e-05\n",
      "  batch 69 loss: 7.124475814634934e-05\n",
      "  batch 70 loss: 0.00012514182890299708\n",
      "  batch 71 loss: 7.830270624253899e-05\n",
      "  batch 72 loss: 6.990170368226245e-05\n",
      "  batch 73 loss: 7.449441909557208e-05\n",
      "  batch 74 loss: 9.30655951378867e-05\n",
      "  batch 75 loss: 0.00012905491166748106\n",
      "  batch 76 loss: 7.263922452693805e-05\n",
      "  batch 77 loss: 8.277810411527753e-05\n",
      "  batch 78 loss: 7.772622484480962e-05\n",
      "  batch 79 loss: 5.709580364055e-05\n",
      "  batch 80 loss: 7.951672887429595e-05\n",
      "  batch 81 loss: 0.0001012237262330018\n",
      "  batch 82 loss: 0.00011391151929274201\n",
      "  batch 83 loss: 0.00010948473209282383\n",
      "  batch 84 loss: 7.167820876929909e-05\n",
      "  batch 85 loss: 8.176072151400149e-05\n",
      "  batch 86 loss: 9.933649562299252e-05\n",
      "  batch 87 loss: 9.754033817443997e-05\n",
      "  batch 88 loss: 0.00016369175864383578\n",
      "  batch 89 loss: 0.00010505274258321151\n",
      "  batch 90 loss: 0.00016051586135290563\n",
      "  batch 91 loss: 9.215973113896325e-05\n",
      "  batch 92 loss: 0.00010180621757172048\n",
      "  batch 93 loss: 9.768258314579725e-05\n",
      "  batch 94 loss: 0.00010211415064986795\n",
      "  batch 95 loss: 0.00017041894898284227\n",
      "LOSS train 0.00017041894898284227 valid 0.0014547007158398628\n",
      "LOSS train 0.00017041894898284227 valid 0.0015194659354165196\n",
      "LOSS train 0.00017041894898284227 valid 0.0014236726565286517\n",
      "LOSS train 0.00017041894898284227 valid 0.0013696630485355854\n",
      "LOSS train 0.00017041894898284227 valid 0.0013637746451422572\n",
      "LOSS train 0.00017041894898284227 valid 0.001274174777790904\n",
      "LOSS train 0.00017041894898284227 valid 0.0013105054385960102\n",
      "LOSS train 0.00017041894898284227 valid 0.001376211759634316\n",
      "LOSS train 0.00017041894898284227 valid 0.0013379522133618593\n",
      "LOSS train 0.00017041894898284227 valid 0.0013622414553537965\n",
      "LOSS train 0.00017041894898284227 valid 0.0014079295797273517\n",
      "LOSS train 0.00017041894898284227 valid 0.0014088773168623447\n",
      "LOSS train 0.00017041894898284227 valid 0.00141550088301301\n",
      "LOSS train 0.00017041894898284227 valid 0.0014415348414331675\n",
      "LOSS train 0.00017041894898284227 valid 0.0015272456221282482\n",
      "LOSS train 0.00017041894898284227 valid 0.0015875736717134714\n",
      "LOSS train 0.00017041894898284227 valid 0.0015954704722389579\n",
      "LOSS train 0.00017041894898284227 valid 0.0016108786221593618\n",
      "LOSS train 0.00017041894898284227 valid 0.0016150922747328877\n",
      "LOSS train 0.00017041894898284227 valid 0.0016105960821732879\n",
      "LOSS train 0.00017041894898284227 valid 0.0015893367817625403\n",
      "LOSS train 0.00017041894898284227 valid 0.0016072201542556286\n",
      "LOSS train 0.00017041894898284227 valid 0.0015950393863022327\n",
      "LOSS train 0.00017041894898284227 valid 0.001596363028511405\n",
      "EPOCH 96:\n",
      "  batch 1 loss: 0.0001491269504185766\n",
      "  batch 2 loss: 8.541929128114134e-05\n",
      "  batch 3 loss: 0.00013738805137109011\n",
      "  batch 4 loss: 0.00013221654808148742\n",
      "  batch 5 loss: 0.00010075734462589025\n",
      "  batch 6 loss: 9.956826397683471e-05\n",
      "  batch 7 loss: 6.72062233206816e-05\n",
      "  batch 8 loss: 7.666346937185153e-05\n",
      "  batch 9 loss: 8.417354547418654e-05\n",
      "  batch 10 loss: 5.0726586778182536e-05\n",
      "  batch 11 loss: 0.00010059893247671425\n",
      "  batch 12 loss: 0.00012054198305122554\n",
      "  batch 13 loss: 0.000161414296599105\n",
      "  batch 14 loss: 0.00025530130369588733\n",
      "  batch 15 loss: 0.00016677836538292468\n",
      "  batch 16 loss: 8.868208533385769e-05\n",
      "  batch 17 loss: 7.886791718192399e-05\n",
      "  batch 18 loss: 9.225436951965094e-05\n",
      "  batch 19 loss: 0.00018789127352647483\n",
      "  batch 20 loss: 0.00013766298070549965\n",
      "  batch 21 loss: 0.00019036492449231446\n",
      "  batch 22 loss: 0.00010734495299402624\n",
      "  batch 23 loss: 0.00017253210535272956\n",
      "  batch 24 loss: 0.00015855726087465882\n",
      "  batch 25 loss: 0.00019924872322008014\n",
      "  batch 26 loss: 0.00019129672728013247\n",
      "  batch 27 loss: 8.200484444387257e-05\n",
      "  batch 28 loss: 0.0001311412052018568\n",
      "  batch 29 loss: 6.810413469793275e-05\n",
      "  batch 30 loss: 5.9248752222629264e-05\n",
      "  batch 31 loss: 5.3877145546721295e-05\n",
      "  batch 32 loss: 9.658248745836318e-05\n",
      "  batch 33 loss: 0.00010537868365645409\n",
      "  batch 34 loss: 0.00012658887135330588\n",
      "  batch 35 loss: 5.473816781886853e-05\n",
      "  batch 36 loss: 5.4281736083794385e-05\n",
      "  batch 37 loss: 7.076125621097162e-05\n",
      "  batch 38 loss: 7.455916056642309e-05\n",
      "  batch 39 loss: 0.00010710857168305665\n",
      "  batch 40 loss: 9.832640353124589e-05\n",
      "  batch 41 loss: 5.981626236462034e-05\n",
      "  batch 42 loss: 9.176250023301691e-05\n",
      "  batch 43 loss: 6.125431536929682e-05\n",
      "  batch 44 loss: 5.3118237701710314e-05\n",
      "  batch 45 loss: 5.405099000199698e-05\n",
      "  batch 46 loss: 4.689746492658742e-05\n",
      "  batch 47 loss: 7.748037751298398e-05\n",
      "  batch 48 loss: 8.179178985301405e-05\n",
      "  batch 49 loss: 8.99114238563925e-05\n",
      "  batch 50 loss: 0.00010121221566805616\n",
      "  batch 51 loss: 0.00011447931319708005\n",
      "  batch 52 loss: 8.69616778800264e-05\n",
      "  batch 53 loss: 8.918900130083784e-05\n",
      "  batch 54 loss: 9.202025830745697e-05\n",
      "  batch 55 loss: 0.00011529347102623433\n",
      "  batch 56 loss: 7.789673691149801e-05\n",
      "  batch 57 loss: 6.187685357872397e-05\n",
      "  batch 58 loss: 8.583116868976504e-05\n",
      "  batch 59 loss: 7.723999442532659e-05\n",
      "  batch 60 loss: 0.00012006553879473358\n",
      "  batch 61 loss: 4.737081326311454e-05\n",
      "  batch 62 loss: 0.00012110136594856158\n",
      "  batch 63 loss: 7.402992923744023e-05\n",
      "  batch 64 loss: 7.474927406292409e-05\n",
      "  batch 65 loss: 6.379096885211766e-05\n",
      "  batch 66 loss: 6.423341255867854e-05\n",
      "  batch 67 loss: 5.6071363360388204e-05\n",
      "  batch 68 loss: 7.033524889266118e-05\n",
      "  batch 69 loss: 7.596796058351174e-05\n",
      "  batch 70 loss: 0.00012822041753679514\n",
      "  batch 71 loss: 8.82556923897937e-05\n",
      "  batch 72 loss: 7.68401805544272e-05\n",
      "  batch 73 loss: 7.66701705288142e-05\n",
      "  batch 74 loss: 9.12535106181167e-05\n",
      "  batch 75 loss: 0.00013049958215560764\n",
      "  batch 76 loss: 7.490193820558488e-05\n",
      "  batch 77 loss: 8.583229646319523e-05\n",
      "  batch 78 loss: 8.695961878402159e-05\n",
      "  batch 79 loss: 7.249689952004701e-05\n",
      "  batch 80 loss: 8.292405982501805e-05\n",
      "  batch 81 loss: 9.362371929455549e-05\n",
      "  batch 82 loss: 0.00010253088112222031\n",
      "  batch 83 loss: 0.00010713837400544435\n",
      "  batch 84 loss: 7.156693027354777e-05\n",
      "  batch 85 loss: 8.05134404799901e-05\n",
      "  batch 86 loss: 9.608446998754516e-05\n",
      "  batch 87 loss: 9.717996726976708e-05\n",
      "  batch 88 loss: 0.00016732765652704984\n",
      "  batch 89 loss: 0.00010595386265777051\n",
      "  batch 90 loss: 0.00015398164396174252\n",
      "  batch 91 loss: 9.60665856837295e-05\n",
      "  batch 92 loss: 0.00011940320109715685\n",
      "  batch 93 loss: 0.00010917600593529642\n",
      "  batch 94 loss: 0.00010916221071965992\n",
      "  batch 95 loss: 0.00017355222371406853\n",
      "LOSS train 0.00017355222371406853 valid 0.0014052308397367597\n",
      "LOSS train 0.00017355222371406853 valid 0.001489059068262577\n",
      "LOSS train 0.00017355222371406853 valid 0.001463259570300579\n",
      "LOSS train 0.00017355222371406853 valid 0.001402757246978581\n",
      "LOSS train 0.00017355222371406853 valid 0.0013776393607258797\n",
      "LOSS train 0.00017355222371406853 valid 0.0012674513272941113\n",
      "LOSS train 0.00017355222371406853 valid 0.0013009911635890603\n",
      "LOSS train 0.00017355222371406853 valid 0.001360887661576271\n",
      "LOSS train 0.00017355222371406853 valid 0.0013189815217629075\n",
      "LOSS train 0.00017355222371406853 valid 0.0013348907232284546\n",
      "LOSS train 0.00017355222371406853 valid 0.0013712011277675629\n",
      "LOSS train 0.00017355222371406853 valid 0.0013782958267256618\n",
      "LOSS train 0.00017355222371406853 valid 0.00138865748886019\n",
      "LOSS train 0.00017355222371406853 valid 0.0014167100889608264\n",
      "LOSS train 0.00017355222371406853 valid 0.0015139967435970902\n",
      "LOSS train 0.00017355222371406853 valid 0.001571912900544703\n",
      "LOSS train 0.00017355222371406853 valid 0.0015762468101456761\n",
      "LOSS train 0.00017355222371406853 valid 0.0015898619312793016\n",
      "LOSS train 0.00017355222371406853 valid 0.0015956813003867865\n",
      "LOSS train 0.00017355222371406853 valid 0.0015936449635773897\n",
      "LOSS train 0.00017355222371406853 valid 0.00158030167222023\n",
      "LOSS train 0.00017355222371406853 valid 0.0016019679605960846\n",
      "LOSS train 0.00017355222371406853 valid 0.001590036554262042\n",
      "LOSS train 0.00017355222371406853 valid 0.0015834213700145483\n",
      "EPOCH 97:\n",
      "  batch 1 loss: 0.0001463404332753271\n",
      "  batch 2 loss: 8.336264727404341e-05\n",
      "  batch 3 loss: 0.0001392297854181379\n",
      "  batch 4 loss: 0.00012157736637163907\n",
      "  batch 5 loss: 0.00010621199908200651\n",
      "  batch 6 loss: 0.00012119438906665891\n",
      "  batch 7 loss: 7.951394945848733e-05\n",
      "  batch 8 loss: 8.717529271962121e-05\n",
      "  batch 9 loss: 0.00010307368938811123\n",
      "  batch 10 loss: 6.624509114772081e-05\n",
      "  batch 11 loss: 0.00010583348921500146\n",
      "  batch 12 loss: 0.00012460365542210639\n",
      "  batch 13 loss: 0.00016174919437617064\n",
      "  batch 14 loss: 0.00025846256176009774\n",
      "  batch 15 loss: 0.00017170625505968928\n",
      "  batch 16 loss: 0.00010033174476120621\n",
      "  batch 17 loss: 9.544406202621758e-05\n",
      "  batch 18 loss: 0.00010392766853328794\n",
      "  batch 19 loss: 0.00021529705554712564\n",
      "  batch 20 loss: 0.0001529701694380492\n",
      "  batch 21 loss: 0.00018406013259664178\n",
      "  batch 22 loss: 0.00011547072790563107\n",
      "  batch 23 loss: 0.0001724688772810623\n",
      "  batch 24 loss: 0.0001539745571790263\n",
      "  batch 25 loss: 0.00018674551392905414\n",
      "  batch 26 loss: 0.0001847489911597222\n",
      "  batch 27 loss: 7.318539428524673e-05\n",
      "  batch 28 loss: 0.0001246271131094545\n",
      "  batch 29 loss: 6.809279148001224e-05\n",
      "  batch 30 loss: 6.190100975800306e-05\n",
      "  batch 31 loss: 5.264895662548952e-05\n",
      "  batch 32 loss: 0.00010104842658620328\n",
      "  batch 33 loss: 0.00010694527009036392\n",
      "  batch 34 loss: 0.00012820308620575815\n",
      "  batch 35 loss: 5.42354719073046e-05\n",
      "  batch 36 loss: 5.785177199868485e-05\n",
      "  batch 37 loss: 6.789097096771002e-05\n",
      "  batch 38 loss: 7.270919741131365e-05\n",
      "  batch 39 loss: 9.33021365199238e-05\n",
      "  batch 40 loss: 0.00011102507414761931\n",
      "  batch 41 loss: 6.344486610032618e-05\n",
      "  batch 42 loss: 8.95762350410223e-05\n",
      "  batch 43 loss: 6.17791956756264e-05\n",
      "  batch 44 loss: 4.0372731746174395e-05\n",
      "  batch 45 loss: 4.971869202563539e-05\n",
      "  batch 46 loss: 4.1523846448399127e-05\n",
      "  batch 47 loss: 6.949406815692782e-05\n",
      "  batch 48 loss: 7.182529225246981e-05\n",
      "  batch 49 loss: 8.907161827664822e-05\n",
      "  batch 50 loss: 0.0001019597621052526\n",
      "  batch 51 loss: 0.000111309731437359\n",
      "  batch 52 loss: 9.048503125086427e-05\n",
      "  batch 53 loss: 8.596597763244063e-05\n",
      "  batch 54 loss: 7.996567728696391e-05\n",
      "  batch 55 loss: 0.00010794628178700805\n",
      "  batch 56 loss: 6.72117093927227e-05\n",
      "  batch 57 loss: 5.974461237201467e-05\n",
      "  batch 58 loss: 7.795817509759218e-05\n",
      "  batch 59 loss: 7.155581988627091e-05\n",
      "  batch 60 loss: 0.00012770324246957898\n",
      "  batch 61 loss: 4.2398663936182857e-05\n",
      "  batch 62 loss: 0.00011051140609197319\n",
      "  batch 63 loss: 6.533059058710933e-05\n",
      "  batch 64 loss: 5.690505713573657e-05\n",
      "  batch 65 loss: 5.758710176451132e-05\n",
      "  batch 66 loss: 5.807935303892009e-05\n",
      "  batch 67 loss: 5.222788968239911e-05\n",
      "  batch 68 loss: 7.195168291218579e-05\n",
      "  batch 69 loss: 7.219355029519647e-05\n",
      "  batch 70 loss: 0.00013062232756055892\n",
      "  batch 71 loss: 9.062394383363426e-05\n",
      "  batch 72 loss: 7.669703336432576e-05\n",
      "  batch 73 loss: 7.785925845382735e-05\n",
      "  batch 74 loss: 9.092502295970917e-05\n",
      "  batch 75 loss: 0.00012470864749047905\n",
      "  batch 76 loss: 7.43320633773692e-05\n",
      "  batch 77 loss: 7.97782267909497e-05\n",
      "  batch 78 loss: 7.257011020556092e-05\n",
      "  batch 79 loss: 5.9699465055018663e-05\n",
      "  batch 80 loss: 8.434628398390487e-05\n",
      "  batch 81 loss: 0.0001069277132046409\n",
      "  batch 82 loss: 0.00010537827620282769\n",
      "  batch 83 loss: 0.00011527871538419276\n",
      "  batch 84 loss: 7.156417268561199e-05\n",
      "  batch 85 loss: 8.164641621988267e-05\n",
      "  batch 86 loss: 9.177436004392803e-05\n",
      "  batch 87 loss: 8.885380520951003e-05\n",
      "  batch 88 loss: 0.0001545386912766844\n",
      "  batch 89 loss: 0.00010366611240897328\n",
      "  batch 90 loss: 0.0001485630200477317\n",
      "  batch 91 loss: 9.47940134210512e-05\n",
      "  batch 92 loss: 0.00010091689910041168\n",
      "  batch 93 loss: 0.0001034314336720854\n",
      "  batch 94 loss: 9.983863856177777e-05\n",
      "  batch 95 loss: 0.00015895641990937293\n",
      "LOSS train 0.00015895641990937293 valid 0.0011998445261269808\n",
      "LOSS train 0.00015895641990937293 valid 0.0013626477448269725\n",
      "LOSS train 0.00015895641990937293 valid 0.0012589161051437259\n",
      "LOSS train 0.00015895641990937293 valid 0.0012125553330406547\n",
      "LOSS train 0.00015895641990937293 valid 0.001187521731480956\n",
      "LOSS train 0.00015895641990937293 valid 0.0010872986167669296\n",
      "LOSS train 0.00015895641990937293 valid 0.0011502326233312488\n",
      "LOSS train 0.00015895641990937293 valid 0.0011986647732555866\n",
      "LOSS train 0.00015895641990937293 valid 0.0011651793029159307\n",
      "LOSS train 0.00015895641990937293 valid 0.0011878707446157932\n",
      "LOSS train 0.00015895641990937293 valid 0.0012344419956207275\n",
      "LOSS train 0.00015895641990937293 valid 0.0012428744230419397\n",
      "LOSS train 0.00015895641990937293 valid 0.0012416470563039184\n",
      "LOSS train 0.00015895641990937293 valid 0.0012625945964828134\n",
      "LOSS train 0.00015895641990937293 valid 0.0013510531280189753\n",
      "LOSS train 0.00015895641990937293 valid 0.0013976135523989797\n",
      "LOSS train 0.00015895641990937293 valid 0.0013997899368405342\n",
      "LOSS train 0.00015895641990937293 valid 0.0014162221923470497\n",
      "LOSS train 0.00015895641990937293 valid 0.0014212265377864242\n",
      "LOSS train 0.00015895641990937293 valid 0.0014200498117133975\n",
      "LOSS train 0.00015895641990937293 valid 0.0014025724958628416\n",
      "LOSS train 0.00015895641990937293 valid 0.0014222838217392564\n",
      "LOSS train 0.00015895641990937293 valid 0.0014153685187920928\n",
      "LOSS train 0.00015895641990937293 valid 0.0014259838499128819\n",
      "EPOCH 98:\n",
      "  batch 1 loss: 0.00014419161016121507\n",
      "  batch 2 loss: 8.127413457259536e-05\n",
      "  batch 3 loss: 0.00013014147407375276\n",
      "  batch 4 loss: 0.00012020855501759797\n",
      "  batch 5 loss: 9.726570715429261e-05\n",
      "  batch 6 loss: 0.00010037240281235427\n",
      "  batch 7 loss: 7.060186180751771e-05\n",
      "  batch 8 loss: 8.270647958852351e-05\n",
      "  batch 9 loss: 0.00010002467752201483\n",
      "  batch 10 loss: 6.406413740478456e-05\n",
      "  batch 11 loss: 0.00011676488793455064\n",
      "  batch 12 loss: 0.0001335393317276612\n",
      "  batch 13 loss: 0.00016155539196915925\n",
      "  batch 14 loss: 0.00024076091358438134\n",
      "  batch 15 loss: 0.00016605271957814693\n",
      "  batch 16 loss: 0.00010790243686642498\n",
      "  batch 17 loss: 0.00011091450869571418\n",
      "  batch 18 loss: 0.00010657895472832024\n",
      "  batch 19 loss: 0.0002010645839618519\n",
      "  batch 20 loss: 0.00013749902427662164\n",
      "  batch 21 loss: 0.00018288915453013033\n",
      "  batch 22 loss: 0.00012276292545720935\n",
      "  batch 23 loss: 0.00019693205831572413\n",
      "  batch 24 loss: 0.0001791993563529104\n",
      "  batch 25 loss: 0.00023604804300703108\n",
      "  batch 26 loss: 0.00020006118575111032\n",
      "  batch 27 loss: 8.546144817955792e-05\n",
      "  batch 28 loss: 0.00011874122719746083\n",
      "  batch 29 loss: 7.437585736624897e-05\n",
      "  batch 30 loss: 6.379748811013997e-05\n",
      "  batch 31 loss: 6.062525790184736e-05\n",
      "  batch 32 loss: 0.00011042271216865629\n",
      "  batch 33 loss: 0.00011270382674410939\n",
      "  batch 34 loss: 0.00013261099229566753\n",
      "  batch 35 loss: 6.520040187751874e-05\n",
      "  batch 36 loss: 6.431084329960868e-05\n",
      "  batch 37 loss: 8.173880632966757e-05\n",
      "  batch 38 loss: 7.772182289045304e-05\n",
      "  batch 39 loss: 9.72227135207504e-05\n",
      "  batch 40 loss: 0.00011352192086633295\n",
      "  batch 41 loss: 5.8638750488171354e-05\n",
      "  batch 42 loss: 8.973376679932699e-05\n",
      "  batch 43 loss: 6.403595762094483e-05\n",
      "  batch 44 loss: 4.708019696408883e-05\n",
      "  batch 45 loss: 5.637823778670281e-05\n",
      "  batch 46 loss: 5.162442903383635e-05\n",
      "  batch 47 loss: 8.04251030785963e-05\n",
      "  batch 48 loss: 7.836666190996766e-05\n",
      "  batch 49 loss: 8.667670044815168e-05\n",
      "  batch 50 loss: 0.0001135601632995531\n",
      "  batch 51 loss: 0.00010713406663853675\n",
      "  batch 52 loss: 8.323104702867568e-05\n",
      "  batch 53 loss: 9.944784687831998e-05\n",
      "  batch 54 loss: 8.787486149230972e-05\n",
      "  batch 55 loss: 0.00011861129314638674\n",
      "  batch 56 loss: 7.421225018333644e-05\n",
      "  batch 57 loss: 6.128868699306622e-05\n",
      "  batch 58 loss: 6.950475653866306e-05\n",
      "  batch 59 loss: 6.599856715183705e-05\n",
      "  batch 60 loss: 0.00012950329983141273\n",
      "  batch 61 loss: 5.161708759260364e-05\n",
      "  batch 62 loss: 0.00010508834384381771\n",
      "  batch 63 loss: 6.353575008688495e-05\n",
      "  batch 64 loss: 5.900673568248749e-05\n",
      "  batch 65 loss: 6.332540942821652e-05\n",
      "  batch 66 loss: 6.758589006494731e-05\n",
      "  batch 67 loss: 5.965361197013408e-05\n",
      "  batch 68 loss: 7.28758896002546e-05\n",
      "  batch 69 loss: 7.498155900975689e-05\n",
      "  batch 70 loss: 0.0001263561425730586\n",
      "  batch 71 loss: 8.405006519751623e-05\n",
      "  batch 72 loss: 8.227466605603695e-05\n",
      "  batch 73 loss: 8.651037205709144e-05\n",
      "  batch 74 loss: 0.00010617307270877063\n",
      "  batch 75 loss: 0.00013552981545217335\n",
      "  batch 76 loss: 7.876347808633e-05\n",
      "  batch 77 loss: 8.36644830997102e-05\n",
      "  batch 78 loss: 7.655424269614741e-05\n",
      "  batch 79 loss: 6.247659621294588e-05\n",
      "  batch 80 loss: 8.854831685312092e-05\n",
      "  batch 81 loss: 0.00010884340736083686\n",
      "  batch 82 loss: 0.00012186540698166937\n",
      "  batch 83 loss: 0.00012678682105615735\n",
      "  batch 84 loss: 8.660799358040094e-05\n",
      "  batch 85 loss: 8.708980749361217e-05\n",
      "  batch 86 loss: 9.751642210176215e-05\n",
      "  batch 87 loss: 0.0001000147603917867\n",
      "  batch 88 loss: 0.00017435537301935256\n",
      "  batch 89 loss: 0.00010716072574723512\n",
      "  batch 90 loss: 0.00015866939793340862\n",
      "  batch 91 loss: 0.00010021937487181276\n",
      "  batch 92 loss: 0.00011216090933885425\n",
      "  batch 93 loss: 0.00011298939352855086\n",
      "  batch 94 loss: 0.0001048880658345297\n",
      "  batch 95 loss: 0.00017189393111038953\n",
      "LOSS train 0.00017189393111038953 valid 0.0013141932431608438\n",
      "LOSS train 0.00017189393111038953 valid 0.0014894353225827217\n",
      "LOSS train 0.00017189393111038953 valid 0.0012993449345231056\n",
      "LOSS train 0.00017189393111038953 valid 0.0012638592161238194\n",
      "LOSS train 0.00017189393111038953 valid 0.0012556298170238733\n",
      "LOSS train 0.00017189393111038953 valid 0.001160115236416459\n",
      "LOSS train 0.00017189393111038953 valid 0.0012533823028206825\n",
      "LOSS train 0.00017189393111038953 valid 0.00130312773399055\n",
      "LOSS train 0.00017189393111038953 valid 0.0012586632510647178\n",
      "LOSS train 0.00017189393111038953 valid 0.001294399262405932\n",
      "LOSS train 0.00017189393111038953 valid 0.0013969987630844116\n",
      "LOSS train 0.00017189393111038953 valid 0.0014081462286412716\n",
      "LOSS train 0.00017189393111038953 valid 0.0013918007025495172\n",
      "LOSS train 0.00017189393111038953 valid 0.0014226152561604977\n",
      "LOSS train 0.00017189393111038953 valid 0.0015515113482251763\n",
      "LOSS train 0.00017189393111038953 valid 0.0016059778863564134\n",
      "LOSS train 0.00017189393111038953 valid 0.0016060316702350974\n",
      "LOSS train 0.00017189393111038953 valid 0.001629906240850687\n",
      "LOSS train 0.00017189393111038953 valid 0.0016421399777755141\n",
      "LOSS train 0.00017189393111038953 valid 0.0016503379447385669\n",
      "LOSS train 0.00017189393111038953 valid 0.0016253831563517451\n",
      "LOSS train 0.00017189393111038953 valid 0.0016532964073121548\n",
      "LOSS train 0.00017189393111038953 valid 0.0016487977700307965\n",
      "LOSS train 0.00017189393111038953 valid 0.001673126476816833\n",
      "EPOCH 99:\n",
      "  batch 1 loss: 0.00014528908650390804\n",
      "  batch 2 loss: 7.89278929005377e-05\n",
      "  batch 3 loss: 0.00013383491022977978\n",
      "  batch 4 loss: 0.00011453309707576409\n",
      "  batch 5 loss: 9.657317423261702e-05\n",
      "  batch 6 loss: 9.28749141166918e-05\n",
      "  batch 7 loss: 6.80298253428191e-05\n",
      "  batch 8 loss: 8.657494618091732e-05\n",
      "  batch 9 loss: 0.00010317373380530626\n",
      "  batch 10 loss: 6.130508700152859e-05\n",
      "  batch 11 loss: 0.0001138955558417365\n",
      "  batch 12 loss: 0.00013599454541690648\n",
      "  batch 13 loss: 0.00015544064808636904\n",
      "  batch 14 loss: 0.00026394534506835043\n",
      "  batch 15 loss: 0.00017193683015648276\n",
      "  batch 16 loss: 9.525218774797395e-05\n",
      "  batch 17 loss: 9.264933032682166e-05\n",
      "  batch 18 loss: 9.945073543349281e-05\n",
      "  batch 19 loss: 0.0002205937635153532\n",
      "  batch 20 loss: 0.00016560043150093406\n",
      "  batch 21 loss: 0.00020543170103337616\n",
      "  batch 22 loss: 0.0001058144771377556\n",
      "  batch 23 loss: 0.00017027341527864337\n",
      "  batch 24 loss: 0.00016722094733268023\n",
      "  batch 25 loss: 0.0002556358522269875\n",
      "  batch 26 loss: 0.0002493090578354895\n",
      "  batch 27 loss: 8.637032442493364e-05\n",
      "  batch 28 loss: 0.00013073405716568232\n",
      "  batch 29 loss: 7.207623275462538e-05\n",
      "  batch 30 loss: 6.938006117707118e-05\n",
      "  batch 31 loss: 5.7097240642178804e-05\n",
      "  batch 32 loss: 0.00010997158824466169\n",
      "  batch 33 loss: 0.0001065139367710799\n",
      "  batch 34 loss: 0.0001340447342954576\n",
      "  batch 35 loss: 5.916350710322149e-05\n",
      "  batch 36 loss: 7.215182995423675e-05\n",
      "  batch 37 loss: 7.648285827599466e-05\n",
      "  batch 38 loss: 7.143030961742625e-05\n",
      "  batch 39 loss: 9.55530849751085e-05\n",
      "  batch 40 loss: 0.00011672384425764903\n",
      "  batch 41 loss: 6.0328980907797813e-05\n",
      "  batch 42 loss: 0.00010264223965350538\n",
      "  batch 43 loss: 6.748306623194367e-05\n",
      "  batch 44 loss: 4.910555435344577e-05\n",
      "  batch 45 loss: 5.520294143934734e-05\n",
      "  batch 46 loss: 4.942117084283382e-05\n",
      "  batch 47 loss: 7.028787513263524e-05\n",
      "  batch 48 loss: 7.466026727342978e-05\n",
      "  batch 49 loss: 8.305526716867462e-05\n",
      "  batch 50 loss: 9.832765499595553e-05\n",
      "  batch 51 loss: 9.501801832811907e-05\n",
      "  batch 52 loss: 9.318957745563239e-05\n",
      "  batch 53 loss: 0.00010015245788963512\n",
      "  batch 54 loss: 8.683143823873252e-05\n",
      "  batch 55 loss: 0.00011026776337530464\n",
      "  batch 56 loss: 6.81962410453707e-05\n",
      "  batch 57 loss: 5.6834687711670995e-05\n",
      "  batch 58 loss: 7.506542897317559e-05\n",
      "  batch 59 loss: 7.032891153357923e-05\n",
      "  batch 60 loss: 0.00011121276475023478\n",
      "  batch 61 loss: 4.178025119472295e-05\n",
      "  batch 62 loss: 0.000104643011582084\n",
      "  batch 63 loss: 6.873912934679538e-05\n",
      "  batch 64 loss: 5.58959181944374e-05\n",
      "  batch 65 loss: 6.187162944115698e-05\n",
      "  batch 66 loss: 6.36622280580923e-05\n",
      "  batch 67 loss: 6.365150329656899e-05\n",
      "  batch 68 loss: 7.833557901903987e-05\n",
      "  batch 69 loss: 8.27432086225599e-05\n",
      "  batch 70 loss: 0.00011842831736430526\n",
      "  batch 71 loss: 8.429014997091144e-05\n",
      "  batch 72 loss: 6.999398465268314e-05\n",
      "  batch 73 loss: 8.096385863609612e-05\n",
      "  batch 74 loss: 9.661483636591583e-05\n",
      "  batch 75 loss: 0.00013157162175048143\n",
      "  batch 76 loss: 7.910755812190473e-05\n",
      "  batch 77 loss: 8.651941607240587e-05\n",
      "  batch 78 loss: 8.275799336843193e-05\n",
      "  batch 79 loss: 6.262109673116356e-05\n",
      "  batch 80 loss: 8.343383524334058e-05\n",
      "  batch 81 loss: 9.444855095352978e-05\n",
      "  batch 82 loss: 0.00010504061356186867\n",
      "  batch 83 loss: 0.00011614920367719606\n",
      "  batch 84 loss: 8.666420035297051e-05\n",
      "  batch 85 loss: 9.010153007693589e-05\n",
      "  batch 86 loss: 9.910624066833407e-05\n",
      "  batch 87 loss: 0.00010016417945735157\n",
      "  batch 88 loss: 0.00015513887046836317\n",
      "  batch 89 loss: 9.555535507388413e-05\n",
      "  batch 90 loss: 0.00015477603301405907\n",
      "  batch 91 loss: 9.851044160313904e-05\n",
      "  batch 92 loss: 0.00010903850488830358\n",
      "  batch 93 loss: 0.00011594581155804917\n",
      "  batch 94 loss: 0.00011795268801506609\n",
      "  batch 95 loss: 0.00018447534239385277\n",
      "LOSS train 0.00018447534239385277 valid 0.00132129923440516\n",
      "LOSS train 0.00018447534239385277 valid 0.0014813379384577274\n",
      "LOSS train 0.00018447534239385277 valid 0.0013732286170125008\n",
      "LOSS train 0.00018447534239385277 valid 0.0013088559499010444\n",
      "LOSS train 0.00018447534239385277 valid 0.001291537657380104\n",
      "LOSS train 0.00018447534239385277 valid 0.0011928530875593424\n",
      "LOSS train 0.00018447534239385277 valid 0.0012431936338543892\n",
      "LOSS train 0.00018447534239385277 valid 0.001313184853643179\n",
      "LOSS train 0.00018447534239385277 valid 0.0012691995361819863\n",
      "LOSS train 0.00018447534239385277 valid 0.0012971748365089297\n",
      "LOSS train 0.00018447534239385277 valid 0.0013648758176714182\n",
      "LOSS train 0.00018447534239385277 valid 0.0013791476376354694\n",
      "LOSS train 0.00018447534239385277 valid 0.0013751647202298045\n",
      "LOSS train 0.00018447534239385277 valid 0.0014054733328521252\n",
      "LOSS train 0.00018447534239385277 valid 0.001519733457826078\n",
      "LOSS train 0.00018447534239385277 valid 0.0015847813338041306\n",
      "LOSS train 0.00018447534239385277 valid 0.0015892048832029104\n",
      "LOSS train 0.00018447534239385277 valid 0.0016050264239311218\n",
      "LOSS train 0.00018447534239385277 valid 0.0016086007235571742\n",
      "LOSS train 0.00018447534239385277 valid 0.0016052796272560954\n",
      "LOSS train 0.00018447534239385277 valid 0.0015801535919308662\n",
      "LOSS train 0.00018447534239385277 valid 0.001599088660441339\n",
      "LOSS train 0.00018447534239385277 valid 0.0015865493332967162\n",
      "LOSS train 0.00018447534239385277 valid 0.0015921297017484903\n",
      "EPOCH 100:\n",
      "  batch 1 loss: 0.00015108851948753\n",
      "  batch 2 loss: 8.807997801341116e-05\n",
      "  batch 3 loss: 0.00014708231901749969\n",
      "  batch 4 loss: 0.00011805351095972583\n",
      "  batch 5 loss: 9.60191318881698e-05\n",
      "  batch 6 loss: 0.00010085840040119365\n",
      "  batch 7 loss: 7.787304639350623e-05\n",
      "  batch 8 loss: 9.427316399523988e-05\n",
      "  batch 9 loss: 0.00010218217357760295\n",
      "  batch 10 loss: 7.43101627449505e-05\n",
      "  batch 11 loss: 0.00010874766303459182\n",
      "  batch 12 loss: 0.00012663446250371635\n",
      "  batch 13 loss: 0.00015147848171181977\n",
      "  batch 14 loss: 0.00023683534527663141\n",
      "  batch 15 loss: 0.00015451505896635354\n",
      "  batch 16 loss: 8.203776087611914e-05\n",
      "  batch 17 loss: 8.547376637579873e-05\n",
      "  batch 18 loss: 0.00010529921564739197\n",
      "  batch 19 loss: 0.00022571219597011805\n",
      "  batch 20 loss: 0.00018131865363102406\n",
      "  batch 21 loss: 0.00021172635024413466\n",
      "  batch 22 loss: 0.00010714477684814483\n",
      "  batch 23 loss: 0.000171694002347067\n",
      "  batch 24 loss: 0.00017150603525806218\n",
      "  batch 25 loss: 0.00023420655634254217\n",
      "  batch 26 loss: 0.00025758863193914294\n",
      "  batch 27 loss: 8.613662794232368e-05\n",
      "  batch 28 loss: 0.00011651928798528388\n",
      "  batch 29 loss: 7.339045987464488e-05\n",
      "  batch 30 loss: 6.628676783293486e-05\n",
      "  batch 31 loss: 6.612701690755785e-05\n",
      "  batch 32 loss: 0.0001221007842104882\n",
      "  batch 33 loss: 0.00011795936734415591\n",
      "  batch 34 loss: 0.00015678488125558943\n",
      "  batch 35 loss: 6.20225619059056e-05\n",
      "  batch 36 loss: 5.8418641856405884e-05\n",
      "  batch 37 loss: 7.955652836244553e-05\n",
      "  batch 38 loss: 7.585248386021703e-05\n",
      "  batch 39 loss: 0.00010537079651840031\n",
      "  batch 40 loss: 0.00011261734471190721\n",
      "  batch 41 loss: 6.495753768831491e-05\n",
      "  batch 42 loss: 9.455193503526971e-05\n",
      "  batch 43 loss: 6.442501762649044e-05\n",
      "  batch 44 loss: 4.2959331040037796e-05\n",
      "  batch 45 loss: 5.3512918384512886e-05\n",
      "  batch 46 loss: 5.176161357667297e-05\n",
      "  batch 47 loss: 7.559468213003129e-05\n",
      "  batch 48 loss: 8.207655628211796e-05\n",
      "  batch 49 loss: 9.346783917862922e-05\n",
      "  batch 50 loss: 0.00011380572686903179\n",
      "  batch 51 loss: 0.00010517086775507778\n",
      "  batch 52 loss: 7.718046254012734e-05\n",
      "  batch 53 loss: 7.990955782588571e-05\n",
      "  batch 54 loss: 7.771911623422056e-05\n",
      "  batch 55 loss: 0.0001093298924388364\n",
      "  batch 56 loss: 7.841333717806265e-05\n",
      "  batch 57 loss: 6.0386795667000115e-05\n",
      "  batch 58 loss: 8.916085789678618e-05\n",
      "  batch 59 loss: 7.482653018087149e-05\n",
      "  batch 60 loss: 0.00011366336548235267\n",
      "  batch 61 loss: 4.1760304156923667e-05\n",
      "  batch 62 loss: 9.696651977719739e-05\n",
      "  batch 63 loss: 6.446973566198722e-05\n",
      "  batch 64 loss: 5.41658591828309e-05\n",
      "  batch 65 loss: 6.393846706487238e-05\n",
      "  batch 66 loss: 5.857824726263061e-05\n",
      "  batch 67 loss: 5.7158220442943275e-05\n",
      "  batch 68 loss: 7.04697085893713e-05\n",
      "  batch 69 loss: 7.338774594245479e-05\n",
      "  batch 70 loss: 0.00011292525596218184\n",
      "  batch 71 loss: 7.464081863872707e-05\n",
      "  batch 72 loss: 6.633278098888695e-05\n",
      "  batch 73 loss: 7.231544441310689e-05\n",
      "  batch 74 loss: 9.15099517442286e-05\n",
      "  batch 75 loss: 0.0001225076848641038\n",
      "  batch 76 loss: 7.023115176707506e-05\n",
      "  batch 77 loss: 7.930956780910492e-05\n",
      "  batch 78 loss: 7.97873071860522e-05\n",
      "  batch 79 loss: 6.836437387391925e-05\n",
      "  batch 80 loss: 9.008187043946236e-05\n",
      "  batch 81 loss: 9.81066477834247e-05\n",
      "  batch 82 loss: 9.249256254406646e-05\n",
      "  batch 83 loss: 0.00010459270561113954\n",
      "  batch 84 loss: 7.243666186695918e-05\n",
      "  batch 85 loss: 8.092187636066228e-05\n",
      "  batch 86 loss: 9.693307220004499e-05\n",
      "  batch 87 loss: 9.976836736313999e-05\n",
      "  batch 88 loss: 0.0001568064617458731\n",
      "  batch 89 loss: 9.588220564182848e-05\n",
      "  batch 90 loss: 0.00014622443995904177\n",
      "  batch 91 loss: 8.359360799659044e-05\n",
      "  batch 92 loss: 0.0001036773610394448\n",
      "  batch 93 loss: 0.00010095193283632398\n",
      "  batch 94 loss: 0.00010628331074258313\n",
      "  batch 95 loss: 0.0001767207431839779\n",
      "LOSS train 0.0001767207431839779 valid 0.0012247241102159023\n",
      "LOSS train 0.0001767207431839779 valid 0.0014104262227192521\n",
      "LOSS train 0.0001767207431839779 valid 0.0013464835938066244\n",
      "LOSS train 0.0001767207431839779 valid 0.0012826994061470032\n",
      "LOSS train 0.0001767207431839779 valid 0.001277811941690743\n",
      "LOSS train 0.0001767207431839779 valid 0.0011770857963711023\n",
      "LOSS train 0.0001767207431839779 valid 0.0012302822433412075\n",
      "LOSS train 0.0001767207431839779 valid 0.001288631814531982\n",
      "LOSS train 0.0001767207431839779 valid 0.0012538693845272064\n",
      "LOSS train 0.0001767207431839779 valid 0.0012723165564239025\n",
      "LOSS train 0.0001767207431839779 valid 0.0013267674949020147\n",
      "LOSS train 0.0001767207431839779 valid 0.0013361539458855987\n",
      "LOSS train 0.0001767207431839779 valid 0.0013386009959504008\n",
      "LOSS train 0.0001767207431839779 valid 0.0013647356536239386\n",
      "LOSS train 0.0001767207431839779 valid 0.0014642177848145366\n",
      "LOSS train 0.0001767207431839779 valid 0.0015195469604805112\n",
      "LOSS train 0.0001767207431839779 valid 0.001523833372630179\n",
      "LOSS train 0.0001767207431839779 valid 0.0015330319292843342\n",
      "LOSS train 0.0001767207431839779 valid 0.0015339136589318514\n",
      "LOSS train 0.0001767207431839779 valid 0.0015316682402044535\n",
      "LOSS train 0.0001767207431839779 valid 0.0015150591498240829\n",
      "LOSS train 0.0001767207431839779 valid 0.0015346596483141184\n",
      "LOSS train 0.0001767207431839779 valid 0.0015245894901454449\n",
      "LOSS train 0.0001767207431839779 valid 0.0015248203417286277\n",
      "EPOCH 101:\n",
      "  batch 1 loss: 0.00015179072215687484\n",
      "  batch 2 loss: 9.506746573606506e-05\n",
      "  batch 3 loss: 0.00013891811249777675\n",
      "  batch 4 loss: 0.0001127416908275336\n",
      "  batch 5 loss: 9.599023178452626e-05\n",
      "  batch 6 loss: 9.780892287380993e-05\n",
      "  batch 7 loss: 7.003366044955328e-05\n",
      "  batch 8 loss: 8.860204252414405e-05\n",
      "  batch 9 loss: 9.366688027512282e-05\n",
      "  batch 10 loss: 6.85115228407085e-05\n",
      "  batch 11 loss: 0.00011885118146892637\n",
      "  batch 12 loss: 0.00014822278171777725\n",
      "  batch 13 loss: 0.00018924084724858403\n",
      "  batch 14 loss: 0.0002643965999595821\n",
      "  batch 15 loss: 0.0001535287592560053\n",
      "  batch 16 loss: 8.934910874813795e-05\n",
      "  batch 17 loss: 8.080619591055438e-05\n",
      "  batch 18 loss: 0.00010615639621391892\n",
      "  batch 19 loss: 0.000198495268705301\n",
      "  batch 20 loss: 0.0001649400219321251\n",
      "  batch 21 loss: 0.00020659848814830184\n",
      "  batch 22 loss: 0.00012816871458198875\n",
      "  batch 23 loss: 0.0001830979308579117\n",
      "  batch 24 loss: 0.0001683931186562404\n",
      "  batch 25 loss: 0.00019533505837898701\n",
      "  batch 26 loss: 0.0002280416083522141\n",
      "  batch 27 loss: 8.086072921287268e-05\n",
      "  batch 28 loss: 0.00012914447870571166\n",
      "  batch 29 loss: 7.38354428904131e-05\n",
      "  batch 30 loss: 6.73477043164894e-05\n",
      "  batch 31 loss: 6.564601790159941e-05\n",
      "  batch 32 loss: 0.00011480218381620944\n",
      "  batch 33 loss: 0.00011879672092618421\n",
      "  batch 34 loss: 0.00014367405674420297\n",
      "  batch 35 loss: 6.33003146504052e-05\n",
      "  batch 36 loss: 6.194147863425314e-05\n",
      "  batch 37 loss: 7.211914635263383e-05\n",
      "  batch 38 loss: 8.003735274542123e-05\n",
      "  batch 39 loss: 0.00010420831677038223\n",
      "  batch 40 loss: 0.000120728844194673\n",
      "  batch 41 loss: 6.721218232996762e-05\n",
      "  batch 42 loss: 9.575528383720666e-05\n",
      "  batch 43 loss: 6.795341323595494e-05\n",
      "  batch 44 loss: 4.6692985051777214e-05\n",
      "  batch 45 loss: 5.7470184401609004e-05\n",
      "  batch 46 loss: 5.1539660489652306e-05\n",
      "  batch 47 loss: 7.635533984284848e-05\n",
      "  batch 48 loss: 7.682008435949683e-05\n",
      "  batch 49 loss: 8.66925111040473e-05\n",
      "  batch 50 loss: 0.00011158234701724723\n",
      "  batch 51 loss: 0.00011265156354056671\n",
      "  batch 52 loss: 9.373699867865071e-05\n",
      "  batch 53 loss: 8.976760727819055e-05\n",
      "  batch 54 loss: 8.590477955294773e-05\n",
      "  batch 55 loss: 0.00011890471796505153\n",
      "  batch 56 loss: 7.791661482769996e-05\n",
      "  batch 57 loss: 5.638653237838298e-05\n",
      "  batch 58 loss: 7.168244337663054e-05\n",
      "  batch 59 loss: 6.692820170428604e-05\n",
      "  batch 60 loss: 0.0001255759270861745\n",
      "  batch 61 loss: 4.2754134483402595e-05\n",
      "  batch 62 loss: 0.00010021161870099604\n",
      "  batch 63 loss: 6.633486191276461e-05\n",
      "  batch 64 loss: 5.220732055022381e-05\n",
      "  batch 65 loss: 5.4658619774272665e-05\n",
      "  batch 66 loss: 5.727996176574379e-05\n",
      "  batch 67 loss: 5.2134913858026266e-05\n",
      "  batch 68 loss: 6.472526001743972e-05\n",
      "  batch 69 loss: 6.938308069948107e-05\n",
      "  batch 70 loss: 0.00011489136522868648\n",
      "  batch 71 loss: 6.972964183660224e-05\n",
      "  batch 72 loss: 6.884694448672235e-05\n",
      "  batch 73 loss: 6.916634447406977e-05\n",
      "  batch 74 loss: 8.77428101375699e-05\n",
      "  batch 75 loss: 0.00011678471491904929\n",
      "  batch 76 loss: 7.544923573732376e-05\n",
      "  batch 77 loss: 8.07967662694864e-05\n",
      "  batch 78 loss: 7.749805808998644e-05\n",
      "  batch 79 loss: 6.459122232627124e-05\n",
      "  batch 80 loss: 8.62237429828383e-05\n",
      "  batch 81 loss: 0.00010133610339835286\n",
      "  batch 82 loss: 9.436880645807832e-05\n",
      "  batch 83 loss: 0.0001111044330173172\n",
      "  batch 84 loss: 8.462222467642277e-05\n",
      "  batch 85 loss: 9.23899351619184e-05\n",
      "  batch 86 loss: 9.997950110118836e-05\n",
      "  batch 87 loss: 9.859319106908515e-05\n",
      "  batch 88 loss: 0.00015601265477016568\n",
      "  batch 89 loss: 9.72592897596769e-05\n",
      "  batch 90 loss: 0.00014482656843028963\n",
      "  batch 91 loss: 8.262899063993245e-05\n",
      "  batch 92 loss: 0.00010659997496986762\n",
      "  batch 93 loss: 9.792923810891807e-05\n",
      "  batch 94 loss: 9.93262801785022e-05\n",
      "  batch 95 loss: 0.00016454410797450691\n",
      "LOSS train 0.00016454410797450691 valid 0.0013025207445025444\n",
      "LOSS train 0.00016454410797450691 valid 0.0014958931133151054\n",
      "LOSS train 0.00016454410797450691 valid 0.0013762976741418242\n",
      "LOSS train 0.00016454410797450691 valid 0.001325669465586543\n",
      "LOSS train 0.00016454410797450691 valid 0.0013397398870438337\n",
      "LOSS train 0.00016454410797450691 valid 0.0012362058041617274\n",
      "LOSS train 0.00016454410797450691 valid 0.001310823136009276\n",
      "LOSS train 0.00016454410797450691 valid 0.0013681872515007854\n",
      "LOSS train 0.00016454410797450691 valid 0.0013227366143837571\n",
      "LOSS train 0.00016454410797450691 valid 0.001351547776721418\n",
      "LOSS train 0.00016454410797450691 valid 0.0014360310742631555\n",
      "LOSS train 0.00016454410797450691 valid 0.0014408836141228676\n",
      "LOSS train 0.00016454410797450691 valid 0.001434153295122087\n",
      "LOSS train 0.00016454410797450691 valid 0.0014546908205375075\n",
      "LOSS train 0.00016454410797450691 valid 0.0015554582932963967\n",
      "LOSS train 0.00016454410797450691 valid 0.0016121452208608389\n",
      "LOSS train 0.00016454410797450691 valid 0.0016193232731893659\n",
      "LOSS train 0.00016454410797450691 valid 0.001642937189899385\n",
      "LOSS train 0.00016454410797450691 valid 0.001651868224143982\n",
      "LOSS train 0.00016454410797450691 valid 0.0016568148275837302\n",
      "LOSS train 0.00016454410797450691 valid 0.0016346032498404384\n",
      "LOSS train 0.00016454410797450691 valid 0.0016529684653505683\n",
      "LOSS train 0.00016454410797450691 valid 0.0016449899412691593\n",
      "LOSS train 0.00016454410797450691 valid 0.0016652758931741118\n",
      "EPOCH 102:\n",
      "  batch 1 loss: 0.00015095793060027063\n",
      "  batch 2 loss: 8.747611718717963e-05\n",
      "  batch 3 loss: 0.00013348666834644973\n",
      "  batch 4 loss: 0.00011332595022395253\n",
      "  batch 5 loss: 9.714074985822663e-05\n",
      "  batch 6 loss: 9.514995326753706e-05\n",
      "  batch 7 loss: 7.00718373991549e-05\n",
      "  batch 8 loss: 7.64045980758965e-05\n",
      "  batch 9 loss: 8.520617848262191e-05\n",
      "  batch 10 loss: 5.7145287428284064e-05\n",
      "  batch 11 loss: 0.0001032547079375945\n",
      "  batch 12 loss: 0.00013565277913585305\n",
      "  batch 13 loss: 0.00017786923854146153\n",
      "  batch 14 loss: 0.00028291356284171343\n",
      "  batch 15 loss: 0.00018431898206472397\n",
      "  batch 16 loss: 0.00010086277325171977\n",
      "  batch 17 loss: 8.844881813274696e-05\n",
      "  batch 18 loss: 0.00010918554471572861\n",
      "  batch 19 loss: 0.00022493964934255928\n",
      "  batch 20 loss: 0.00016065160161815584\n",
      "  batch 21 loss: 0.00019477441674098372\n",
      "  batch 22 loss: 0.00013790269440505654\n",
      "  batch 23 loss: 0.0002080488484352827\n",
      "  batch 24 loss: 0.00018350050959270447\n",
      "  batch 25 loss: 0.00021032613585703075\n",
      "  batch 26 loss: 0.00020136823877692223\n",
      "  batch 27 loss: 8.858623186824843e-05\n",
      "  batch 28 loss: 0.00012758949014823884\n",
      "  batch 29 loss: 7.566912972833961e-05\n",
      "  batch 30 loss: 7.681510760448873e-05\n",
      "  batch 31 loss: 6.916627171449363e-05\n",
      "  batch 32 loss: 0.00012000514107057825\n",
      "  batch 33 loss: 0.00011590611393330619\n",
      "  batch 34 loss: 0.0001427812676411122\n",
      "  batch 35 loss: 6.897374987602234e-05\n",
      "  batch 36 loss: 6.573591963388026e-05\n",
      "  batch 37 loss: 7.379636372206733e-05\n",
      "  batch 38 loss: 7.218676182674244e-05\n",
      "  batch 39 loss: 9.810368646867573e-05\n",
      "  batch 40 loss: 0.00011573357915040106\n",
      "  batch 41 loss: 6.280073284870014e-05\n",
      "  batch 42 loss: 9.387599129695445e-05\n",
      "  batch 43 loss: 6.999941979302093e-05\n",
      "  batch 44 loss: 5.483127824845724e-05\n",
      "  batch 45 loss: 6.202069926075637e-05\n",
      "  batch 46 loss: 5.099798727314919e-05\n",
      "  batch 47 loss: 7.932519656606019e-05\n",
      "  batch 48 loss: 8.06712923804298e-05\n",
      "  batch 49 loss: 9.13492840481922e-05\n",
      "  batch 50 loss: 0.00011301285121589899\n",
      "  batch 51 loss: 0.00011845093104057014\n",
      "  batch 52 loss: 9.971764666261151e-05\n",
      "  batch 53 loss: 8.590931247454137e-05\n",
      "  batch 54 loss: 8.842213719617575e-05\n",
      "  batch 55 loss: 0.00011518207611516118\n",
      "  batch 56 loss: 7.99062690930441e-05\n",
      "  batch 57 loss: 6.209804996615276e-05\n",
      "  batch 58 loss: 8.014884951990098e-05\n",
      "  batch 59 loss: 7.362096948781982e-05\n",
      "  batch 60 loss: 0.00012965677888132632\n",
      "  batch 61 loss: 4.459179763216525e-05\n",
      "  batch 62 loss: 0.00011010424350388348\n",
      "  batch 63 loss: 7.171092147473246e-05\n",
      "  batch 64 loss: 5.8371006161905825e-05\n",
      "  batch 65 loss: 5.6297987612197176e-05\n",
      "  batch 66 loss: 6.42309823888354e-05\n",
      "  batch 67 loss: 5.193459219299257e-05\n",
      "  batch 68 loss: 6.678419595118612e-05\n",
      "  batch 69 loss: 7.764596375636756e-05\n",
      "  batch 70 loss: 0.00012163538485765457\n",
      "  batch 71 loss: 7.715096580795944e-05\n",
      "  batch 72 loss: 7.081026706146076e-05\n",
      "  batch 73 loss: 7.270264177350327e-05\n",
      "  batch 74 loss: 8.971531497081742e-05\n",
      "  batch 75 loss: 0.00013166858116164804\n",
      "  batch 76 loss: 7.540066144429147e-05\n",
      "  batch 77 loss: 7.475209713447839e-05\n",
      "  batch 78 loss: 7.777003338560462e-05\n",
      "  batch 79 loss: 6.75615738146007e-05\n",
      "  batch 80 loss: 8.647053618915379e-05\n",
      "  batch 81 loss: 0.00010860590555239469\n",
      "  batch 82 loss: 9.88290921668522e-05\n",
      "  batch 83 loss: 0.00010513863526284695\n",
      "  batch 84 loss: 7.264906889759004e-05\n",
      "  batch 85 loss: 8.060206891968846e-05\n",
      "  batch 86 loss: 9.854468225967139e-05\n",
      "  batch 87 loss: 0.00010052293509943411\n",
      "  batch 88 loss: 0.00016540265642106533\n",
      "  batch 89 loss: 0.00010321777517674491\n",
      "  batch 90 loss: 0.0001528195571154356\n",
      "  batch 91 loss: 9.252646123059094e-05\n",
      "  batch 92 loss: 0.00011548421753104776\n",
      "  batch 93 loss: 9.916673297993839e-05\n",
      "  batch 94 loss: 9.582828351994976e-05\n",
      "  batch 95 loss: 0.00016697181854397058\n",
      "LOSS train 0.00016697181854397058 valid 0.0013597756624221802\n",
      "LOSS train 0.00016697181854397058 valid 0.0014528910396620631\n",
      "LOSS train 0.00016697181854397058 valid 0.0014150652568787336\n",
      "LOSS train 0.00016697181854397058 valid 0.0013659559190273285\n",
      "LOSS train 0.00016697181854397058 valid 0.0013785535702481866\n",
      "LOSS train 0.00016697181854397058 valid 0.001279857475310564\n",
      "LOSS train 0.00016697181854397058 valid 0.001303387340158224\n",
      "LOSS train 0.00016697181854397058 valid 0.0013607508735731244\n",
      "LOSS train 0.00016697181854397058 valid 0.0013289828784763813\n",
      "LOSS train 0.00016697181854397058 valid 0.0013540706131607294\n",
      "LOSS train 0.00016697181854397058 valid 0.0014009898295626044\n",
      "LOSS train 0.00016697181854397058 valid 0.0014184042811393738\n",
      "LOSS train 0.00016697181854397058 valid 0.0014309401158243418\n",
      "LOSS train 0.00016697181854397058 valid 0.001453878590837121\n",
      "LOSS train 0.00016697181854397058 valid 0.001525926636531949\n",
      "LOSS train 0.00016697181854397058 valid 0.0015795126091688871\n",
      "LOSS train 0.00016697181854397058 valid 0.0015912498347461224\n",
      "LOSS train 0.00016697181854397058 valid 0.001596046844497323\n",
      "LOSS train 0.00016697181854397058 valid 0.0015948537038639188\n",
      "LOSS train 0.00016697181854397058 valid 0.0015885931206867099\n",
      "LOSS train 0.00016697181854397058 valid 0.0015705018304288387\n",
      "LOSS train 0.00016697181854397058 valid 0.0015871021896600723\n",
      "LOSS train 0.00016697181854397058 valid 0.0015792830381542444\n",
      "LOSS train 0.00016697181854397058 valid 0.0015750810271129012\n",
      "EPOCH 103:\n",
      "  batch 1 loss: 0.00014641121379099786\n",
      "  batch 2 loss: 8.203496690839529e-05\n",
      "  batch 3 loss: 0.00012992651318199933\n",
      "  batch 4 loss: 0.00012403051368892193\n",
      "  batch 5 loss: 0.00011069325410062447\n",
      "  batch 6 loss: 0.00011376898328308016\n",
      "  batch 7 loss: 7.368743536062539e-05\n",
      "  batch 8 loss: 7.37302761990577e-05\n",
      "  batch 9 loss: 8.191432425519451e-05\n",
      "  batch 10 loss: 5.776957550551742e-05\n",
      "  batch 11 loss: 9.418815170647576e-05\n",
      "  batch 12 loss: 0.00012708810390904546\n",
      "  batch 13 loss: 0.00016416696598753333\n",
      "  batch 14 loss: 0.0002659751335158944\n",
      "  batch 15 loss: 0.00019764903117902577\n",
      "  batch 16 loss: 9.510607924312353e-05\n",
      "  batch 17 loss: 9.395006054546684e-05\n",
      "  batch 18 loss: 0.00010611265315674245\n",
      "  batch 19 loss: 0.0001968504802789539\n",
      "  batch 20 loss: 0.00015075375267770141\n",
      "  batch 21 loss: 0.00017912167822942138\n",
      "  batch 22 loss: 0.00011343182995915413\n",
      "  batch 23 loss: 0.00018954576808027923\n",
      "  batch 24 loss: 0.000177639041794464\n",
      "  batch 25 loss: 0.00024155629216693342\n",
      "  batch 26 loss: 0.0002120179997291416\n",
      "  batch 27 loss: 8.731091656954959e-05\n",
      "  batch 28 loss: 0.0001388106174999848\n",
      "  batch 29 loss: 7.364709017565474e-05\n",
      "  batch 30 loss: 6.750114698661491e-05\n",
      "  batch 31 loss: 6.11721770837903e-05\n",
      "  batch 32 loss: 0.00010951497097266838\n",
      "  batch 33 loss: 0.0001149656527559273\n",
      "  batch 34 loss: 0.00014042344992049038\n",
      "  batch 35 loss: 6.347820453811437e-05\n",
      "  batch 36 loss: 7.464762893505394e-05\n",
      "  batch 37 loss: 7.945676043163985e-05\n",
      "  batch 38 loss: 8.833027095533907e-05\n",
      "  batch 39 loss: 0.00010587467841105536\n",
      "  batch 40 loss: 0.00011701601033564657\n",
      "  batch 41 loss: 6.5579530200921e-05\n",
      "  batch 42 loss: 9.366826270706952e-05\n",
      "  batch 43 loss: 6.666036642855033e-05\n",
      "  batch 44 loss: 4.956965858582407e-05\n",
      "  batch 45 loss: 5.320980562828481e-05\n",
      "  batch 46 loss: 4.7492285375483334e-05\n",
      "  batch 47 loss: 7.495161844417453e-05\n",
      "  batch 48 loss: 7.7496086305473e-05\n",
      "  batch 49 loss: 8.560153946746141e-05\n",
      "  batch 50 loss: 9.869644418358803e-05\n",
      "  batch 51 loss: 0.00011760681809391826\n",
      "  batch 52 loss: 8.577709377277642e-05\n",
      "  batch 53 loss: 9.129092359216884e-05\n",
      "  batch 54 loss: 9.034256800077856e-05\n",
      "  batch 55 loss: 0.00011353207810316235\n",
      "  batch 56 loss: 6.670666334684938e-05\n",
      "  batch 57 loss: 5.432598845800385e-05\n",
      "  batch 58 loss: 7.78870380599983e-05\n",
      "  batch 59 loss: 6.713435868732631e-05\n",
      "  batch 60 loss: 0.00012872295337729156\n",
      "  batch 61 loss: 5.191282252781093e-05\n",
      "  batch 62 loss: 0.00013353320537135005\n",
      "  batch 63 loss: 7.770930096739903e-05\n",
      "  batch 64 loss: 6.368237518472597e-05\n",
      "  batch 65 loss: 5.6985507399076596e-05\n",
      "  batch 66 loss: 6.12236253800802e-05\n",
      "  batch 67 loss: 5.436801438918337e-05\n",
      "  batch 68 loss: 7.051866850815713e-05\n",
      "  batch 69 loss: 8.114190131891519e-05\n",
      "  batch 70 loss: 0.0001223785220645368\n",
      "  batch 71 loss: 7.652778731426224e-05\n",
      "  batch 72 loss: 7.435815496137366e-05\n",
      "  batch 73 loss: 7.546086271759123e-05\n",
      "  batch 74 loss: 0.00010089991701534018\n",
      "  batch 75 loss: 0.00014370953431352973\n",
      "  batch 76 loss: 8.613224053988233e-05\n",
      "  batch 77 loss: 7.979814108693972e-05\n",
      "  batch 78 loss: 7.352770626312122e-05\n",
      "  batch 79 loss: 6.0386897530406713e-05\n",
      "  batch 80 loss: 8.857733337208629e-05\n",
      "  batch 81 loss: 0.00010589618614176288\n",
      "  batch 82 loss: 0.00010236403613816947\n",
      "  batch 83 loss: 0.00011151341459481046\n",
      "  batch 84 loss: 7.002794154686853e-05\n",
      "  batch 85 loss: 8.180399890989065e-05\n",
      "  batch 86 loss: 9.100408351514488e-05\n",
      "  batch 87 loss: 8.948171307565644e-05\n",
      "  batch 88 loss: 0.00014820412616245449\n",
      "  batch 89 loss: 0.00010432145791128278\n",
      "  batch 90 loss: 0.00015597546007484198\n",
      "  batch 91 loss: 9.282232349505648e-05\n",
      "  batch 92 loss: 0.00010696267418097705\n",
      "  batch 93 loss: 0.00010463692888151854\n",
      "  batch 94 loss: 0.00010307635966455564\n",
      "  batch 95 loss: 0.0001667807373451069\n",
      "LOSS train 0.0001667807373451069 valid 0.0013089936692267656\n",
      "LOSS train 0.0001667807373451069 valid 0.001456918427720666\n",
      "LOSS train 0.0001667807373451069 valid 0.0013444931246340275\n",
      "LOSS train 0.0001667807373451069 valid 0.0013004071079194546\n",
      "LOSS train 0.0001667807373451069 valid 0.001314537599682808\n",
      "LOSS train 0.0001667807373451069 valid 0.001219763420522213\n",
      "LOSS train 0.0001667807373451069 valid 0.0012816337402909994\n",
      "LOSS train 0.0001667807373451069 valid 0.0013485706876963377\n",
      "LOSS train 0.0001667807373451069 valid 0.0013142694951966405\n",
      "LOSS train 0.0001667807373451069 valid 0.0013423763448372483\n",
      "LOSS train 0.0001667807373451069 valid 0.0013999055372551084\n",
      "LOSS train 0.0001667807373451069 valid 0.0014128488255664706\n",
      "LOSS train 0.0001667807373451069 valid 0.0014091937337070704\n",
      "LOSS train 0.0001667807373451069 valid 0.001437041792087257\n",
      "LOSS train 0.0001667807373451069 valid 0.0015374008798971772\n",
      "LOSS train 0.0001667807373451069 valid 0.00158875179477036\n",
      "LOSS train 0.0001667807373451069 valid 0.0015947561478242278\n",
      "LOSS train 0.0001667807373451069 valid 0.0016095965402200818\n",
      "LOSS train 0.0001667807373451069 valid 0.0016130900476127863\n",
      "LOSS train 0.0001667807373451069 valid 0.0016044346848502755\n",
      "LOSS train 0.0001667807373451069 valid 0.0015795043436810374\n",
      "LOSS train 0.0001667807373451069 valid 0.0015950025990605354\n",
      "LOSS train 0.0001667807373451069 valid 0.0015884613385424018\n",
      "LOSS train 0.0001667807373451069 valid 0.001587859122082591\n",
      "EPOCH 104:\n",
      "  batch 1 loss: 0.00014908912999089807\n",
      "  batch 2 loss: 9.097891597775742e-05\n",
      "  batch 3 loss: 0.00012705472181551158\n",
      "  batch 4 loss: 0.00011573913798201829\n",
      "  batch 5 loss: 0.0001119991356972605\n",
      "  batch 6 loss: 0.0001289701904170215\n",
      "  batch 7 loss: 8.393421012442559e-05\n",
      "  batch 8 loss: 0.00011210818774998188\n",
      "  batch 9 loss: 9.903046884573996e-05\n",
      "  batch 10 loss: 6.463952013291419e-05\n",
      "  batch 11 loss: 0.00010684307926567271\n",
      "  batch 12 loss: 0.0001360724854748696\n",
      "  batch 13 loss: 0.00016138621140271425\n",
      "  batch 14 loss: 0.0002499349066056311\n",
      "  batch 15 loss: 0.00018962041940540075\n",
      "  batch 16 loss: 0.00010616335202939808\n",
      "  batch 17 loss: 0.00010959385690512136\n",
      "  batch 18 loss: 0.00011299808829789981\n",
      "  batch 19 loss: 0.00022054699365980923\n",
      "  batch 20 loss: 0.00015587346570100635\n",
      "  batch 21 loss: 0.0001846173545345664\n",
      "  batch 22 loss: 0.00011238225852139294\n",
      "  batch 23 loss: 0.00017661499441601336\n",
      "  batch 24 loss: 0.0001695095852483064\n",
      "  batch 25 loss: 0.00023836383479647338\n",
      "  batch 26 loss: 0.0002313598815817386\n",
      "  batch 27 loss: 9.653637243900448e-05\n",
      "  batch 28 loss: 0.00014386075781658292\n",
      "  batch 29 loss: 8.129359048325568e-05\n",
      "  batch 30 loss: 7.829139940440655e-05\n",
      "  batch 31 loss: 7.30213214410469e-05\n",
      "  batch 32 loss: 0.00011713171988958493\n",
      "  batch 33 loss: 0.00013779755681753159\n",
      "  batch 34 loss: 0.00014856192865408957\n",
      "  batch 35 loss: 6.994439172558486e-05\n",
      "  batch 36 loss: 6.284798291744664e-05\n",
      "  batch 37 loss: 7.799775630701333e-05\n",
      "  batch 38 loss: 8.16639803815633e-05\n",
      "  batch 39 loss: 0.00010756857227534056\n",
      "  batch 40 loss: 0.00011167670891154557\n",
      "  batch 41 loss: 6.924071931280196e-05\n",
      "  batch 42 loss: 0.00010330433724448085\n",
      "  batch 43 loss: 7.604219717904925e-05\n",
      "  batch 44 loss: 5.823628816870041e-05\n",
      "  batch 45 loss: 5.942055577179417e-05\n",
      "  batch 46 loss: 4.934087337460369e-05\n",
      "  batch 47 loss: 7.451340206898749e-05\n",
      "  batch 48 loss: 7.592943438794464e-05\n",
      "  batch 49 loss: 8.297775639221072e-05\n",
      "  batch 50 loss: 9.595572191756219e-05\n",
      "  batch 51 loss: 0.00011661169992294163\n",
      "  batch 52 loss: 9.551428956910968e-05\n",
      "  batch 53 loss: 9.955532004823908e-05\n",
      "  batch 54 loss: 9.987299563363194e-05\n",
      "  batch 55 loss: 0.00011403343523852527\n",
      "  batch 56 loss: 6.949852831894532e-05\n",
      "  batch 57 loss: 5.7701072364579886e-05\n",
      "  batch 58 loss: 7.510670548072085e-05\n",
      "  batch 59 loss: 6.252370803849772e-05\n",
      "  batch 60 loss: 0.00010992325405823067\n",
      "  batch 61 loss: 4.507889389060438e-05\n",
      "  batch 62 loss: 0.0001216200107592158\n",
      "  batch 63 loss: 7.409501995425671e-05\n",
      "  batch 64 loss: 7.287849439308047e-05\n",
      "  batch 65 loss: 8.374956087209284e-05\n",
      "  batch 66 loss: 6.526280049001798e-05\n",
      "  batch 67 loss: 6.112956907600164e-05\n",
      "  batch 68 loss: 6.370361370500177e-05\n",
      "  batch 69 loss: 7.130891754059121e-05\n",
      "  batch 70 loss: 0.00012528880324680358\n",
      "  batch 71 loss: 7.78274770709686e-05\n",
      "  batch 72 loss: 7.714943785686046e-05\n",
      "  batch 73 loss: 8.062797132879496e-05\n",
      "  batch 74 loss: 0.00010685104643926024\n",
      "  batch 75 loss: 0.0001461824867874384\n",
      "  batch 76 loss: 8.909933967515826e-05\n",
      "  batch 77 loss: 8.38322303025052e-05\n",
      "  batch 78 loss: 8.933542994782329e-05\n",
      "  batch 79 loss: 6.8975321482867e-05\n",
      "  batch 80 loss: 8.857184730004519e-05\n",
      "  batch 81 loss: 0.0001094914332497865\n",
      "  batch 82 loss: 0.00011851501767523587\n",
      "  batch 83 loss: 0.00012694271572399884\n",
      "  batch 84 loss: 9.144886280409992e-05\n",
      "  batch 85 loss: 0.00010612456389935687\n",
      "  batch 86 loss: 0.00010937326442217454\n",
      "  batch 87 loss: 9.711596794659272e-05\n",
      "  batch 88 loss: 0.0001649714686209336\n",
      "  batch 89 loss: 0.00010534057946642861\n",
      "  batch 90 loss: 0.00016636319924145937\n",
      "  batch 91 loss: 9.8979908216279e-05\n",
      "  batch 92 loss: 0.00012755344505421817\n",
      "  batch 93 loss: 0.0001317804417340085\n",
      "  batch 94 loss: 0.00012325379066169262\n",
      "  batch 95 loss: 0.00018481933511793613\n",
      "LOSS train 0.00018481933511793613 valid 0.0013443280477076769\n",
      "LOSS train 0.00018481933511793613 valid 0.0015045476611703634\n",
      "LOSS train 0.00018481933511793613 valid 0.0013304081512615085\n",
      "LOSS train 0.00018481933511793613 valid 0.0012885348405689\n",
      "LOSS train 0.00018481933511793613 valid 0.0012887446209788322\n",
      "LOSS train 0.00018481933511793613 valid 0.001204074826091528\n",
      "LOSS train 0.00018481933511793613 valid 0.001279934193007648\n",
      "LOSS train 0.00018481933511793613 valid 0.0013395498972386122\n",
      "LOSS train 0.00018481933511793613 valid 0.0012974004494026303\n",
      "LOSS train 0.00018481933511793613 valid 0.0013245124137029052\n",
      "LOSS train 0.00018481933511793613 valid 0.001403258298523724\n",
      "LOSS train 0.00018481933511793613 valid 0.0014145963359624147\n",
      "LOSS train 0.00018481933511793613 valid 0.0014034874038770795\n",
      "LOSS train 0.00018481933511793613 valid 0.0014382863882929087\n",
      "LOSS train 0.00018481933511793613 valid 0.0015510295052081347\n",
      "LOSS train 0.00018481933511793613 valid 0.0016065535601228476\n",
      "LOSS train 0.00018481933511793613 valid 0.001612105406820774\n",
      "LOSS train 0.00018481933511793613 valid 0.001637259148992598\n",
      "LOSS train 0.00018481933511793613 valid 0.0016486556269228458\n",
      "LOSS train 0.00018481933511793613 valid 0.0016481176717206836\n",
      "LOSS train 0.00018481933511793613 valid 0.001621403032913804\n",
      "LOSS train 0.00018481933511793613 valid 0.0016420409083366394\n",
      "LOSS train 0.00018481933511793613 valid 0.0016368841752409935\n",
      "LOSS train 0.00018481933511793613 valid 0.0016627470031380653\n",
      "EPOCH 105:\n",
      "  batch 1 loss: 0.0001493668823968619\n",
      "  batch 2 loss: 8.138942939694971e-05\n",
      "  batch 3 loss: 0.00013821799075230956\n",
      "  batch 4 loss: 0.0001205368316732347\n",
      "  batch 5 loss: 0.00010473495785845444\n",
      "  batch 6 loss: 0.0001159964085672982\n",
      "  batch 7 loss: 7.93484432506375e-05\n",
      "  batch 8 loss: 8.910660835681483e-05\n",
      "  batch 9 loss: 0.00011306458327453583\n",
      "  batch 10 loss: 7.099172216840088e-05\n",
      "  batch 11 loss: 0.000119716816698201\n",
      "  batch 12 loss: 0.0001454335288144648\n",
      "  batch 13 loss: 0.00017624774773139507\n",
      "  batch 14 loss: 0.00026666373014450073\n",
      "  batch 15 loss: 0.0001594229252077639\n",
      "  batch 16 loss: 0.00010029968689195812\n",
      "  batch 17 loss: 9.232179581886157e-05\n",
      "  batch 18 loss: 9.225380199495703e-05\n",
      "  batch 19 loss: 0.0002343140367884189\n",
      "  batch 20 loss: 0.0001891482388600707\n",
      "  batch 21 loss: 0.00021813600324094296\n",
      "  batch 22 loss: 0.00012013837113045156\n",
      "  batch 23 loss: 0.00017885505803860724\n",
      "  batch 24 loss: 0.00015934657130856067\n",
      "  batch 25 loss: 0.00020395434694364667\n",
      "  batch 26 loss: 0.0002079442492686212\n",
      "  batch 27 loss: 9.122453775489703e-05\n",
      "  batch 28 loss: 0.0001287059421883896\n",
      "  batch 29 loss: 9.022174344863743e-05\n",
      "  batch 30 loss: 8.442997932434082e-05\n",
      "  batch 31 loss: 6.857104017399251e-05\n",
      "  batch 32 loss: 0.00011548911425052211\n",
      "  batch 33 loss: 0.0001048989943228662\n",
      "  batch 34 loss: 0.0001356455177301541\n",
      "  batch 35 loss: 7.703214942011982e-05\n",
      "  batch 36 loss: 7.366188947344199e-05\n",
      "  batch 37 loss: 9.912094537867233e-05\n",
      "  batch 38 loss: 9.145015792455524e-05\n",
      "  batch 39 loss: 0.00010988887515850365\n",
      "  batch 40 loss: 0.00011389012797735631\n",
      "  batch 41 loss: 6.220105569809675e-05\n",
      "  batch 42 loss: 9.49133827816695e-05\n",
      "  batch 43 loss: 6.433113594539464e-05\n",
      "  batch 44 loss: 5.2818431868217885e-05\n",
      "  batch 45 loss: 6.653211312368512e-05\n",
      "  batch 46 loss: 6.403521547326818e-05\n",
      "  batch 47 loss: 9.619788033887744e-05\n",
      "  batch 48 loss: 8.302523929160088e-05\n",
      "  batch 49 loss: 9.014940587803721e-05\n",
      "  batch 50 loss: 0.00010048344847746193\n",
      "  batch 51 loss: 0.00010917981853708625\n",
      "  batch 52 loss: 8.428923320025206e-05\n",
      "  batch 53 loss: 9.034590766532347e-05\n",
      "  batch 54 loss: 9.425089228898287e-05\n",
      "  batch 55 loss: 0.00012281379895284772\n",
      "  batch 56 loss: 7.852551061660051e-05\n",
      "  batch 57 loss: 7.551101589342579e-05\n",
      "  batch 58 loss: 9.228306589648128e-05\n",
      "  batch 59 loss: 8.0736746895127e-05\n",
      "  batch 60 loss: 0.00011955817899433896\n",
      "  batch 61 loss: 4.769560109707527e-05\n",
      "  batch 62 loss: 0.0001233382208738476\n",
      "  batch 63 loss: 7.822475163266063e-05\n",
      "  batch 64 loss: 7.657395326532423e-05\n",
      "  batch 65 loss: 7.886302773840725e-05\n",
      "  batch 66 loss: 7.689886842854321e-05\n",
      "  batch 67 loss: 6.679868965875357e-05\n",
      "  batch 68 loss: 7.407815428450704e-05\n",
      "  batch 69 loss: 7.686765457037836e-05\n",
      "  batch 70 loss: 0.00012109530507586896\n",
      "  batch 71 loss: 8.367656118934974e-05\n",
      "  batch 72 loss: 8.152543159667403e-05\n",
      "  batch 73 loss: 8.427788270637393e-05\n",
      "  batch 74 loss: 0.00011168495984748006\n",
      "  batch 75 loss: 0.0001508935820311308\n",
      "  batch 76 loss: 9.383022552356124e-05\n",
      "  batch 77 loss: 9.56089497776702e-05\n",
      "  batch 78 loss: 8.809618884697556e-05\n",
      "  batch 79 loss: 6.847512850072235e-05\n",
      "  batch 80 loss: 8.707909000804648e-05\n",
      "  batch 81 loss: 0.00011528869799803942\n",
      "  batch 82 loss: 0.00011734337022062391\n",
      "  batch 83 loss: 0.00012430640344973654\n",
      "  batch 84 loss: 0.00010253585787722841\n",
      "  batch 85 loss: 0.0001298952556680888\n",
      "  batch 86 loss: 0.00014186535554472357\n",
      "  batch 87 loss: 0.00010931861470453441\n",
      "  batch 88 loss: 0.0001700317661743611\n",
      "  batch 89 loss: 0.00010599706729408354\n",
      "  batch 90 loss: 0.00016351646627299488\n",
      "  batch 91 loss: 0.00011410805745981634\n",
      "  batch 92 loss: 0.0001455158635508269\n",
      "  batch 93 loss: 0.00014471117174252868\n",
      "  batch 94 loss: 0.00015279671060852706\n",
      "  batch 95 loss: 0.00021654499869327992\n",
      "LOSS train 0.00021654499869327992 valid 0.001361240865662694\n",
      "LOSS train 0.00021654499869327992 valid 0.0014565178425982594\n",
      "LOSS train 0.00021654499869327992 valid 0.0013759478460997343\n",
      "LOSS train 0.00021654499869327992 valid 0.0013083519879728556\n",
      "LOSS train 0.00021654499869327992 valid 0.0013005086220800877\n",
      "LOSS train 0.00021654499869327992 valid 0.0012337735388427973\n",
      "LOSS train 0.00021654499869327992 valid 0.0012641012435778975\n",
      "LOSS train 0.00021654499869327992 valid 0.0013220192631706595\n",
      "LOSS train 0.00021654499869327992 valid 0.0012906012125313282\n",
      "LOSS train 0.00021654499869327992 valid 0.001305907964706421\n",
      "LOSS train 0.00021654499869327992 valid 0.001353250234387815\n",
      "LOSS train 0.00021654499869327992 valid 0.0013667966704815626\n",
      "LOSS train 0.00021654499869327992 valid 0.0013743073213845491\n",
      "LOSS train 0.00021654499869327992 valid 0.0013993316097185016\n",
      "LOSS train 0.00021654499869327992 valid 0.0014805225655436516\n",
      "LOSS train 0.00021654499869327992 valid 0.0015449713682755828\n",
      "LOSS train 0.00021654499869327992 valid 0.001547849620692432\n",
      "LOSS train 0.00021654499869327992 valid 0.001556588220410049\n",
      "LOSS train 0.00021654499869327992 valid 0.0015565701760351658\n",
      "LOSS train 0.00021654499869327992 valid 0.0015471596270799637\n",
      "LOSS train 0.00021654499869327992 valid 0.0015262074302881956\n",
      "LOSS train 0.00021654499869327992 valid 0.0015426783356815577\n",
      "LOSS train 0.00021654499869327992 valid 0.0015300449449568987\n",
      "LOSS train 0.00021654499869327992 valid 0.0015241316286846995\n",
      "EPOCH 106:\n",
      "  batch 1 loss: 0.00016923993825912476\n",
      "  batch 2 loss: 0.00011675372661557049\n",
      "  batch 3 loss: 0.00014990198542363942\n",
      "  batch 4 loss: 0.0001169228125945665\n",
      "  batch 5 loss: 0.00010054420272354037\n",
      "  batch 6 loss: 0.00011405666737118736\n",
      "  batch 7 loss: 7.981702219694853e-05\n",
      "  batch 8 loss: 9.03019608813338e-05\n",
      "  batch 9 loss: 9.686948033049703e-05\n",
      "  batch 10 loss: 7.935220492072403e-05\n",
      "  batch 11 loss: 0.00011755988089134917\n",
      "  batch 12 loss: 0.00014045473653823137\n",
      "  batch 13 loss: 0.00016937617328949273\n",
      "  batch 14 loss: 0.00023792577849235386\n",
      "  batch 15 loss: 0.00016461039194837213\n",
      "  batch 16 loss: 9.267893619835377e-05\n",
      "  batch 17 loss: 9.214712190441787e-05\n",
      "  batch 18 loss: 8.672055264469236e-05\n",
      "  batch 19 loss: 0.00019344969769008458\n",
      "  batch 20 loss: 0.00013699552800972015\n",
      "  batch 21 loss: 0.0002214405540144071\n",
      "  batch 22 loss: 0.00012853879889007658\n",
      "  batch 23 loss: 0.0001929190184455365\n",
      "  batch 24 loss: 0.0001698893611319363\n",
      "  batch 25 loss: 0.00018882332369685173\n",
      "  batch 26 loss: 0.00019185901328455657\n",
      "  batch 27 loss: 7.544513937318698e-05\n",
      "  batch 28 loss: 0.00011742439528461546\n",
      "  batch 29 loss: 6.992850103415549e-05\n",
      "  batch 30 loss: 7.212700438685715e-05\n",
      "  batch 31 loss: 5.904183490201831e-05\n",
      "  batch 32 loss: 0.0001078003115253523\n",
      "  batch 33 loss: 9.771101758815348e-05\n",
      "  batch 34 loss: 0.0001228150649694726\n",
      "  batch 35 loss: 6.413831579266116e-05\n",
      "  batch 36 loss: 5.568501364905387e-05\n",
      "  batch 37 loss: 7.472730794688687e-05\n",
      "  batch 38 loss: 8.324580994667485e-05\n",
      "  batch 39 loss: 0.00010400370956631377\n",
      "  batch 40 loss: 0.00013697505346499383\n",
      "  batch 41 loss: 6.340399704640731e-05\n",
      "  batch 42 loss: 9.559077443554997e-05\n",
      "  batch 43 loss: 6.557705637533218e-05\n",
      "  batch 44 loss: 4.3020525481551886e-05\n",
      "  batch 45 loss: 5.364036405808292e-05\n",
      "  batch 46 loss: 4.7051929868757725e-05\n",
      "  batch 47 loss: 8.050038741203025e-05\n",
      "  batch 48 loss: 8.603584137745202e-05\n",
      "  batch 49 loss: 9.669129212852567e-05\n",
      "  batch 50 loss: 0.0001125196140492335\n",
      "  batch 51 loss: 0.00011655908019747585\n",
      "  batch 52 loss: 8.262154005933553e-05\n",
      "  batch 53 loss: 9.245739784091711e-05\n",
      "  batch 54 loss: 8.423349936492741e-05\n",
      "  batch 55 loss: 0.0001314262335654348\n",
      "  batch 56 loss: 7.966185512486845e-05\n",
      "  batch 57 loss: 7.092420128174126e-05\n",
      "  batch 58 loss: 8.768575935391709e-05\n",
      "  batch 59 loss: 6.969708192627877e-05\n",
      "  batch 60 loss: 0.00012278062058612704\n",
      "  batch 61 loss: 4.796968642040156e-05\n",
      "  batch 62 loss: 0.00011100633128080517\n",
      "  batch 63 loss: 7.408342935377732e-05\n",
      "  batch 64 loss: 6.671289884252474e-05\n",
      "  batch 65 loss: 8.615068509243429e-05\n",
      "  batch 66 loss: 7.222354179248214e-05\n",
      "  batch 67 loss: 6.77450152579695e-05\n",
      "  batch 68 loss: 8.595203689765185e-05\n",
      "  batch 69 loss: 9.26880311453715e-05\n",
      "  batch 70 loss: 0.00012095452984794974\n",
      "  batch 71 loss: 7.765529880998656e-05\n",
      "  batch 72 loss: 7.554220064776018e-05\n",
      "  batch 73 loss: 7.955913315527141e-05\n",
      "  batch 74 loss: 9.065422636922449e-05\n",
      "  batch 75 loss: 0.00013220933033153415\n",
      "  batch 76 loss: 8.307317330036312e-05\n",
      "  batch 77 loss: 0.00010205183934886009\n",
      "  batch 78 loss: 9.594836592441425e-05\n",
      "  batch 79 loss: 7.966005796333775e-05\n",
      "  batch 80 loss: 9.853236406343058e-05\n",
      "  batch 81 loss: 0.00010574793122941628\n",
      "  batch 82 loss: 0.00010917869803961366\n",
      "  batch 83 loss: 0.00011371338041499257\n",
      "  batch 84 loss: 9.449310891795903e-05\n",
      "  batch 85 loss: 0.00010602595284581184\n",
      "  batch 86 loss: 0.00012923046597279608\n",
      "  batch 87 loss: 0.00012970856914762408\n",
      "  batch 88 loss: 0.00019853161938954145\n",
      "  batch 89 loss: 0.00011735019506886601\n",
      "  batch 90 loss: 0.00016356309060938656\n",
      "  batch 91 loss: 9.91146735032089e-05\n",
      "  batch 92 loss: 0.00011732328857760876\n",
      "  batch 93 loss: 0.00011762959911720827\n",
      "  batch 94 loss: 0.00012682982196565717\n",
      "  batch 95 loss: 0.00019075239833910018\n",
      "LOSS train 0.00019075239833910018 valid 0.0013169923331588507\n",
      "LOSS train 0.00019075239833910018 valid 0.0014005005359649658\n",
      "LOSS train 0.00019075239833910018 valid 0.0014175178948789835\n",
      "LOSS train 0.00019075239833910018 valid 0.0013438888126984239\n",
      "LOSS train 0.00019075239833910018 valid 0.0013313561212271452\n",
      "LOSS train 0.00019075239833910018 valid 0.0012322419788688421\n",
      "LOSS train 0.00019075239833910018 valid 0.0012612341670319438\n",
      "LOSS train 0.00019075239833910018 valid 0.0013067512772977352\n",
      "LOSS train 0.00019075239833910018 valid 0.001267142128199339\n",
      "LOSS train 0.00019075239833910018 valid 0.0012801297707483172\n",
      "LOSS train 0.00019075239833910018 valid 0.0013069899287074804\n",
      "LOSS train 0.00019075239833910018 valid 0.001316798385232687\n",
      "LOSS train 0.00019075239833910018 valid 0.0013306643813848495\n",
      "LOSS train 0.00019075239833910018 valid 0.0013479827903211117\n",
      "LOSS train 0.00019075239833910018 valid 0.001394087914377451\n",
      "LOSS train 0.00019075239833910018 valid 0.001449397299438715\n",
      "LOSS train 0.00019075239833910018 valid 0.0014554850058630109\n",
      "LOSS train 0.00019075239833910018 valid 0.0014671203680336475\n",
      "LOSS train 0.00019075239833910018 valid 0.0014734033029526472\n",
      "LOSS train 0.00019075239833910018 valid 0.0014724445063620806\n",
      "LOSS train 0.00019075239833910018 valid 0.0014602015726268291\n",
      "LOSS train 0.00019075239833910018 valid 0.0014828576240688562\n",
      "LOSS train 0.00019075239833910018 valid 0.0014701725449413061\n",
      "LOSS train 0.00019075239833910018 valid 0.001467698602937162\n",
      "EPOCH 107:\n",
      "  batch 1 loss: 0.00016945514653343707\n",
      "  batch 2 loss: 0.00013484968803822994\n",
      "  batch 3 loss: 0.00019584278925321996\n",
      "  batch 4 loss: 0.00013951759319752455\n",
      "  batch 5 loss: 0.00010465776722412556\n",
      "  batch 6 loss: 0.00010003593342844397\n",
      "  batch 7 loss: 7.785682828398421e-05\n",
      "  batch 8 loss: 8.528289617970586e-05\n",
      "  batch 9 loss: 0.0001008812541840598\n",
      "  batch 10 loss: 7.488331903005019e-05\n",
      "  batch 11 loss: 0.00011092782369814813\n",
      "  batch 12 loss: 0.00013771397061645985\n",
      "  batch 13 loss: 0.00017201335867866874\n",
      "  batch 14 loss: 0.00025030638789758086\n",
      "  batch 15 loss: 0.00017076941730920225\n",
      "  batch 16 loss: 0.00010342126915929839\n",
      "  batch 17 loss: 9.1449728643056e-05\n",
      "  batch 18 loss: 0.00010088382987305522\n",
      "  batch 19 loss: 0.00018781973631121218\n",
      "  batch 20 loss: 0.00012768199667334557\n",
      "  batch 21 loss: 0.00018416786042507738\n",
      "  batch 22 loss: 0.00011305205407552421\n",
      "  batch 23 loss: 0.00017206821939907968\n",
      "  batch 24 loss: 0.00016830055392347276\n",
      "  batch 25 loss: 0.00022143579553812742\n",
      "  batch 26 loss: 0.00020646982011385262\n",
      "  batch 27 loss: 8.409151632804424e-05\n",
      "  batch 28 loss: 0.00011812912998721004\n",
      "  batch 29 loss: 6.821815622970462e-05\n",
      "  batch 30 loss: 6.173434667289257e-05\n",
      "  batch 31 loss: 5.403204704634845e-05\n",
      "  batch 32 loss: 0.00010700714483391494\n",
      "  batch 33 loss: 0.00010680632840376347\n",
      "  batch 34 loss: 0.00013321888400241733\n",
      "  batch 35 loss: 5.517894896911457e-05\n",
      "  batch 36 loss: 5.6230201153084636e-05\n",
      "  batch 37 loss: 6.796800153097138e-05\n",
      "  batch 38 loss: 6.860779103590176e-05\n",
      "  batch 39 loss: 9.092057007364929e-05\n",
      "  batch 40 loss: 0.0001248060434591025\n",
      "  batch 41 loss: 6.626035610679537e-05\n",
      "  batch 42 loss: 0.00010523270611884072\n",
      "  batch 43 loss: 6.634824239881709e-05\n",
      "  batch 44 loss: 4.9962414777837694e-05\n",
      "  batch 45 loss: 5.3350915550254285e-05\n",
      "  batch 46 loss: 4.9467169446870685e-05\n",
      "  batch 47 loss: 7.130258018150926e-05\n",
      "  batch 48 loss: 7.200344407465309e-05\n",
      "  batch 49 loss: 7.81470735091716e-05\n",
      "  batch 50 loss: 9.360765398014337e-05\n",
      "  batch 51 loss: 9.887637861538678e-05\n",
      "  batch 52 loss: 8.028455340536311e-05\n",
      "  batch 53 loss: 8.830270962789655e-05\n",
      "  batch 54 loss: 8.133285155054182e-05\n",
      "  batch 55 loss: 0.0001198548125103116\n",
      "  batch 56 loss: 7.224021828733385e-05\n",
      "  batch 57 loss: 5.663724732585251e-05\n",
      "  batch 58 loss: 7.991696475073695e-05\n",
      "  batch 59 loss: 6.606451643165201e-05\n",
      "  batch 60 loss: 0.00013774522813037038\n",
      "  batch 61 loss: 5.453607809613459e-05\n",
      "  batch 62 loss: 0.00011491229815874249\n",
      "  batch 63 loss: 6.820095586590469e-05\n",
      "  batch 64 loss: 5.32023113919422e-05\n",
      "  batch 65 loss: 5.6441880587954074e-05\n",
      "  batch 66 loss: 5.699450775864534e-05\n",
      "  batch 67 loss: 5.1795406761812046e-05\n",
      "  batch 68 loss: 7.718525012023747e-05\n",
      "  batch 69 loss: 7.99661866039969e-05\n",
      "  batch 70 loss: 0.0001227972243214026\n",
      "  batch 71 loss: 8.547447214368731e-05\n",
      "  batch 72 loss: 7.583874685224146e-05\n",
      "  batch 73 loss: 7.213397475425154e-05\n",
      "  batch 74 loss: 9.289183071814477e-05\n",
      "  batch 75 loss: 0.0001353010447928682\n",
      "  batch 76 loss: 7.587060827063397e-05\n",
      "  batch 77 loss: 9.318463708041236e-05\n",
      "  batch 78 loss: 9.382488497067243e-05\n",
      "  batch 79 loss: 8.584858733229339e-05\n",
      "  batch 80 loss: 9.441089059691876e-05\n",
      "  batch 81 loss: 0.00011105665180366486\n",
      "  batch 82 loss: 0.00011107150930911303\n",
      "  batch 83 loss: 0.00011173315579071641\n",
      "  batch 84 loss: 7.064723467919976e-05\n",
      "  batch 85 loss: 8.537990652257577e-05\n",
      "  batch 86 loss: 0.0001001097698463127\n",
      "  batch 87 loss: 0.00010753462265711278\n",
      "  batch 88 loss: 0.00017323950305581093\n",
      "  batch 89 loss: 0.00011531596828717738\n",
      "  batch 90 loss: 0.00015629843983333558\n",
      "  batch 91 loss: 0.00010922364890575409\n",
      "  batch 92 loss: 0.0001153185949078761\n",
      "  batch 93 loss: 0.00011111468484159559\n",
      "  batch 94 loss: 0.00012031767255393788\n",
      "  batch 95 loss: 0.00017460000526625663\n",
      "LOSS train 0.00017460000526625663 valid 0.0011359464842826128\n",
      "LOSS train 0.00017460000526625663 valid 0.0012900290312245488\n",
      "LOSS train 0.00017460000526625663 valid 0.0011723958887159824\n",
      "LOSS train 0.00017460000526625663 valid 0.0011289266403764486\n",
      "LOSS train 0.00017460000526625663 valid 0.0011303286300972104\n",
      "LOSS train 0.00017460000526625663 valid 0.0010415923316031694\n",
      "LOSS train 0.00017460000526625663 valid 0.0011262688785791397\n",
      "LOSS train 0.00017460000526625663 valid 0.0011759881163015962\n",
      "LOSS train 0.00017460000526625663 valid 0.0011328940745443106\n",
      "LOSS train 0.00017460000526625663 valid 0.0011639193398877978\n",
      "LOSS train 0.00017460000526625663 valid 0.0012232315493747592\n",
      "LOSS train 0.00017460000526625663 valid 0.0012363018468022346\n",
      "LOSS train 0.00017460000526625663 valid 0.0012203907826915383\n",
      "LOSS train 0.00017460000526625663 valid 0.0012438909616321325\n",
      "LOSS train 0.00017460000526625663 valid 0.001329950988292694\n",
      "LOSS train 0.00017460000526625663 valid 0.001379332272335887\n",
      "LOSS train 0.00017460000526625663 valid 0.0013827766524627805\n",
      "LOSS train 0.00017460000526625663 valid 0.0014008384896442294\n",
      "LOSS train 0.00017460000526625663 valid 0.001412432175129652\n",
      "LOSS train 0.00017460000526625663 valid 0.0014138774713501334\n",
      "LOSS train 0.00017460000526625663 valid 0.001394307822920382\n",
      "LOSS train 0.00017460000526625663 valid 0.0014089877950027585\n",
      "LOSS train 0.00017460000526625663 valid 0.0014017067151144147\n",
      "LOSS train 0.00017460000526625663 valid 0.001424391521140933\n",
      "EPOCH 108:\n",
      "  batch 1 loss: 0.00015798807726241648\n",
      "  batch 2 loss: 9.886569023365155e-05\n",
      "  batch 3 loss: 0.0001523594546597451\n",
      "  batch 4 loss: 0.00014070727047510445\n",
      "  batch 5 loss: 0.00011433627514634281\n",
      "  batch 6 loss: 0.00011510105832712725\n",
      "  batch 7 loss: 9.311965550296009e-05\n",
      "  batch 8 loss: 9.65576182352379e-05\n",
      "  batch 9 loss: 0.00013074616435915232\n",
      "  batch 10 loss: 7.379498856607825e-05\n",
      "  batch 11 loss: 0.0001367621443932876\n",
      "  batch 12 loss: 0.00014827947597950697\n",
      "  batch 13 loss: 0.0001609654282219708\n",
      "  batch 14 loss: 0.00022772967349737883\n",
      "  batch 15 loss: 0.00016285828314721584\n",
      "  batch 16 loss: 0.00010856855078600347\n",
      "  batch 17 loss: 0.00010832519910763949\n",
      "  batch 18 loss: 0.0001144809793913737\n",
      "  batch 19 loss: 0.00021261603978928179\n",
      "  batch 20 loss: 0.00014274832210503519\n",
      "  batch 21 loss: 0.00018200126942247152\n",
      "  batch 22 loss: 0.00011190029908902943\n",
      "  batch 23 loss: 0.00015268931747414172\n",
      "  batch 24 loss: 0.00014341840869747102\n",
      "  batch 25 loss: 0.0002059080288745463\n",
      "  batch 26 loss: 0.00020858093921560794\n",
      "  batch 27 loss: 7.184635614976287e-05\n",
      "  batch 28 loss: 0.00011856385390274227\n",
      "  batch 29 loss: 6.659651990048587e-05\n",
      "  batch 30 loss: 5.941030030953698e-05\n",
      "  batch 31 loss: 5.156311090104282e-05\n",
      "  batch 32 loss: 9.881389269139618e-05\n",
      "  batch 33 loss: 9.968387894332409e-05\n",
      "  batch 34 loss: 0.00012125198554713279\n",
      "  batch 35 loss: 5.760855128755793e-05\n",
      "  batch 36 loss: 6.778804527129978e-05\n",
      "  batch 37 loss: 6.90946908434853e-05\n",
      "  batch 38 loss: 7.04247213434428e-05\n",
      "  batch 39 loss: 9.258248610422015e-05\n",
      "  batch 40 loss: 0.00015660037752240896\n",
      "  batch 41 loss: 5.561753278016113e-05\n",
      "  batch 42 loss: 9.065368794836104e-05\n",
      "  batch 43 loss: 5.930793849984184e-05\n",
      "  batch 44 loss: 4.678905679611489e-05\n",
      "  batch 45 loss: 5.135738319950178e-05\n",
      "  batch 46 loss: 4.6611312427558005e-05\n",
      "  batch 47 loss: 6.854813545942307e-05\n",
      "  batch 48 loss: 7.467526302207261e-05\n",
      "  batch 49 loss: 8.43418893055059e-05\n",
      "  batch 50 loss: 9.82897327048704e-05\n",
      "  batch 51 loss: 8.741448982618749e-05\n",
      "  batch 52 loss: 7.495213503716514e-05\n",
      "  batch 53 loss: 7.584732520626858e-05\n",
      "  batch 54 loss: 7.52242558519356e-05\n",
      "  batch 55 loss: 0.00010894282604567707\n",
      "  batch 56 loss: 6.301931716734543e-05\n",
      "  batch 57 loss: 5.729747499572113e-05\n",
      "  batch 58 loss: 7.28186932974495e-05\n",
      "  batch 59 loss: 6.415061943698674e-05\n",
      "  batch 60 loss: 0.00011529425682965666\n",
      "  batch 61 loss: 4.329793591750786e-05\n",
      "  batch 62 loss: 0.00010138631478184834\n",
      "  batch 63 loss: 6.322241097223014e-05\n",
      "  batch 64 loss: 5.600504664471373e-05\n",
      "  batch 65 loss: 6.728246808052063e-05\n",
      "  batch 66 loss: 5.71838318137452e-05\n",
      "  batch 67 loss: 4.939109567203559e-05\n",
      "  batch 68 loss: 6.479673902504146e-05\n",
      "  batch 69 loss: 6.715048948535696e-05\n",
      "  batch 70 loss: 0.0001168463786598295\n",
      "  batch 71 loss: 8.441699901595712e-05\n",
      "  batch 72 loss: 7.004150393186137e-05\n",
      "  batch 73 loss: 6.611605931539088e-05\n",
      "  batch 74 loss: 8.32162331789732e-05\n",
      "  batch 75 loss: 0.00010752398520708084\n",
      "  batch 76 loss: 6.420013960450888e-05\n",
      "  batch 77 loss: 7.598711817990988e-05\n",
      "  batch 78 loss: 7.407686644000933e-05\n",
      "  batch 79 loss: 6.424215098377317e-05\n",
      "  batch 80 loss: 8.22667934698984e-05\n",
      "  batch 81 loss: 9.629337000660598e-05\n",
      "  batch 82 loss: 9.345734724774957e-05\n",
      "  batch 83 loss: 9.792493801796809e-05\n",
      "  batch 84 loss: 6.409647176042199e-05\n",
      "  batch 85 loss: 7.799145532771945e-05\n",
      "  batch 86 loss: 8.659582817927003e-05\n",
      "  batch 87 loss: 9.112743282457814e-05\n",
      "  batch 88 loss: 0.00015028772759251297\n",
      "  batch 89 loss: 0.00010167239815928042\n",
      "  batch 90 loss: 0.00013690022751688957\n",
      "  batch 91 loss: 8.587689080741256e-05\n",
      "  batch 92 loss: 9.482757013756782e-05\n",
      "  batch 93 loss: 9.540902101434767e-05\n",
      "  batch 94 loss: 9.525871428195387e-05\n",
      "  batch 95 loss: 0.0001587323349667713\n",
      "LOSS train 0.0001587323349667713 valid 0.0013003360945731401\n",
      "LOSS train 0.0001587323349667713 valid 0.001403774949721992\n",
      "LOSS train 0.0001587323349667713 valid 0.0013311586808413267\n",
      "LOSS train 0.0001587323349667713 valid 0.001304709818214178\n",
      "LOSS train 0.0001587323349667713 valid 0.001307614496909082\n",
      "LOSS train 0.0001587323349667713 valid 0.0012136735022068024\n",
      "LOSS train 0.0001587323349667713 valid 0.001262623816728592\n",
      "LOSS train 0.0001587323349667713 valid 0.0013173407642170787\n",
      "LOSS train 0.0001587323349667713 valid 0.0012874992098659277\n",
      "LOSS train 0.0001587323349667713 valid 0.001312910346314311\n",
      "LOSS train 0.0001587323349667713 valid 0.0013545835390686989\n",
      "LOSS train 0.0001587323349667713 valid 0.0013706011231988668\n",
      "LOSS train 0.0001587323349667713 valid 0.001369969453662634\n",
      "LOSS train 0.0001587323349667713 valid 0.0013939491473138332\n",
      "LOSS train 0.0001587323349667713 valid 0.0014715609140694141\n",
      "LOSS train 0.0001587323349667713 valid 0.0015325627755373716\n",
      "LOSS train 0.0001587323349667713 valid 0.001543998601846397\n",
      "LOSS train 0.0001587323349667713 valid 0.0015563121996819973\n",
      "LOSS train 0.0001587323349667713 valid 0.0015556204598397017\n",
      "LOSS train 0.0001587323349667713 valid 0.0015522718895226717\n",
      "LOSS train 0.0001587323349667713 valid 0.001531005254946649\n",
      "LOSS train 0.0001587323349667713 valid 0.0015464021125808358\n",
      "LOSS train 0.0001587323349667713 valid 0.001535031944513321\n",
      "LOSS train 0.0001587323349667713 valid 0.0015454727690666914\n",
      "EPOCH 109:\n",
      "  batch 1 loss: 0.00016196064825635403\n",
      "  batch 2 loss: 8.891035395208746e-05\n",
      "  batch 3 loss: 0.00014622874732594937\n",
      "  batch 4 loss: 0.00013898291217628866\n",
      "  batch 5 loss: 0.00010005917283706367\n",
      "  batch 6 loss: 9.602945647202432e-05\n",
      "  batch 7 loss: 7.086990808602422e-05\n",
      "  batch 8 loss: 7.699528941884637e-05\n",
      "  batch 9 loss: 8.945654553826898e-05\n",
      "  batch 10 loss: 5.4054624342825264e-05\n",
      "  batch 11 loss: 0.00010905516683124006\n",
      "  batch 12 loss: 0.0001295240654144436\n",
      "  batch 13 loss: 0.00016773160314187407\n",
      "  batch 14 loss: 0.0002463277487549931\n",
      "  batch 15 loss: 0.00015528271615039557\n",
      "  batch 16 loss: 8.92414027475752e-05\n",
      "  batch 17 loss: 8.733384311199188e-05\n",
      "  batch 18 loss: 9.59367462201044e-05\n",
      "  batch 19 loss: 0.0001966084528248757\n",
      "  batch 20 loss: 0.00013924200902692974\n",
      "  batch 21 loss: 0.00018502002058085054\n",
      "  batch 22 loss: 0.0001151051910710521\n",
      "  batch 23 loss: 0.00018369435565546155\n",
      "  batch 24 loss: 0.00015408569015562534\n",
      "  batch 25 loss: 0.00018630120030138642\n",
      "  batch 26 loss: 0.00021957499848213047\n",
      "  batch 27 loss: 7.60281618568115e-05\n",
      "  batch 28 loss: 0.00011341307981638238\n",
      "  batch 29 loss: 6.625369132962078e-05\n",
      "  batch 30 loss: 5.45479852007702e-05\n",
      "  batch 31 loss: 5.158994099474512e-05\n",
      "  batch 32 loss: 0.00010336404375266284\n",
      "  batch 33 loss: 0.00010941455548163503\n",
      "  batch 34 loss: 0.00012647395487874746\n",
      "  batch 35 loss: 5.42882080480922e-05\n",
      "  batch 36 loss: 5.996298204991035e-05\n",
      "  batch 37 loss: 6.626312097068876e-05\n",
      "  batch 38 loss: 6.922656029928476e-05\n",
      "  batch 39 loss: 9.086157660931349e-05\n",
      "  batch 40 loss: 0.00011554975208127871\n",
      "  batch 41 loss: 5.164720641914755e-05\n",
      "  batch 42 loss: 8.849747973727062e-05\n",
      "  batch 43 loss: 6.207974365679547e-05\n",
      "  batch 44 loss: 4.439242911757901e-05\n",
      "  batch 45 loss: 5.094857624499127e-05\n",
      "  batch 46 loss: 4.4037020415998995e-05\n",
      "  batch 47 loss: 6.260867667151615e-05\n",
      "  batch 48 loss: 6.83994876453653e-05\n",
      "  batch 49 loss: 7.83419527579099e-05\n",
      "  batch 50 loss: 9.165485244011506e-05\n",
      "  batch 51 loss: 8.73874596436508e-05\n",
      "  batch 52 loss: 8.122981671476737e-05\n",
      "  batch 53 loss: 7.617822848260403e-05\n",
      "  batch 54 loss: 7.252864452311769e-05\n",
      "  batch 55 loss: 0.0001033078006003052\n",
      "  batch 56 loss: 6.129304529167712e-05\n",
      "  batch 57 loss: 4.783046824741177e-05\n",
      "  batch 58 loss: 6.151640263851732e-05\n",
      "  batch 59 loss: 6.083667540224269e-05\n",
      "  batch 60 loss: 0.00010582238610368222\n",
      "  batch 61 loss: 3.410884164622985e-05\n",
      "  batch 62 loss: 8.721675840206444e-05\n",
      "  batch 63 loss: 5.545829844777472e-05\n",
      "  batch 64 loss: 4.498712587519549e-05\n",
      "  batch 65 loss: 5.010542372474447e-05\n",
      "  batch 66 loss: 5.174524267204106e-05\n",
      "  batch 67 loss: 4.4217475078767166e-05\n",
      "  batch 68 loss: 5.821296508656815e-05\n",
      "  batch 69 loss: 6.0007958381902426e-05\n",
      "  batch 70 loss: 0.00010182351979892701\n",
      "  batch 71 loss: 7.35287248971872e-05\n",
      "  batch 72 loss: 5.9865484217880294e-05\n",
      "  batch 73 loss: 6.210969149833545e-05\n",
      "  batch 74 loss: 7.892514986451715e-05\n",
      "  batch 75 loss: 0.00010152265895158052\n",
      "  batch 76 loss: 5.673823761753738e-05\n",
      "  batch 77 loss: 6.724218837916851e-05\n",
      "  batch 78 loss: 6.070785821066238e-05\n",
      "  batch 79 loss: 4.647499736165628e-05\n",
      "  batch 80 loss: 6.722312537021935e-05\n",
      "  batch 81 loss: 7.962500967551023e-05\n",
      "  batch 82 loss: 7.836523582227528e-05\n",
      "  batch 83 loss: 9.543739724904299e-05\n",
      "  batch 84 loss: 5.7800920330919325e-05\n",
      "  batch 85 loss: 6.72520836815238e-05\n",
      "  batch 86 loss: 7.526146509917453e-05\n",
      "  batch 87 loss: 7.805313362041488e-05\n",
      "  batch 88 loss: 0.000135949463583529\n",
      "  batch 89 loss: 8.543964941054583e-05\n",
      "  batch 90 loss: 0.0001312612439505756\n",
      "  batch 91 loss: 7.259469566633925e-05\n",
      "  batch 92 loss: 8.948172762757167e-05\n",
      "  batch 93 loss: 8.986572356661782e-05\n",
      "  batch 94 loss: 8.839590009301901e-05\n",
      "  batch 95 loss: 0.00015621756028849632\n",
      "LOSS train 0.00015621756028849632 valid 0.0014067513402551413\n",
      "LOSS train 0.00015621756028849632 valid 0.0015359658282250166\n",
      "LOSS train 0.00015621756028849632 valid 0.0014468368608504534\n",
      "LOSS train 0.00015621756028849632 valid 0.0014122271677479148\n",
      "LOSS train 0.00015621756028849632 valid 0.0014126923633739352\n",
      "LOSS train 0.00015621756028849632 valid 0.0013122957898303866\n",
      "LOSS train 0.00015621756028849632 valid 0.0013606493594124913\n",
      "LOSS train 0.00015621756028849632 valid 0.0014172961236909032\n",
      "LOSS train 0.00015621756028849632 valid 0.0013896103482693434\n",
      "LOSS train 0.00015621756028849632 valid 0.0014143238076940179\n",
      "LOSS train 0.00015621756028849632 valid 0.0014783258084207773\n",
      "LOSS train 0.00015621756028849632 valid 0.0014883338008075953\n",
      "LOSS train 0.00015621756028849632 valid 0.0014929124154150486\n",
      "LOSS train 0.00015621756028849632 valid 0.0015216782921925187\n",
      "LOSS train 0.00015621756028849632 valid 0.0016155625926330686\n",
      "LOSS train 0.00015621756028849632 valid 0.001684653339907527\n",
      "LOSS train 0.00015621756028849632 valid 0.0016928582917898893\n",
      "LOSS train 0.00015621756028849632 valid 0.0017074543284252286\n",
      "LOSS train 0.00015621756028849632 valid 0.0017126461025327444\n",
      "LOSS train 0.00015621756028849632 valid 0.001708931289613247\n",
      "LOSS train 0.00015621756028849632 valid 0.0016843933844938874\n",
      "LOSS train 0.00015621756028849632 valid 0.0017034756019711494\n",
      "LOSS train 0.00015621756028849632 valid 0.0016892915591597557\n",
      "LOSS train 0.00015621756028849632 valid 0.0017005030531436205\n",
      "EPOCH 110:\n",
      "  batch 1 loss: 0.0001291517837671563\n",
      "  batch 2 loss: 7.125923002604395e-05\n",
      "  batch 3 loss: 0.0001206558954436332\n",
      "  batch 4 loss: 0.00011257438745815307\n",
      "  batch 5 loss: 9.413540101377293e-05\n",
      "  batch 6 loss: 0.00010647070303093642\n",
      "  batch 7 loss: 6.737784133292735e-05\n",
      "  batch 8 loss: 7.703932351432741e-05\n",
      "  batch 9 loss: 9.603457147022709e-05\n",
      "  batch 10 loss: 4.7437119064852595e-05\n",
      "  batch 11 loss: 8.120774145936593e-05\n",
      "  batch 12 loss: 0.00010494080925127491\n",
      "  batch 13 loss: 0.00014129665214568377\n",
      "  batch 14 loss: 0.0002388486173003912\n",
      "  batch 15 loss: 0.00014206173364073038\n",
      "  batch 16 loss: 0.00010112298332387581\n",
      "  batch 17 loss: 8.934146899264306e-05\n",
      "  batch 18 loss: 9.402517025591806e-05\n",
      "  batch 19 loss: 0.00023450181470252573\n",
      "  batch 20 loss: 0.0001413846475770697\n",
      "  batch 21 loss: 0.0001902909716591239\n",
      "  batch 22 loss: 0.000104504608316347\n",
      "  batch 23 loss: 0.00016363362374249846\n",
      "  batch 24 loss: 0.00015817498206160963\n",
      "  batch 25 loss: 0.0001845087535912171\n",
      "  batch 26 loss: 0.00018442959117237478\n",
      "  batch 27 loss: 8.263249765150249e-05\n",
      "  batch 28 loss: 0.00013344149920158088\n",
      "  batch 29 loss: 8.973258809419349e-05\n",
      "  batch 30 loss: 6.660734652541578e-05\n",
      "  batch 31 loss: 5.18587876285892e-05\n",
      "  batch 32 loss: 8.5089573985897e-05\n",
      "  batch 33 loss: 8.889927994459867e-05\n",
      "  batch 34 loss: 0.00010804003977682441\n",
      "  batch 35 loss: 5.047750164521858e-05\n",
      "  batch 36 loss: 5.3926982218399644e-05\n",
      "  batch 37 loss: 6.697482604067773e-05\n",
      "  batch 38 loss: 7.702048606006429e-05\n",
      "  batch 39 loss: 9.708369907457381e-05\n",
      "  batch 40 loss: 0.00011918068048544228\n",
      "  batch 41 loss: 5.4875275964150205e-05\n",
      "  batch 42 loss: 8.74803081387654e-05\n",
      "  batch 43 loss: 5.779364437330514e-05\n",
      "  batch 44 loss: 3.5262477467767894e-05\n",
      "  batch 45 loss: 4.380070822662674e-05\n",
      "  batch 46 loss: 4.019859625259414e-05\n",
      "  batch 47 loss: 5.8991292462451383e-05\n",
      "  batch 48 loss: 6.598189065698534e-05\n",
      "  batch 49 loss: 7.835387805243954e-05\n",
      "  batch 50 loss: 9.064333426067606e-05\n",
      "  batch 51 loss: 8.687375520821661e-05\n",
      "  batch 52 loss: 7.415037543978542e-05\n",
      "  batch 53 loss: 7.399766764137894e-05\n",
      "  batch 54 loss: 7.161195389926434e-05\n",
      "  batch 55 loss: 0.00010347005445510149\n",
      "  batch 56 loss: 6.068949369364418e-05\n",
      "  batch 57 loss: 5.195030098548159e-05\n",
      "  batch 58 loss: 7.179075328167528e-05\n",
      "  batch 59 loss: 5.5410375352948904e-05\n",
      "  batch 60 loss: 0.0001069812715286389\n",
      "  batch 61 loss: 3.6053497751709074e-05\n",
      "  batch 62 loss: 8.734622679185122e-05\n",
      "  batch 63 loss: 5.795104516437277e-05\n",
      "  batch 64 loss: 4.686786269303411e-05\n",
      "  batch 65 loss: 5.4522959544556215e-05\n",
      "  batch 66 loss: 5.264683932182379e-05\n",
      "  batch 67 loss: 4.238504698150791e-05\n",
      "  batch 68 loss: 5.4252588597591966e-05\n",
      "  batch 69 loss: 5.9382822655607015e-05\n",
      "  batch 70 loss: 9.756049257703125e-05\n",
      "  batch 71 loss: 6.387133907992393e-05\n",
      "  batch 72 loss: 5.944118311163038e-05\n",
      "  batch 73 loss: 6.382162973750383e-05\n",
      "  batch 74 loss: 7.76347442297265e-05\n",
      "  batch 75 loss: 0.0001028920742101036\n",
      "  batch 76 loss: 5.519492697203532e-05\n",
      "  batch 77 loss: 6.590554403373972e-05\n",
      "  batch 78 loss: 6.098453377489932e-05\n",
      "  batch 79 loss: 4.356916906544939e-05\n",
      "  batch 80 loss: 6.006644252920523e-05\n",
      "  batch 81 loss: 7.349281804636121e-05\n",
      "  batch 82 loss: 7.359318260569125e-05\n",
      "  batch 83 loss: 8.935597725212574e-05\n",
      "  batch 84 loss: 5.6851837143767625e-05\n",
      "  batch 85 loss: 6.56235424685292e-05\n",
      "  batch 86 loss: 7.457916217390448e-05\n",
      "  batch 87 loss: 7.496926991734654e-05\n",
      "  batch 88 loss: 0.00013310974463820457\n",
      "  batch 89 loss: 8.07475735200569e-05\n",
      "  batch 90 loss: 0.00012022115697618574\n",
      "  batch 91 loss: 6.82436439092271e-05\n",
      "  batch 92 loss: 8.720151527086273e-05\n",
      "  batch 93 loss: 9.072092507267371e-05\n",
      "  batch 94 loss: 8.795083704171702e-05\n",
      "  batch 95 loss: 0.00015256497135851532\n",
      "LOSS train 0.00015256497135851532 valid 0.0013596001081168652\n",
      "LOSS train 0.00015256497135851532 valid 0.001470858696848154\n",
      "LOSS train 0.00015256497135851532 valid 0.001419929787516594\n",
      "LOSS train 0.00015256497135851532 valid 0.0013819568557664752\n",
      "LOSS train 0.00015256497135851532 valid 0.001390004763379693\n",
      "LOSS train 0.00015256497135851532 valid 0.0012895254185423255\n",
      "LOSS train 0.00015256497135851532 valid 0.0013286421308293939\n",
      "LOSS train 0.00015256497135851532 valid 0.0013810747768729925\n",
      "LOSS train 0.00015256497135851532 valid 0.0013508473057299852\n",
      "LOSS train 0.00015256497135851532 valid 0.001371082616969943\n",
      "LOSS train 0.00015256497135851532 valid 0.0014208639040589333\n",
      "LOSS train 0.00015256497135851532 valid 0.0014361765934154391\n",
      "LOSS train 0.00015256497135851532 valid 0.0014425648842006922\n",
      "LOSS train 0.00015256497135851532 valid 0.0014684632187709212\n",
      "LOSS train 0.00015256497135851532 valid 0.0015551104443147779\n",
      "LOSS train 0.00015256497135851532 valid 0.0016205902211368084\n",
      "LOSS train 0.00015256497135851532 valid 0.0016315239481627941\n",
      "LOSS train 0.00015256497135851532 valid 0.001641969196498394\n",
      "LOSS train 0.00015256497135851532 valid 0.0016424778150394559\n",
      "LOSS train 0.00015256497135851532 valid 0.0016349883517250419\n",
      "LOSS train 0.00015256497135851532 valid 0.0016134732868522406\n",
      "LOSS train 0.00015256497135851532 valid 0.0016312531661242247\n",
      "LOSS train 0.00015256497135851532 valid 0.0016205678693950176\n",
      "LOSS train 0.00015256497135851532 valid 0.001624937984161079\n",
      "EPOCH 111:\n",
      "  batch 1 loss: 0.00013195966312196106\n",
      "  batch 2 loss: 6.697956268908456e-05\n",
      "  batch 3 loss: 0.00011564664600882679\n",
      "  batch 4 loss: 9.954444249160588e-05\n",
      "  batch 5 loss: 8.461983816232532e-05\n",
      "  batch 6 loss: 8.818424976198003e-05\n",
      "  batch 7 loss: 6.854254752397537e-05\n",
      "  batch 8 loss: 7.454557635355741e-05\n",
      "  batch 9 loss: 9.028879867400974e-05\n",
      "  batch 10 loss: 5.280311597744003e-05\n",
      "  batch 11 loss: 8.449107554042712e-05\n",
      "  batch 12 loss: 0.00010041426867246628\n",
      "  batch 13 loss: 0.000127933279145509\n",
      "  batch 14 loss: 0.00019770977087318897\n",
      "  batch 15 loss: 0.00013032399874646217\n",
      "  batch 16 loss: 8.185205660993233e-05\n",
      "  batch 17 loss: 7.770707452436909e-05\n",
      "  batch 18 loss: 8.1565223808866e-05\n",
      "  batch 19 loss: 0.0001731246302369982\n",
      "  batch 20 loss: 0.00011538391117937863\n",
      "  batch 21 loss: 0.00016589259030297399\n",
      "  batch 22 loss: 8.340181375388056e-05\n",
      "  batch 23 loss: 0.00013694382505491376\n",
      "  batch 24 loss: 0.00012727678404189646\n",
      "  batch 25 loss: 0.00016764413157943636\n",
      "  batch 26 loss: 0.00018031394574791193\n",
      "  batch 27 loss: 6.37440534774214e-05\n",
      "  batch 28 loss: 0.0001264707971131429\n",
      "  batch 29 loss: 6.589878466911614e-05\n",
      "  batch 30 loss: 5.457731458591297e-05\n",
      "  batch 31 loss: 5.294420770951547e-05\n",
      "  batch 32 loss: 9.997535380534828e-05\n",
      "  batch 33 loss: 0.00012088000221410766\n",
      "  batch 34 loss: 0.00011617196287261322\n",
      "  batch 35 loss: 5.079317634226754e-05\n",
      "  batch 36 loss: 4.5426946599036455e-05\n",
      "  batch 37 loss: 5.776650141342543e-05\n",
      "  batch 38 loss: 5.9526126278797165e-05\n",
      "  batch 39 loss: 7.98146502347663e-05\n",
      "  batch 40 loss: 9.501748718321323e-05\n",
      "  batch 41 loss: 5.2211507863830775e-05\n",
      "  batch 42 loss: 8.55815305840224e-05\n",
      "  batch 43 loss: 5.9459453041199595e-05\n",
      "  batch 44 loss: 3.838729753624648e-05\n",
      "  batch 45 loss: 4.774687113240361e-05\n",
      "  batch 46 loss: 4.4742941099684685e-05\n",
      "  batch 47 loss: 5.738862819271162e-05\n",
      "  batch 48 loss: 6.142966594779864e-05\n",
      "  batch 49 loss: 6.808782927691936e-05\n",
      "  batch 50 loss: 8.350989082828164e-05\n",
      "  batch 51 loss: 7.718589040450752e-05\n",
      "  batch 52 loss: 6.663710519205779e-05\n",
      "  batch 53 loss: 6.746652070432901e-05\n",
      "  batch 54 loss: 6.623506487812847e-05\n",
      "  batch 55 loss: 9.574123396305367e-05\n",
      "  batch 56 loss: 5.849453009432182e-05\n",
      "  batch 57 loss: 4.717584670288488e-05\n",
      "  batch 58 loss: 6.350198236759752e-05\n",
      "  batch 59 loss: 4.842962516704574e-05\n",
      "  batch 60 loss: 9.652870357967913e-05\n",
      "  batch 61 loss: 3.492973337415606e-05\n",
      "  batch 62 loss: 8.945706213125959e-05\n",
      "  batch 63 loss: 6.109402602305636e-05\n",
      "  batch 64 loss: 4.475782043300569e-05\n",
      "  batch 65 loss: 5.117399268783629e-05\n",
      "  batch 66 loss: 4.944059037370607e-05\n",
      "  batch 67 loss: 3.937711881007999e-05\n",
      "  batch 68 loss: 4.981322126695886e-05\n",
      "  batch 69 loss: 5.587991836364381e-05\n",
      "  batch 70 loss: 9.820509876590222e-05\n",
      "  batch 71 loss: 6.078862497815862e-05\n",
      "  batch 72 loss: 5.594901813310571e-05\n",
      "  batch 73 loss: 6.11577634117566e-05\n",
      "  batch 74 loss: 7.285705214599147e-05\n",
      "  batch 75 loss: 0.00010056365863420069\n",
      "  batch 76 loss: 5.837676872033626e-05\n",
      "  batch 77 loss: 7.16004433343187e-05\n",
      "  batch 78 loss: 6.670927541563287e-05\n",
      "  batch 79 loss: 5.2657065680250525e-05\n",
      "  batch 80 loss: 6.675014446955174e-05\n",
      "  batch 81 loss: 7.311608351301402e-05\n",
      "  batch 82 loss: 7.170272147050127e-05\n",
      "  batch 83 loss: 8.790436550043523e-05\n",
      "  batch 84 loss: 5.4943877330515534e-05\n",
      "  batch 85 loss: 6.48062996333465e-05\n",
      "  batch 86 loss: 7.7341879659798e-05\n",
      "  batch 87 loss: 7.692385406699032e-05\n",
      "  batch 88 loss: 0.00012986564252059907\n",
      "  batch 89 loss: 8.071737829595804e-05\n",
      "  batch 90 loss: 0.0001168697708635591\n",
      "  batch 91 loss: 6.55234616715461e-05\n",
      "  batch 92 loss: 8.502780110575259e-05\n",
      "  batch 93 loss: 8.351249562110752e-05\n",
      "  batch 94 loss: 8.179343421943486e-05\n",
      "  batch 95 loss: 0.00013787236821372062\n",
      "LOSS train 0.00013787236821372062 valid 0.0013385480269789696\n",
      "LOSS train 0.00013787236821372062 valid 0.0014823215315118432\n",
      "LOSS train 0.00013787236821372062 valid 0.0014010758604854345\n",
      "LOSS train 0.00013787236821372062 valid 0.0013555409386754036\n",
      "LOSS train 0.00013787236821372062 valid 0.0013631678884848952\n",
      "LOSS train 0.00013787236821372062 valid 0.001263376441784203\n",
      "LOSS train 0.00013787236821372062 valid 0.0013208423042669892\n",
      "LOSS train 0.00013787236821372062 valid 0.0013739503920078278\n",
      "LOSS train 0.00013787236821372062 valid 0.001341131399385631\n",
      "LOSS train 0.00013787236821372062 valid 0.001367723336443305\n",
      "LOSS train 0.00013787236821372062 valid 0.0014303609495982528\n",
      "LOSS train 0.00013787236821372062 valid 0.001444650231860578\n",
      "LOSS train 0.00013787236821372062 valid 0.0014483870472759008\n",
      "LOSS train 0.00013787236821372062 valid 0.0014685139758512378\n",
      "LOSS train 0.00013787236821372062 valid 0.0015539772575721145\n",
      "LOSS train 0.00013787236821372062 valid 0.001615599961951375\n",
      "LOSS train 0.00013787236821372062 valid 0.0016257192473858595\n",
      "LOSS train 0.00013787236821372062 valid 0.0016374009428545833\n",
      "LOSS train 0.00013787236821372062 valid 0.0016390170203521848\n",
      "LOSS train 0.00013787236821372062 valid 0.0016342966118827462\n",
      "LOSS train 0.00013787236821372062 valid 0.0016134475590661168\n",
      "LOSS train 0.00013787236821372062 valid 0.0016323740128427744\n",
      "LOSS train 0.00013787236821372062 valid 0.0016249114414677024\n",
      "LOSS train 0.00013787236821372062 valid 0.0016395717393606901\n",
      "EPOCH 112:\n",
      "  batch 1 loss: 0.00012855247769039124\n",
      "  batch 2 loss: 7.285099854925647e-05\n",
      "  batch 3 loss: 0.00011350156273692846\n",
      "  batch 4 loss: 9.653064626036212e-05\n",
      "  batch 5 loss: 8.530781633453444e-05\n",
      "  batch 6 loss: 9.628632687963545e-05\n",
      "  batch 7 loss: 6.457004928961396e-05\n",
      "  batch 8 loss: 6.677131750620902e-05\n",
      "  batch 9 loss: 7.035290764179081e-05\n",
      "  batch 10 loss: 4.43792378064245e-05\n",
      "  batch 11 loss: 8.330766286235303e-05\n",
      "  batch 12 loss: 9.960131865227595e-05\n",
      "  batch 13 loss: 0.00013302091974765062\n",
      "  batch 14 loss: 0.00019359399448148906\n",
      "  batch 15 loss: 0.0001260122808162123\n",
      "  batch 16 loss: 6.563398346770555e-05\n",
      "  batch 17 loss: 6.373567157424986e-05\n",
      "  batch 18 loss: 8.026957220863551e-05\n",
      "  batch 19 loss: 0.0001777603174559772\n",
      "  batch 20 loss: 0.00012157998571638018\n",
      "  batch 21 loss: 0.00016384644550271332\n",
      "  batch 22 loss: 9.237083577318117e-05\n",
      "  batch 23 loss: 0.00014105856826063246\n",
      "  batch 24 loss: 0.00012443507148418576\n",
      "  batch 25 loss: 0.00017049830057658255\n",
      "  batch 26 loss: 0.00016131722077261657\n",
      "  batch 27 loss: 5.926181984250434e-05\n",
      "  batch 28 loss: 9.281425445806235e-05\n",
      "  batch 29 loss: 5.927719394094311e-05\n",
      "  batch 30 loss: 5.621294258162379e-05\n",
      "  batch 31 loss: 6.014929749653675e-05\n",
      "  batch 32 loss: 0.00010045070666819811\n",
      "  batch 33 loss: 9.519222658127546e-05\n",
      "  batch 34 loss: 0.00011031638860004023\n",
      "  batch 35 loss: 4.909944982500747e-05\n",
      "  batch 36 loss: 4.5961107389302924e-05\n",
      "  batch 37 loss: 6.33691088296473e-05\n",
      "  batch 38 loss: 6.606439274037257e-05\n",
      "  batch 39 loss: 8.565164171159267e-05\n",
      "  batch 40 loss: 9.43765917327255e-05\n",
      "  batch 41 loss: 5.312882785801776e-05\n",
      "  batch 42 loss: 8.902089757611975e-05\n",
      "  batch 43 loss: 5.436648280010559e-05\n",
      "  batch 44 loss: 3.945739445043728e-05\n",
      "  batch 45 loss: 4.9136469897348434e-05\n",
      "  batch 46 loss: 4.42756136180833e-05\n",
      "  batch 47 loss: 6.224095704965293e-05\n",
      "  batch 48 loss: 6.577865133294836e-05\n",
      "  batch 49 loss: 7.104549877112731e-05\n",
      "  batch 50 loss: 8.714333671377972e-05\n",
      "  batch 51 loss: 8.064930443651974e-05\n",
      "  batch 52 loss: 6.622425280511379e-05\n",
      "  batch 53 loss: 6.680269143544137e-05\n",
      "  batch 54 loss: 6.410016794689e-05\n",
      "  batch 55 loss: 9.266902634408325e-05\n",
      "  batch 56 loss: 5.580242577707395e-05\n",
      "  batch 57 loss: 4.715396062238142e-05\n",
      "  batch 58 loss: 5.784103268524632e-05\n",
      "  batch 59 loss: 5.181159940548241e-05\n",
      "  batch 60 loss: 9.138502355199307e-05\n",
      "  batch 61 loss: 3.271863533882424e-05\n",
      "  batch 62 loss: 8.397058991249651e-05\n",
      "  batch 63 loss: 5.6933320593088865e-05\n",
      "  batch 64 loss: 4.2071456846315414e-05\n",
      "  batch 65 loss: 4.404279025038704e-05\n",
      "  batch 66 loss: 4.6197401388781145e-05\n",
      "  batch 67 loss: 4.556315252557397e-05\n",
      "  batch 68 loss: 5.200055238674395e-05\n",
      "  batch 69 loss: 5.466557922773063e-05\n",
      "  batch 70 loss: 9.611928544472903e-05\n",
      "  batch 71 loss: 6.0412876337068155e-05\n",
      "  batch 72 loss: 5.6963828683365136e-05\n",
      "  batch 73 loss: 6.118920282460749e-05\n",
      "  batch 74 loss: 7.694651139900088e-05\n",
      "  batch 75 loss: 9.977583249565214e-05\n",
      "  batch 76 loss: 5.507464084075764e-05\n",
      "  batch 77 loss: 7.252722571138293e-05\n",
      "  batch 78 loss: 6.351507909130305e-05\n",
      "  batch 79 loss: 4.821869151783176e-05\n",
      "  batch 80 loss: 6.224510434549302e-05\n",
      "  batch 81 loss: 7.081728108460084e-05\n",
      "  batch 82 loss: 6.740509707015008e-05\n",
      "  batch 83 loss: 8.264071948360652e-05\n",
      "  batch 84 loss: 5.308494291966781e-05\n",
      "  batch 85 loss: 6.521586328744888e-05\n",
      "  batch 86 loss: 7.715784886386245e-05\n",
      "  batch 87 loss: 7.500927313230932e-05\n",
      "  batch 88 loss: 0.0001252036017831415\n",
      "  batch 89 loss: 7.901425124146044e-05\n",
      "  batch 90 loss: 0.00012089054507669061\n",
      "  batch 91 loss: 6.714667688356712e-05\n",
      "  batch 92 loss: 8.514290675520897e-05\n",
      "  batch 93 loss: 8.279592293547466e-05\n",
      "  batch 94 loss: 8.156039984896779e-05\n",
      "  batch 95 loss: 0.0001266837352886796\n",
      "LOSS train 0.0001266837352886796 valid 0.0013976460322737694\n",
      "LOSS train 0.0001266837352886796 valid 0.0015681348741054535\n",
      "LOSS train 0.0001266837352886796 valid 0.0014285191427916288\n",
      "LOSS train 0.0001266837352886796 valid 0.0013773947721347213\n",
      "LOSS train 0.0001266837352886796 valid 0.0013843997148796916\n",
      "LOSS train 0.0001266837352886796 valid 0.0012900608126074076\n",
      "LOSS train 0.0001266837352886796 valid 0.001379696186631918\n",
      "LOSS train 0.0001266837352886796 valid 0.0014448695583269\n",
      "LOSS train 0.0001266837352886796 valid 0.001404284848831594\n",
      "LOSS train 0.0001266837352886796 valid 0.0014387244591489434\n",
      "LOSS train 0.0001266837352886796 valid 0.0015154393622651696\n",
      "LOSS train 0.0001266837352886796 valid 0.0015272086020559072\n",
      "LOSS train 0.0001266837352886796 valid 0.0015169958351179957\n",
      "LOSS train 0.0001266837352886796 valid 0.0015428721671923995\n",
      "LOSS train 0.0001266837352886796 valid 0.0016434183344244957\n",
      "LOSS train 0.0001266837352886796 valid 0.0017083754064515233\n",
      "LOSS train 0.0001266837352886796 valid 0.0017158498521894217\n",
      "LOSS train 0.0001266837352886796 valid 0.0017325289081782103\n",
      "LOSS train 0.0001266837352886796 valid 0.0017374824965372682\n",
      "LOSS train 0.0001266837352886796 valid 0.0017328733811154962\n",
      "LOSS train 0.0001266837352886796 valid 0.001706501585431397\n",
      "LOSS train 0.0001266837352886796 valid 0.0017249376978725195\n",
      "LOSS train 0.0001266837352886796 valid 0.0017176360124722123\n",
      "LOSS train 0.0001266837352886796 valid 0.0017431513406336308\n",
      "EPOCH 113:\n",
      "  batch 1 loss: 0.0001230425259564072\n",
      "  batch 2 loss: 6.855999527033418e-05\n",
      "  batch 3 loss: 0.00010709435446187854\n",
      "  batch 4 loss: 9.318826050730422e-05\n",
      "  batch 5 loss: 8.387629350181669e-05\n",
      "  batch 6 loss: 8.327055547852069e-05\n",
      "  batch 7 loss: 6.1034461396047845e-05\n",
      "  batch 8 loss: 6.798453978262842e-05\n",
      "  batch 9 loss: 7.452813588315621e-05\n",
      "  batch 10 loss: 4.3869276851182804e-05\n",
      "  batch 11 loss: 8.867459837347269e-05\n",
      "  batch 12 loss: 0.00010083428060170263\n",
      "  batch 13 loss: 0.00013261640560813248\n",
      "  batch 14 loss: 0.00019206895376555622\n",
      "  batch 15 loss: 0.0001268235791940242\n",
      "  batch 16 loss: 6.730703171342611e-05\n",
      "  batch 17 loss: 6.034026955603622e-05\n",
      "  batch 18 loss: 7.475502206943929e-05\n",
      "  batch 19 loss: 0.00014879023365210742\n",
      "  batch 20 loss: 0.00010610584286041558\n",
      "  batch 21 loss: 0.00016868812963366508\n",
      "  batch 22 loss: 8.224352495744824e-05\n",
      "  batch 23 loss: 0.00014344489318318665\n",
      "  batch 24 loss: 0.0001281335426028818\n",
      "  batch 25 loss: 0.00015948479995131493\n",
      "  batch 26 loss: 0.00016165549459401518\n",
      "  batch 27 loss: 7.414699211949483e-05\n",
      "  batch 28 loss: 9.159922046819702e-05\n",
      "  batch 29 loss: 5.34543432877399e-05\n",
      "  batch 30 loss: 5.1714610890485346e-05\n",
      "  batch 31 loss: 5.3423529607243836e-05\n",
      "  batch 32 loss: 0.00010308885248377919\n",
      "  batch 33 loss: 0.00011077444651164114\n",
      "  batch 34 loss: 0.0001252661895705387\n",
      "  batch 35 loss: 5.100384441902861e-05\n",
      "  batch 36 loss: 4.9118280003312975e-05\n",
      "  batch 37 loss: 6.250951264519244e-05\n",
      "  batch 38 loss: 6.281519745243713e-05\n",
      "  batch 39 loss: 8.493471977999434e-05\n",
      "  batch 40 loss: 8.973620424512774e-05\n",
      "  batch 41 loss: 6.22188308625482e-05\n",
      "  batch 42 loss: 8.894084021449089e-05\n",
      "  batch 43 loss: 5.779259299742989e-05\n",
      "  batch 44 loss: 5.240073369350284e-05\n",
      "  batch 45 loss: 4.777764843311161e-05\n",
      "  batch 46 loss: 3.801236016442999e-05\n",
      "  batch 47 loss: 5.8415374951437116e-05\n",
      "  batch 48 loss: 6.0755162849090993e-05\n",
      "  batch 49 loss: 7.051537977531552e-05\n",
      "  batch 50 loss: 8.557159162592143e-05\n",
      "  batch 51 loss: 8.275883737951517e-05\n",
      "  batch 52 loss: 8.056945080170408e-05\n",
      "  batch 53 loss: 7.55045039113611e-05\n",
      "  batch 54 loss: 7.099717186065391e-05\n",
      "  batch 55 loss: 9.510150994174182e-05\n",
      "  batch 56 loss: 5.586029510595836e-05\n",
      "  batch 57 loss: 4.573292244458571e-05\n",
      "  batch 58 loss: 5.637755384668708e-05\n",
      "  batch 59 loss: 4.6502434997819364e-05\n",
      "  batch 60 loss: 9.628702537156641e-05\n",
      "  batch 61 loss: 3.161537097184919e-05\n",
      "  batch 62 loss: 9.46993677644059e-05\n",
      "  batch 63 loss: 6.131912232376635e-05\n",
      "  batch 64 loss: 4.506488039623946e-05\n",
      "  batch 65 loss: 4.829095269087702e-05\n",
      "  batch 66 loss: 4.994228220311925e-05\n",
      "  batch 67 loss: 4.847383388550952e-05\n",
      "  batch 68 loss: 5.607140337815508e-05\n",
      "  batch 69 loss: 5.8126119256485254e-05\n",
      "  batch 70 loss: 9.10970484255813e-05\n",
      "  batch 71 loss: 5.7585239119362086e-05\n",
      "  batch 72 loss: 5.238134690443985e-05\n",
      "  batch 73 loss: 5.550176865654066e-05\n",
      "  batch 74 loss: 6.724474224029109e-05\n",
      "  batch 75 loss: 9.336030052509159e-05\n",
      "  batch 76 loss: 5.3160503739491105e-05\n",
      "  batch 77 loss: 6.732327165082097e-05\n",
      "  batch 78 loss: 6.76464187563397e-05\n",
      "  batch 79 loss: 5.673333362210542e-05\n",
      "  batch 80 loss: 7.285353785846382e-05\n",
      "  batch 81 loss: 7.469999400200322e-05\n",
      "  batch 82 loss: 6.891743396408856e-05\n",
      "  batch 83 loss: 8.400540536968037e-05\n",
      "  batch 84 loss: 5.6161065003834665e-05\n",
      "  batch 85 loss: 7.10067106410861e-05\n",
      "  batch 86 loss: 7.893530710134655e-05\n",
      "  batch 87 loss: 7.382931653410196e-05\n",
      "  batch 88 loss: 0.00012109559611417353\n",
      "  batch 89 loss: 7.992045721039176e-05\n",
      "  batch 90 loss: 0.00011695158900693059\n",
      "  batch 91 loss: 6.600400956813246e-05\n",
      "  batch 92 loss: 8.705820073373616e-05\n",
      "  batch 93 loss: 8.573731611249968e-05\n",
      "  batch 94 loss: 8.278887253254652e-05\n",
      "  batch 95 loss: 0.0001307714992435649\n",
      "LOSS train 0.0001307714992435649 valid 0.0014465909916907549\n",
      "LOSS train 0.0001307714992435649 valid 0.0016072056023404002\n",
      "LOSS train 0.0001307714992435649 valid 0.0015184483490884304\n",
      "LOSS train 0.0001307714992435649 valid 0.0014625072944909334\n",
      "LOSS train 0.0001307714992435649 valid 0.0014643697068095207\n",
      "LOSS train 0.0001307714992435649 valid 0.0013635412324219942\n",
      "LOSS train 0.0001307714992435649 valid 0.0014185215113684535\n",
      "LOSS train 0.0001307714992435649 valid 0.0014795527094975114\n",
      "LOSS train 0.0001307714992435649 valid 0.0014484532875940204\n",
      "LOSS train 0.0001307714992435649 valid 0.0014721155166625977\n",
      "LOSS train 0.0001307714992435649 valid 0.0015309466980397701\n",
      "LOSS train 0.0001307714992435649 valid 0.0015419138362631202\n",
      "LOSS train 0.0001307714992435649 valid 0.0015451547224074602\n",
      "LOSS train 0.0001307714992435649 valid 0.0015741575043648481\n",
      "LOSS train 0.0001307714992435649 valid 0.0016689246986061335\n",
      "LOSS train 0.0001307714992435649 valid 0.0017287236405536532\n",
      "LOSS train 0.0001307714992435649 valid 0.0017374901799485087\n",
      "LOSS train 0.0001307714992435649 valid 0.001745505491271615\n",
      "LOSS train 0.0001307714992435649 valid 0.001743622706271708\n",
      "LOSS train 0.0001307714992435649 valid 0.0017353714210912585\n",
      "LOSS train 0.0001307714992435649 valid 0.0017114077927544713\n",
      "LOSS train 0.0001307714992435649 valid 0.001728663919493556\n",
      "LOSS train 0.0001307714992435649 valid 0.0017191614024341106\n",
      "LOSS train 0.0001307714992435649 valid 0.0017296315636485815\n",
      "EPOCH 114:\n",
      "  batch 1 loss: 0.00012576632434502244\n",
      "  batch 2 loss: 7.188036397565156e-05\n",
      "  batch 3 loss: 0.00010930256394203752\n",
      "  batch 4 loss: 9.199697524309158e-05\n",
      "  batch 5 loss: 8.152518421411514e-05\n",
      "  batch 6 loss: 7.413575076498091e-05\n",
      "  batch 7 loss: 5.659573071170598e-05\n",
      "  batch 8 loss: 6.180652417242527e-05\n",
      "  batch 9 loss: 6.951333489269018e-05\n",
      "  batch 10 loss: 4.438102405401878e-05\n",
      "  batch 11 loss: 8.60564032336697e-05\n",
      "  batch 12 loss: 0.0001027529506245628\n",
      "  batch 13 loss: 0.0001287135819438845\n",
      "  batch 14 loss: 0.00019483444339130074\n",
      "  batch 15 loss: 0.0001208165631396696\n",
      "  batch 16 loss: 6.46214684820734e-05\n",
      "  batch 17 loss: 5.994668390485458e-05\n",
      "  batch 18 loss: 6.880921864649281e-05\n",
      "  batch 19 loss: 0.0001398977910866961\n",
      "  batch 20 loss: 9.969215898308903e-05\n",
      "  batch 21 loss: 0.00016336329281330109\n",
      "  batch 22 loss: 9.123660856857896e-05\n",
      "  batch 23 loss: 0.00015009712660685182\n",
      "  batch 24 loss: 0.00014226014900486916\n",
      "  batch 25 loss: 0.000163609889568761\n",
      "  batch 26 loss: 0.0001699304993962869\n",
      "  batch 27 loss: 5.635522393276915e-05\n",
      "  batch 28 loss: 8.976551180239767e-05\n",
      "  batch 29 loss: 4.960491060046479e-05\n",
      "  batch 30 loss: 4.758293289341964e-05\n",
      "  batch 31 loss: 5.2366031013661996e-05\n",
      "  batch 32 loss: 9.551068069413304e-05\n",
      "  batch 33 loss: 0.00011352729052305222\n",
      "  batch 34 loss: 0.000137940383865498\n",
      "  batch 35 loss: 5.491200863616541e-05\n",
      "  batch 36 loss: 5.615457484964281e-05\n",
      "  batch 37 loss: 6.93581678206101e-05\n",
      "  batch 38 loss: 6.740580283803865e-05\n",
      "  batch 39 loss: 8.395501208724454e-05\n",
      "  batch 40 loss: 9.793003118829802e-05\n",
      "  batch 41 loss: 4.996152711100876e-05\n",
      "  batch 42 loss: 8.519025868736207e-05\n",
      "  batch 43 loss: 5.6910728744696826e-05\n",
      "  batch 44 loss: 5.091114871902391e-05\n",
      "  batch 45 loss: 5.1171933591831475e-05\n",
      "  batch 46 loss: 4.163455014349893e-05\n",
      "  batch 47 loss: 5.8408470067661256e-05\n",
      "  batch 48 loss: 6.394796946551651e-05\n",
      "  batch 49 loss: 7.384987839031965e-05\n",
      "  batch 50 loss: 8.860533125698566e-05\n",
      "  batch 51 loss: 7.972866296768188e-05\n",
      "  batch 52 loss: 7.50064937165007e-05\n",
      "  batch 53 loss: 6.649878196185455e-05\n",
      "  batch 54 loss: 6.797701644245535e-05\n",
      "  batch 55 loss: 9.673129534348845e-05\n",
      "  batch 56 loss: 5.9672544011846185e-05\n",
      "  batch 57 loss: 4.945982072968036e-05\n",
      "  batch 58 loss: 6.151673733256757e-05\n",
      "  batch 59 loss: 5.4787837143521756e-05\n",
      "  batch 60 loss: 9.648682316765189e-05\n",
      "  batch 61 loss: 2.6868558052228764e-05\n",
      "  batch 62 loss: 9.192184370476753e-05\n",
      "  batch 63 loss: 5.462110857479274e-05\n",
      "  batch 64 loss: 4.4533117034006864e-05\n",
      "  batch 65 loss: 5.079113907413557e-05\n",
      "  batch 66 loss: 4.905743844574317e-05\n",
      "  batch 67 loss: 4.10804568673484e-05\n",
      "  batch 68 loss: 5.25706636835821e-05\n",
      "  batch 69 loss: 5.273969145491719e-05\n",
      "  batch 70 loss: 0.0001037360416376032\n",
      "  batch 71 loss: 6.223312811926007e-05\n",
      "  batch 72 loss: 5.7367709814570844e-05\n",
      "  batch 73 loss: 5.691605474567041e-05\n",
      "  batch 74 loss: 7.106809061951935e-05\n",
      "  batch 75 loss: 9.579416655469686e-05\n",
      "  batch 76 loss: 4.995905328541994e-05\n",
      "  batch 77 loss: 5.999544737278484e-05\n",
      "  batch 78 loss: 5.824783875141293e-05\n",
      "  batch 79 loss: 4.2503379518166184e-05\n",
      "  batch 80 loss: 6.261590169742703e-05\n",
      "  batch 81 loss: 8.22060028440319e-05\n",
      "  batch 82 loss: 7.750048825982958e-05\n",
      "  batch 83 loss: 8.725593943381682e-05\n",
      "  batch 84 loss: 5.5218224588315934e-05\n",
      "  batch 85 loss: 6.458103598561138e-05\n",
      "  batch 86 loss: 7.204047142295167e-05\n",
      "  batch 87 loss: 7.239912520162761e-05\n",
      "  batch 88 loss: 0.00012801983393728733\n",
      "  batch 89 loss: 8.11302088550292e-05\n",
      "  batch 90 loss: 0.00011790559074142948\n",
      "  batch 91 loss: 6.618790212087333e-05\n",
      "  batch 92 loss: 8.255705324700102e-05\n",
      "  batch 93 loss: 7.961888331919909e-05\n",
      "  batch 94 loss: 8.221280586440116e-05\n",
      "  batch 95 loss: 0.00013616250362247229\n",
      "LOSS train 0.00013616250362247229 valid 0.001346249831840396\n",
      "LOSS train 0.00013616250362247229 valid 0.0015542563050985336\n",
      "LOSS train 0.00013616250362247229 valid 0.0013955323956906796\n",
      "LOSS train 0.00013616250362247229 valid 0.0013477812753990293\n",
      "LOSS train 0.00013616250362247229 valid 0.0013676475500687957\n",
      "LOSS train 0.00013616250362247229 valid 0.0012882724404335022\n",
      "LOSS train 0.00013616250362247229 valid 0.0013739160494878888\n",
      "LOSS train 0.00013616250362247229 valid 0.0014339755289256573\n",
      "LOSS train 0.00013616250362247229 valid 0.0014022935647517443\n",
      "LOSS train 0.00013616250362247229 valid 0.0014406730188056827\n",
      "LOSS train 0.00013616250362247229 valid 0.0015192938735708594\n",
      "LOSS train 0.00013616250362247229 valid 0.0015249704010784626\n",
      "LOSS train 0.00013616250362247229 valid 0.0015154514694586396\n",
      "LOSS train 0.00013616250362247229 valid 0.0015492181992158294\n",
      "LOSS train 0.00013616250362247229 valid 0.0016617362853139639\n",
      "LOSS train 0.00013616250362247229 valid 0.0017228599172085524\n",
      "LOSS train 0.00013616250362247229 valid 0.0017260032473132014\n",
      "LOSS train 0.00013616250362247229 valid 0.0017446608981117606\n",
      "LOSS train 0.00013616250362247229 valid 0.0017478052759543061\n",
      "LOSS train 0.00013616250362247229 valid 0.0017433020984753966\n",
      "LOSS train 0.00013616250362247229 valid 0.0017168867634609342\n",
      "LOSS train 0.00013616250362247229 valid 0.0017358912155032158\n",
      "LOSS train 0.00013616250362247229 valid 0.0017288468079641461\n",
      "LOSS train 0.00013616250362247229 valid 0.001747441478073597\n",
      "EPOCH 115:\n",
      "  batch 1 loss: 0.00012662744848057628\n",
      "  batch 2 loss: 7.394365820800886e-05\n",
      "  batch 3 loss: 0.00011722122872015461\n",
      "  batch 4 loss: 9.195080929202959e-05\n",
      "  batch 5 loss: 8.549650374334306e-05\n",
      "  batch 6 loss: 8.14079976407811e-05\n",
      "  batch 7 loss: 5.628986400552094e-05\n",
      "  batch 8 loss: 6.73095928505063e-05\n",
      "  batch 9 loss: 7.098990317899734e-05\n",
      "  batch 10 loss: 4.3013969843741506e-05\n",
      "  batch 11 loss: 8.634663390694186e-05\n",
      "  batch 12 loss: 0.00010807189391925931\n",
      "  batch 13 loss: 0.00013844661589246243\n",
      "  batch 14 loss: 0.00020523769489955157\n",
      "  batch 15 loss: 0.00013013520219828933\n",
      "  batch 16 loss: 6.480079900939018e-05\n",
      "  batch 17 loss: 6.073694385122508e-05\n",
      "  batch 18 loss: 6.842981383670121e-05\n",
      "  batch 19 loss: 0.00013489893171936274\n",
      "  batch 20 loss: 9.432547085452825e-05\n",
      "  batch 21 loss: 0.00015465455362573266\n",
      "  batch 22 loss: 8.146108302753419e-05\n",
      "  batch 23 loss: 0.0001579656673129648\n",
      "  batch 24 loss: 0.00014481644029729068\n",
      "  batch 25 loss: 0.00018196855671703815\n",
      "  batch 26 loss: 0.00017674406990408897\n",
      "  batch 27 loss: 6.64388426230289e-05\n",
      "  batch 28 loss: 9.028003114508465e-05\n",
      "  batch 29 loss: 6.200053030624986e-05\n",
      "  batch 30 loss: 5.235646676737815e-05\n",
      "  batch 31 loss: 5.465249341796152e-05\n",
      "  batch 32 loss: 9.424710151506588e-05\n",
      "  batch 33 loss: 0.00011073836503783241\n",
      "  batch 34 loss: 0.00013187210424803197\n",
      "  batch 35 loss: 6.566676893271506e-05\n",
      "  batch 36 loss: 7.002477650530636e-05\n",
      "  batch 37 loss: 0.0001009611805784516\n",
      "  batch 38 loss: 8.587865886511281e-05\n",
      "  batch 39 loss: 9.650787978898734e-05\n",
      "  batch 40 loss: 0.00010263852891512215\n",
      "  batch 41 loss: 5.182148743188009e-05\n",
      "  batch 42 loss: 8.444080594927073e-05\n",
      "  batch 43 loss: 5.6648714235052466e-05\n",
      "  batch 44 loss: 5.506269371835515e-05\n",
      "  batch 45 loss: 5.6946857512230054e-05\n",
      "  batch 46 loss: 5.174970283405855e-05\n",
      "  batch 47 loss: 7.730010111117736e-05\n",
      "  batch 48 loss: 8.316370804095641e-05\n",
      "  batch 49 loss: 8.573186642024666e-05\n",
      "  batch 50 loss: 9.21111786738038e-05\n",
      "  batch 51 loss: 9.061850141733885e-05\n",
      "  batch 52 loss: 7.647852908121422e-05\n",
      "  batch 53 loss: 7.473648292943835e-05\n",
      "  batch 54 loss: 8.079166582319885e-05\n",
      "  batch 55 loss: 0.00010283280425937846\n",
      "  batch 56 loss: 6.629117706324905e-05\n",
      "  batch 57 loss: 5.488536407938227e-05\n",
      "  batch 58 loss: 6.711710011586547e-05\n",
      "  batch 59 loss: 6.129757093731314e-05\n",
      "  batch 60 loss: 0.00010246036254102364\n",
      "  batch 61 loss: 3.443488458287902e-05\n",
      "  batch 62 loss: 8.827907731756568e-05\n",
      "  batch 63 loss: 5.87658432777971e-05\n",
      "  batch 64 loss: 4.51193300250452e-05\n",
      "  batch 65 loss: 4.753989924211055e-05\n",
      "  batch 66 loss: 5.2825213060714304e-05\n",
      "  batch 67 loss: 4.305658512748778e-05\n",
      "  batch 68 loss: 5.518915713764727e-05\n",
      "  batch 69 loss: 5.765912283095531e-05\n",
      "  batch 70 loss: 0.00011309946421533823\n",
      "  batch 71 loss: 7.139616354834288e-05\n",
      "  batch 72 loss: 5.742290159105323e-05\n",
      "  batch 73 loss: 5.8284993428969756e-05\n",
      "  batch 74 loss: 7.586833817185834e-05\n",
      "  batch 75 loss: 9.82851634034887e-05\n",
      "  batch 76 loss: 5.860416786163114e-05\n",
      "  batch 77 loss: 6.662239320576191e-05\n",
      "  batch 78 loss: 6.849864439573139e-05\n",
      "  batch 79 loss: 4.942274608765729e-05\n",
      "  batch 80 loss: 6.114276766311377e-05\n",
      "  batch 81 loss: 7.466290844604373e-05\n",
      "  batch 82 loss: 7.749922951916233e-05\n",
      "  batch 83 loss: 9.222887456417084e-05\n",
      "  batch 84 loss: 5.6552307796664536e-05\n",
      "  batch 85 loss: 7.305778854060918e-05\n",
      "  batch 86 loss: 8.528553007636219e-05\n",
      "  batch 87 loss: 8.672302647028118e-05\n",
      "  batch 88 loss: 0.0001426230592187494\n",
      "  batch 89 loss: 7.793543045409024e-05\n",
      "  batch 90 loss: 0.00011856573109980673\n",
      "  batch 91 loss: 6.96139395586215e-05\n",
      "  batch 92 loss: 9.464702452532947e-05\n",
      "  batch 93 loss: 9.484220936428756e-05\n",
      "  batch 94 loss: 9.599483018973842e-05\n",
      "  batch 95 loss: 0.0001734444813337177\n",
      "LOSS train 0.0001734444813337177 valid 0.0014413364697247744\n",
      "LOSS train 0.0001734444813337177 valid 0.0016755590913817286\n",
      "LOSS train 0.0001734444813337177 valid 0.001468211761675775\n",
      "LOSS train 0.0001734444813337177 valid 0.001406861818395555\n",
      "LOSS train 0.0001734444813337177 valid 0.0014036622596904635\n",
      "LOSS train 0.0001734444813337177 valid 0.0013041161000728607\n",
      "LOSS train 0.0001734444813337177 valid 0.0014014036860316992\n",
      "LOSS train 0.0001734444813337177 valid 0.001462786109186709\n",
      "LOSS train 0.0001734444813337177 valid 0.0014171586371958256\n",
      "LOSS train 0.0001734444813337177 valid 0.0014567546313628554\n",
      "LOSS train 0.0001734444813337177 valid 0.0015707388520240784\n",
      "LOSS train 0.0001734444813337177 valid 0.0015708327991887927\n",
      "LOSS train 0.0001734444813337177 valid 0.0015510926023125648\n",
      "LOSS train 0.0001734444813337177 valid 0.0015859154518693686\n",
      "LOSS train 0.0001734444813337177 valid 0.0017169476486742496\n",
      "LOSS train 0.0001734444813337177 valid 0.0017849331488832831\n",
      "LOSS train 0.0001734444813337177 valid 0.001782251289114356\n",
      "LOSS train 0.0001734444813337177 valid 0.0018123181071132421\n",
      "LOSS train 0.0001734444813337177 valid 0.0018247568514198065\n",
      "LOSS train 0.0001734444813337177 valid 0.0018399537075310946\n",
      "LOSS train 0.0001734444813337177 valid 0.001817182288505137\n",
      "LOSS train 0.0001734444813337177 valid 0.0018487906781956553\n",
      "LOSS train 0.0001734444813337177 valid 0.0018479694845154881\n",
      "LOSS train 0.0001734444813337177 valid 0.0018874774686992168\n",
      "EPOCH 116:\n",
      "  batch 1 loss: 0.00013972894521430135\n",
      "  batch 2 loss: 7.449582335539162e-05\n",
      "  batch 3 loss: 0.0001269805507035926\n",
      "  batch 4 loss: 0.0001005576632451266\n",
      "  batch 5 loss: 9.889857028611004e-05\n",
      "  batch 6 loss: 8.801781223155558e-05\n",
      "  batch 7 loss: 7.038720650598407e-05\n",
      "  batch 8 loss: 7.221948180813342e-05\n",
      "  batch 9 loss: 9.450854122405872e-05\n",
      "  batch 10 loss: 5.035763024352491e-05\n",
      "  batch 11 loss: 0.00010376352292951196\n",
      "  batch 12 loss: 0.00012406197492964566\n",
      "  batch 13 loss: 0.00015383271966129541\n",
      "  batch 14 loss: 0.0002558496780693531\n",
      "  batch 15 loss: 0.0001686457690084353\n",
      "  batch 16 loss: 8.67479175212793e-05\n",
      "  batch 17 loss: 8.559238631278276e-05\n",
      "  batch 18 loss: 9.156303713098168e-05\n",
      "  batch 19 loss: 0.00018845213344320655\n",
      "  batch 20 loss: 0.0001254276285180822\n",
      "  batch 21 loss: 0.00016474910080432892\n",
      "  batch 22 loss: 9.40109312068671e-05\n",
      "  batch 23 loss: 0.00016383618640247732\n",
      "  batch 24 loss: 0.00016953499289229512\n",
      "  batch 25 loss: 0.00023287039948627353\n",
      "  batch 26 loss: 0.00021406915038824081\n",
      "  batch 27 loss: 8.705272921361029e-05\n",
      "  batch 28 loss: 0.00010672116331988946\n",
      "  batch 29 loss: 7.48936872696504e-05\n",
      "  batch 30 loss: 6.873020174680278e-05\n",
      "  batch 31 loss: 5.088501711725257e-05\n",
      "  batch 32 loss: 9.646466060075909e-05\n",
      "  batch 33 loss: 9.836050594458356e-05\n",
      "  batch 34 loss: 0.0001359443849651143\n",
      "  batch 35 loss: 7.351096428465098e-05\n",
      "  batch 36 loss: 8.740068005863577e-05\n",
      "  batch 37 loss: 9.96106828097254e-05\n",
      "  batch 38 loss: 0.00010555369954090565\n",
      "  batch 39 loss: 0.000117905656225048\n",
      "  batch 40 loss: 0.00013221758126746863\n",
      "  batch 41 loss: 7.168049341998994e-05\n",
      "  batch 42 loss: 9.720244270283729e-05\n",
      "  batch 43 loss: 6.291714089456946e-05\n",
      "  batch 44 loss: 5.105207674205303e-05\n",
      "  batch 45 loss: 5.9950045397272334e-05\n",
      "  batch 46 loss: 5.83012770221103e-05\n",
      "  batch 47 loss: 8.685471402714029e-05\n",
      "  batch 48 loss: 9.237920312443748e-05\n",
      "  batch 49 loss: 0.00010409302922198549\n",
      "  batch 50 loss: 0.00011652756074909121\n",
      "  batch 51 loss: 0.00012056819105055183\n",
      "  batch 52 loss: 0.00011500100663397461\n",
      "  batch 53 loss: 0.00010376561840530485\n",
      "  batch 54 loss: 9.793765639187768e-05\n",
      "  batch 55 loss: 0.00011915080540347844\n",
      "  batch 56 loss: 7.67385572544299e-05\n",
      "  batch 57 loss: 6.0522030253196135e-05\n",
      "  batch 58 loss: 7.91962374933064e-05\n",
      "  batch 59 loss: 8.09130142442882e-05\n",
      "  batch 60 loss: 0.00012372150376904756\n",
      "  batch 61 loss: 4.881017957814038e-05\n",
      "  batch 62 loss: 0.00011924047430511564\n",
      "  batch 63 loss: 8.073299250099808e-05\n",
      "  batch 64 loss: 6.520496390294284e-05\n",
      "  batch 65 loss: 6.212270818650723e-05\n",
      "  batch 66 loss: 5.7083339925156906e-05\n",
      "  batch 67 loss: 5.319703632267192e-05\n",
      "  batch 68 loss: 6.925511115696281e-05\n",
      "  batch 69 loss: 6.725558341713622e-05\n",
      "  batch 70 loss: 0.00010816891153808683\n",
      "  batch 71 loss: 6.97815848980099e-05\n",
      "  batch 72 loss: 6.680982187390327e-05\n",
      "  batch 73 loss: 6.742114055668935e-05\n",
      "  batch 74 loss: 8.814406464807689e-05\n",
      "  batch 75 loss: 0.00012660249194595963\n",
      "  batch 76 loss: 8.234013512264937e-05\n",
      "  batch 77 loss: 8.382972737308592e-05\n",
      "  batch 78 loss: 7.44809876778163e-05\n",
      "  batch 79 loss: 6.304301496129483e-05\n",
      "  batch 80 loss: 7.265201566042379e-05\n",
      "  batch 81 loss: 8.787361002760008e-05\n",
      "  batch 82 loss: 9.285096894018352e-05\n",
      "  batch 83 loss: 9.649023559177294e-05\n",
      "  batch 84 loss: 6.292910256888717e-05\n",
      "  batch 85 loss: 7.723516318947077e-05\n",
      "  batch 86 loss: 8.522892312612385e-05\n",
      "  batch 87 loss: 8.847071148920804e-05\n",
      "  batch 88 loss: 0.0001450104173272848\n",
      "  batch 89 loss: 0.0001002225253614597\n",
      "  batch 90 loss: 0.00014945946168154478\n",
      "  batch 91 loss: 9.419732668902725e-05\n",
      "  batch 92 loss: 9.460157889407128e-05\n",
      "  batch 93 loss: 9.063868492376059e-05\n",
      "  batch 94 loss: 9.687589772511274e-05\n",
      "  batch 95 loss: 0.00016841113392729312\n",
      "LOSS train 0.00016841113392729312 valid 0.001395869068801403\n",
      "LOSS train 0.00016841113392729312 valid 0.0015027427580207586\n",
      "LOSS train 0.00016841113392729312 valid 0.001485586166381836\n",
      "LOSS train 0.00016841113392729312 valid 0.0015007590409368277\n",
      "LOSS train 0.00016841113392729312 valid 0.0015101400204002857\n",
      "LOSS train 0.00016841113392729312 valid 0.0014249358791857958\n",
      "LOSS train 0.00016841113392729312 valid 0.0014438648940995336\n",
      "LOSS train 0.00016841113392729312 valid 0.0014969689073041081\n",
      "LOSS train 0.00016841113392729312 valid 0.00147404579911381\n",
      "LOSS train 0.00016841113392729312 valid 0.0014874048065394163\n",
      "LOSS train 0.00016841113392729312 valid 0.0015283058164641261\n",
      "LOSS train 0.00016841113392729312 valid 0.0015490403166040778\n",
      "LOSS train 0.00016841113392729312 valid 0.0015665190294384956\n",
      "LOSS train 0.00016841113392729312 valid 0.0015883526066318154\n",
      "LOSS train 0.00016841113392729312 valid 0.001639983500353992\n",
      "LOSS train 0.00016841113392729312 valid 0.0016881667543202639\n",
      "LOSS train 0.00016841113392729312 valid 0.0016997730126604438\n",
      "LOSS train 0.00016841113392729312 valid 0.0016862849006429315\n",
      "LOSS train 0.00016841113392729312 valid 0.0016754315001890063\n",
      "LOSS train 0.00016841113392729312 valid 0.001668809331022203\n",
      "LOSS train 0.00016841113392729312 valid 0.0016486807726323605\n",
      "LOSS train 0.00016841113392729312 valid 0.0016696765087544918\n",
      "LOSS train 0.00016841113392729312 valid 0.0016630259342491627\n",
      "LOSS train 0.00016841113392729312 valid 0.0016645530704408884\n",
      "EPOCH 117:\n",
      "  batch 1 loss: 0.0001667413889663294\n",
      "  batch 2 loss: 9.013529779622331e-05\n",
      "  batch 3 loss: 0.00019657981465570629\n",
      "  batch 4 loss: 0.00014080182882025838\n",
      "  batch 5 loss: 0.00010105215915245935\n",
      "  batch 6 loss: 8.175125549314544e-05\n",
      "  batch 7 loss: 6.716478674206883e-05\n",
      "  batch 8 loss: 7.509300485253334e-05\n",
      "  batch 9 loss: 8.824387623462826e-05\n",
      "  batch 10 loss: 5.2561746997525916e-05\n",
      "  batch 11 loss: 0.00011276285658823326\n",
      "  batch 12 loss: 0.00013573761680163443\n",
      "  batch 13 loss: 0.00016057220636866987\n",
      "  batch 14 loss: 0.0003414296661503613\n",
      "  batch 15 loss: 0.00016224311548285186\n",
      "  batch 16 loss: 9.389696060679853e-05\n",
      "  batch 17 loss: 0.00014698822633363307\n",
      "  batch 18 loss: 0.00013222720008343458\n",
      "  batch 19 loss: 0.00024374861095566303\n",
      "  batch 20 loss: 0.00018953096878249198\n",
      "  batch 21 loss: 0.00021247073891572654\n",
      "  batch 22 loss: 0.00014723332424182445\n",
      "  batch 23 loss: 0.00020768845570273697\n",
      "  batch 24 loss: 0.00017446934361942112\n",
      "  batch 25 loss: 0.0002282255736645311\n",
      "  batch 26 loss: 0.0002242614864371717\n",
      "  batch 27 loss: 9.904881881084293e-05\n",
      "  batch 28 loss: 0.00013189975288696587\n",
      "  batch 29 loss: 8.552434155717492e-05\n",
      "  batch 30 loss: 6.886057963129133e-05\n",
      "  batch 31 loss: 6.593154103029519e-05\n",
      "  batch 32 loss: 0.00011239866580581293\n",
      "  batch 33 loss: 0.00010509924322832376\n",
      "  batch 34 loss: 0.00013166296412236989\n",
      "  batch 35 loss: 6.174929876578972e-05\n",
      "  batch 36 loss: 6.571580888703465e-05\n",
      "  batch 37 loss: 8.477596566081047e-05\n",
      "  batch 38 loss: 8.93394390004687e-05\n",
      "  batch 39 loss: 0.00011627233470790088\n",
      "  batch 40 loss: 0.00013872998533770442\n",
      "  batch 41 loss: 7.503735832870007e-05\n",
      "  batch 42 loss: 9.77210293058306e-05\n",
      "  batch 43 loss: 6.855707033537328e-05\n",
      "  batch 44 loss: 4.857672320213169e-05\n",
      "  batch 45 loss: 6.234163447516039e-05\n",
      "  batch 46 loss: 5.391468221205287e-05\n",
      "  batch 47 loss: 7.416597509291023e-05\n",
      "  batch 48 loss: 8.194358088076115e-05\n",
      "  batch 49 loss: 9.752018377184868e-05\n",
      "  batch 50 loss: 0.00012130443064961582\n",
      "  batch 51 loss: 0.00013282577856443822\n",
      "  batch 52 loss: 0.00011182930029463023\n",
      "  batch 53 loss: 9.729427983984351e-05\n",
      "  batch 54 loss: 9.419143316335976e-05\n",
      "  batch 55 loss: 0.00010883314826060086\n",
      "  batch 56 loss: 7.075699977576733e-05\n",
      "  batch 57 loss: 6.17990517639555e-05\n",
      "  batch 58 loss: 9.05200868146494e-05\n",
      "  batch 59 loss: 9.315733041148633e-05\n",
      "  batch 60 loss: 0.00011899470700882375\n",
      "  batch 61 loss: 5.586599945672788e-05\n",
      "  batch 62 loss: 0.00012374381185509264\n",
      "  batch 63 loss: 7.963333337102085e-05\n",
      "  batch 64 loss: 7.3822564445436e-05\n",
      "  batch 65 loss: 7.56712761358358e-05\n",
      "  batch 66 loss: 6.313966150628403e-05\n",
      "  batch 67 loss: 6.823763396823779e-05\n",
      "  batch 68 loss: 7.766115595586598e-05\n",
      "  batch 69 loss: 7.081394142005593e-05\n",
      "  batch 70 loss: 0.00011519830150064081\n",
      "  batch 71 loss: 7.016198651399463e-05\n",
      "  batch 72 loss: 6.928500079084188e-05\n",
      "  batch 73 loss: 7.266264583449811e-05\n",
      "  batch 74 loss: 9.623814548831433e-05\n",
      "  batch 75 loss: 0.00014275431749410927\n",
      "  batch 76 loss: 8.962982974480838e-05\n",
      "  batch 77 loss: 8.722799248062074e-05\n",
      "  batch 78 loss: 7.924075180198997e-05\n",
      "  batch 79 loss: 6.30911672487855e-05\n",
      "  batch 80 loss: 7.59711692808196e-05\n",
      "  batch 81 loss: 8.750878623686731e-05\n",
      "  batch 82 loss: 9.584457438904792e-05\n",
      "  batch 83 loss: 0.00010549700527917594\n",
      "  batch 84 loss: 7.355797424679622e-05\n",
      "  batch 85 loss: 8.740677003515884e-05\n",
      "  batch 86 loss: 9.714385669212788e-05\n",
      "  batch 87 loss: 9.641215729061514e-05\n",
      "  batch 88 loss: 0.00014664683840237558\n",
      "  batch 89 loss: 9.492119716014713e-05\n",
      "  batch 90 loss: 0.00014211219968274236\n",
      "  batch 91 loss: 8.17447726149112e-05\n",
      "  batch 92 loss: 9.447940828977153e-05\n",
      "  batch 93 loss: 9.195088205160573e-05\n",
      "  batch 94 loss: 9.546570072416216e-05\n",
      "  batch 95 loss: 0.00015553829143755138\n",
      "LOSS train 0.00015553829143755138 valid 0.0012635936727747321\n",
      "LOSS train 0.00015553829143755138 valid 0.0014064604183658957\n",
      "LOSS train 0.00015553829143755138 valid 0.0013850450050085783\n",
      "LOSS train 0.00015553829143755138 valid 0.0013474224833771586\n",
      "LOSS train 0.00015553829143755138 valid 0.0013560046209022403\n",
      "LOSS train 0.00015553829143755138 valid 0.0012683846289291978\n",
      "LOSS train 0.00015553829143755138 valid 0.0013071774737909436\n",
      "LOSS train 0.00015553829143755138 valid 0.0013606002321466804\n",
      "LOSS train 0.00015553829143755138 valid 0.001332585816271603\n",
      "LOSS train 0.00015553829143755138 valid 0.001355817774310708\n",
      "LOSS train 0.00015553829143755138 valid 0.001404027920216322\n",
      "LOSS train 0.00015553829143755138 valid 0.001416930346749723\n",
      "LOSS train 0.00015553829143755138 valid 0.0014259153977036476\n",
      "LOSS train 0.00015553829143755138 valid 0.0014492750633507967\n",
      "LOSS train 0.00015553829143755138 valid 0.0015317222569137812\n",
      "LOSS train 0.00015553829143755138 valid 0.0015870950883254409\n",
      "LOSS train 0.00015553829143755138 valid 0.0015943930484354496\n",
      "LOSS train 0.00015553829143755138 valid 0.0015917849959805608\n",
      "LOSS train 0.00015553829143755138 valid 0.0015851461794227362\n",
      "LOSS train 0.00015553829143755138 valid 0.0015837537357583642\n",
      "LOSS train 0.00015553829143755138 valid 0.001564620528370142\n",
      "LOSS train 0.00015553829143755138 valid 0.0015854217344895005\n",
      "LOSS train 0.00015553829143755138 valid 0.001573610003106296\n",
      "LOSS train 0.00015553829143755138 valid 0.001581195741891861\n",
      "EPOCH 118:\n",
      "  batch 1 loss: 0.00014793104492127895\n",
      "  batch 2 loss: 8.539533882867545e-05\n",
      "  batch 3 loss: 0.00015314840129576623\n",
      "  batch 4 loss: 0.00011539403931237757\n",
      "  batch 5 loss: 0.00012272085587028414\n",
      "  batch 6 loss: 0.00016788954962976277\n",
      "  batch 7 loss: 9.780724940355867e-05\n",
      "  batch 8 loss: 0.00010812227264977992\n",
      "  batch 9 loss: 9.019327990245074e-05\n",
      "  batch 10 loss: 5.6860655604396015e-05\n",
      "  batch 11 loss: 8.83974862517789e-05\n",
      "  batch 12 loss: 0.0001239954144693911\n",
      "  batch 13 loss: 0.00015549009549431503\n",
      "  batch 14 loss: 0.00025984918465837836\n",
      "  batch 15 loss: 0.00015761223039589822\n",
      "  batch 16 loss: 9.383702126797289e-05\n",
      "  batch 17 loss: 0.00012026615877402946\n",
      "  batch 18 loss: 0.00011069289757870138\n",
      "  batch 19 loss: 0.00018714027828536928\n",
      "  batch 20 loss: 0.00014804312377236784\n",
      "  batch 21 loss: 0.00017847202252596617\n",
      "  batch 22 loss: 0.00011052928311983123\n",
      "  batch 23 loss: 0.00015219824854284525\n",
      "  batch 24 loss: 0.00013827477232553065\n",
      "  batch 25 loss: 0.00017620567814446986\n",
      "  batch 26 loss: 0.0001822297926992178\n",
      "  batch 27 loss: 7.267553155543283e-05\n",
      "  batch 28 loss: 0.00011292505951132625\n",
      "  batch 29 loss: 8.067052840488032e-05\n",
      "  batch 30 loss: 6.722500256728381e-05\n",
      "  batch 31 loss: 5.592567686107941e-05\n",
      "  batch 32 loss: 9.813776705414057e-05\n",
      "  batch 33 loss: 9.22886683838442e-05\n",
      "  batch 34 loss: 0.00011116899258922786\n",
      "  batch 35 loss: 4.851842095376924e-05\n",
      "  batch 36 loss: 4.1839437471935526e-05\n",
      "  batch 37 loss: 6.388197652995586e-05\n",
      "  batch 38 loss: 7.16974536771886e-05\n",
      "  batch 39 loss: 0.00010017908789450303\n",
      "  batch 40 loss: 0.00010652257333276793\n",
      "  batch 41 loss: 7.04076373949647e-05\n",
      "  batch 42 loss: 9.47473308769986e-05\n",
      "  batch 43 loss: 7.273330993484706e-05\n",
      "  batch 44 loss: 5.2376788516994566e-05\n",
      "  batch 45 loss: 5.813109964947216e-05\n",
      "  batch 46 loss: 4.1551174945198e-05\n",
      "  batch 47 loss: 5.866206993232481e-05\n",
      "  batch 48 loss: 6.211487925611436e-05\n",
      "  batch 49 loss: 7.591229223180562e-05\n",
      "  batch 50 loss: 8.767627878114581e-05\n",
      "  batch 51 loss: 0.00010713045776356012\n",
      "  batch 52 loss: 0.00010578057845123112\n",
      "  batch 53 loss: 9.11733804969117e-05\n",
      "  batch 54 loss: 9.606521052774042e-05\n",
      "  batch 55 loss: 0.00010397355072200298\n",
      "  batch 56 loss: 6.250629667192698e-05\n",
      "  batch 57 loss: 5.386224438552745e-05\n",
      "  batch 58 loss: 7.019421900622547e-05\n",
      "  batch 59 loss: 6.482470053015277e-05\n",
      "  batch 60 loss: 0.00010365470370743424\n",
      "  batch 61 loss: 3.64449733751826e-05\n",
      "  batch 62 loss: 0.00011038382217520848\n",
      "  batch 63 loss: 6.836415559519082e-05\n",
      "  batch 64 loss: 5.572128793573938e-05\n",
      "  batch 65 loss: 5.609253639704548e-05\n",
      "  batch 66 loss: 5.660722672473639e-05\n",
      "  batch 67 loss: 6.099226084188558e-05\n",
      "  batch 68 loss: 7.442670175805688e-05\n",
      "  batch 69 loss: 6.793185457354411e-05\n",
      "  batch 70 loss: 0.0001043098236550577\n",
      "  batch 71 loss: 6.141588528407738e-05\n",
      "  batch 72 loss: 6.123803905211389e-05\n",
      "  batch 73 loss: 6.252031016629189e-05\n",
      "  batch 74 loss: 8.766623795963824e-05\n",
      "  batch 75 loss: 0.0001243188453372568\n",
      "  batch 76 loss: 7.663083670195192e-05\n",
      "  batch 77 loss: 7.927625119918957e-05\n",
      "  batch 78 loss: 7.48950697015971e-05\n",
      "  batch 79 loss: 5.1221759349573404e-05\n",
      "  batch 80 loss: 6.832660437794402e-05\n",
      "  batch 81 loss: 7.97306711319834e-05\n",
      "  batch 82 loss: 8.343951049027964e-05\n",
      "  batch 83 loss: 9.0469911810942e-05\n",
      "  batch 84 loss: 5.908842649660073e-05\n",
      "  batch 85 loss: 7.667468162253499e-05\n",
      "  batch 86 loss: 9.125652286456898e-05\n",
      "  batch 87 loss: 9.027746273204684e-05\n",
      "  batch 88 loss: 0.00014491879846900702\n",
      "  batch 89 loss: 8.914400677895173e-05\n",
      "  batch 90 loss: 0.00013611247413791716\n",
      "  batch 91 loss: 7.44967837817967e-05\n",
      "  batch 92 loss: 8.185645856428891e-05\n",
      "  batch 93 loss: 7.89188634371385e-05\n",
      "  batch 94 loss: 8.365660323761404e-05\n",
      "  batch 95 loss: 0.00012563425116240978\n",
      "LOSS train 0.00012563425116240978 valid 0.001446923241019249\n",
      "LOSS train 0.00012563425116240978 valid 0.001588677754625678\n",
      "LOSS train 0.00012563425116240978 valid 0.001456371508538723\n",
      "LOSS train 0.00012563425116240978 valid 0.001414512051269412\n",
      "LOSS train 0.00012563425116240978 valid 0.001427186536602676\n",
      "LOSS train 0.00012563425116240978 valid 0.0013383314944803715\n",
      "LOSS train 0.00012563425116240978 valid 0.0014179230201989412\n",
      "LOSS train 0.00012563425116240978 valid 0.0014874988701194525\n",
      "LOSS train 0.00012563425116240978 valid 0.0014562277356162667\n",
      "LOSS train 0.00012563425116240978 valid 0.0014953286154195666\n",
      "LOSS train 0.00012563425116240978 valid 0.0015613961732015014\n",
      "LOSS train 0.00012563425116240978 valid 0.0015826530288904905\n",
      "LOSS train 0.00012563425116240978 valid 0.0015823516296222806\n",
      "LOSS train 0.00012563425116240978 valid 0.0016134538454934955\n",
      "LOSS train 0.00012563425116240978 valid 0.0017366805113852024\n",
      "LOSS train 0.00012563425116240978 valid 0.001808021916076541\n",
      "LOSS train 0.00012563425116240978 valid 0.0018151364056393504\n",
      "LOSS train 0.00012563425116240978 valid 0.0018347203731536865\n",
      "LOSS train 0.00012563425116240978 valid 0.0018388052703812718\n",
      "LOSS train 0.00012563425116240978 valid 0.001833063899539411\n",
      "LOSS train 0.00012563425116240978 valid 0.0018022079020738602\n",
      "LOSS train 0.00012563425116240978 valid 0.0018166452646255493\n",
      "LOSS train 0.00012563425116240978 valid 0.0018012470100075006\n",
      "LOSS train 0.00012563425116240978 valid 0.001817614189349115\n",
      "EPOCH 119:\n",
      "  batch 1 loss: 0.00012900287401862442\n",
      "  batch 2 loss: 7.200251275207847e-05\n",
      "  batch 3 loss: 0.00011807834380306304\n",
      "  batch 4 loss: 9.967404184862971e-05\n",
      "  batch 5 loss: 9.33803676161915e-05\n",
      "  batch 6 loss: 9.464487811783329e-05\n",
      "  batch 7 loss: 6.79498043609783e-05\n",
      "  batch 8 loss: 8.491068729199469e-05\n",
      "  batch 9 loss: 9.160087938653305e-05\n",
      "  batch 10 loss: 7.315137190744281e-05\n",
      "  batch 11 loss: 9.747071453602985e-05\n",
      "  batch 12 loss: 0.00012462302402127534\n",
      "  batch 13 loss: 0.00016430079995188862\n",
      "  batch 14 loss: 0.00022759800776839256\n",
      "  batch 15 loss: 0.0001550883025629446\n",
      "  batch 16 loss: 7.5572170317173e-05\n",
      "  batch 17 loss: 7.572481990791857e-05\n",
      "  batch 18 loss: 8.424275438301265e-05\n",
      "  batch 19 loss: 0.00016600440721958876\n",
      "  batch 20 loss: 0.00013700444833375514\n",
      "  batch 21 loss: 0.00016488917754031718\n",
      "  batch 22 loss: 9.533920092508197e-05\n",
      "  batch 23 loss: 0.0001483122177887708\n",
      "  batch 24 loss: 0.00012527932994998991\n",
      "  batch 25 loss: 0.00018386155716143548\n",
      "  batch 26 loss: 0.0001684327726252377\n",
      "  batch 27 loss: 7.295829709619284e-05\n",
      "  batch 28 loss: 0.00011181068111909553\n",
      "  batch 29 loss: 6.647354894084856e-05\n",
      "  batch 30 loss: 5.5383043218171224e-05\n",
      "  batch 31 loss: 6.006827607052401e-05\n",
      "  batch 32 loss: 9.963114280253649e-05\n",
      "  batch 33 loss: 9.717222565086558e-05\n",
      "  batch 34 loss: 0.00011655518028419465\n",
      "  batch 35 loss: 5.5544056522194296e-05\n",
      "  batch 36 loss: 4.4508171413326636e-05\n",
      "  batch 37 loss: 6.0951388149987906e-05\n",
      "  batch 38 loss: 5.9373916883487254e-05\n",
      "  batch 39 loss: 8.295145380543545e-05\n",
      "  batch 40 loss: 9.694664913695306e-05\n",
      "  batch 41 loss: 6.016685802023858e-05\n",
      "  batch 42 loss: 9.07508801901713e-05\n",
      "  batch 43 loss: 7.388277299469337e-05\n",
      "  batch 44 loss: 5.519525439012796e-05\n",
      "  batch 45 loss: 6.920030864421278e-05\n",
      "  batch 46 loss: 5.6696677347645164e-05\n",
      "  batch 47 loss: 6.294186459854245e-05\n",
      "  batch 48 loss: 6.626989488722757e-05\n",
      "  batch 49 loss: 7.020555494818836e-05\n",
      "  batch 50 loss: 8.160668221535161e-05\n",
      "  batch 51 loss: 8.98144644452259e-05\n",
      "  batch 52 loss: 8.354074088856578e-05\n",
      "  batch 53 loss: 7.984981493791565e-05\n",
      "  batch 54 loss: 9.091451647691429e-05\n",
      "  batch 55 loss: 0.00010605677380226552\n",
      "  batch 56 loss: 6.267306162044406e-05\n",
      "  batch 57 loss: 6.194642628543079e-05\n",
      "  batch 58 loss: 7.541236118413508e-05\n",
      "  batch 59 loss: 6.3807558035478e-05\n",
      "  batch 60 loss: 9.838923870120198e-05\n",
      "  batch 61 loss: 3.2278516300721094e-05\n",
      "  batch 62 loss: 0.00013649309403263032\n",
      "  batch 63 loss: 6.14097953075543e-05\n",
      "  batch 64 loss: 5.842538666911423e-05\n",
      "  batch 65 loss: 6.264456897042692e-05\n",
      "  batch 66 loss: 5.4318850743584335e-05\n",
      "  batch 67 loss: 5.4105246817925945e-05\n",
      "  batch 68 loss: 6.342018605209887e-05\n",
      "  batch 69 loss: 6.375362863764167e-05\n",
      "  batch 70 loss: 0.00011165473551955074\n",
      "  batch 71 loss: 6.274247425608337e-05\n",
      "  batch 72 loss: 6.212549487827346e-05\n",
      "  batch 73 loss: 6.016848055878654e-05\n",
      "  batch 74 loss: 7.64225551392883e-05\n",
      "  batch 75 loss: 0.00010704892338253558\n",
      "  batch 76 loss: 6.464491889346391e-05\n",
      "  batch 77 loss: 7.44670833228156e-05\n",
      "  batch 78 loss: 6.899303116369992e-05\n",
      "  batch 79 loss: 5.130656063556671e-05\n",
      "  batch 80 loss: 7.551581074949354e-05\n",
      "  batch 81 loss: 9.030179353430867e-05\n",
      "  batch 82 loss: 9.385540033690631e-05\n",
      "  batch 83 loss: 9.454661631025374e-05\n",
      "  batch 84 loss: 6.08088266744744e-05\n",
      "  batch 85 loss: 6.688492430839688e-05\n",
      "  batch 86 loss: 8.086016168817878e-05\n",
      "  batch 87 loss: 8.427600550930947e-05\n",
      "  batch 88 loss: 0.0001319562434218824\n",
      "  batch 89 loss: 9.232987213181332e-05\n",
      "  batch 90 loss: 0.00014586580800823867\n",
      "  batch 91 loss: 8.417753269895911e-05\n",
      "  batch 92 loss: 0.00010210886102868244\n",
      "  batch 93 loss: 9.263239189749584e-05\n",
      "  batch 94 loss: 9.902468445943668e-05\n",
      "  batch 95 loss: 0.0001388396485708654\n",
      "LOSS train 0.0001388396485708654 valid 0.0013992695603519678\n",
      "LOSS train 0.0001388396485708654 valid 0.0015634703449904919\n",
      "LOSS train 0.0001388396485708654 valid 0.0014947348972782493\n",
      "LOSS train 0.0001388396485708654 valid 0.0014402223750948906\n",
      "LOSS train 0.0001388396485708654 valid 0.0014360398054122925\n",
      "LOSS train 0.0001388396485708654 valid 0.0013427494559437037\n",
      "LOSS train 0.0001388396485708654 valid 0.0013975416077300906\n",
      "LOSS train 0.0001388396485708654 valid 0.0014619309222325683\n",
      "LOSS train 0.0001388396485708654 valid 0.0014326419914141297\n",
      "LOSS train 0.0001388396485708654 valid 0.001462134881876409\n",
      "LOSS train 0.0001388396485708654 valid 0.0015090923989191651\n",
      "LOSS train 0.0001388396485708654 valid 0.0015272250166162848\n",
      "LOSS train 0.0001388396485708654 valid 0.0015371594345197082\n",
      "LOSS train 0.0001388396485708654 valid 0.0015666007529944181\n",
      "LOSS train 0.0001388396485708654 valid 0.0016804729821160436\n",
      "LOSS train 0.0001388396485708654 valid 0.0017478583613410592\n",
      "LOSS train 0.0001388396485708654 valid 0.0017517635133117437\n",
      "LOSS train 0.0001388396485708654 valid 0.0017564886948093772\n",
      "LOSS train 0.0001388396485708654 valid 0.0017517933156341314\n",
      "LOSS train 0.0001388396485708654 valid 0.0017433235188946128\n",
      "LOSS train 0.0001388396485708654 valid 0.001713454956188798\n",
      "LOSS train 0.0001388396485708654 valid 0.001732468488626182\n",
      "LOSS train 0.0001388396485708654 valid 0.001719126128591597\n",
      "LOSS train 0.0001388396485708654 valid 0.0017178970156237483\n",
      "EPOCH 120:\n",
      "  batch 1 loss: 0.00012962189794052392\n",
      "  batch 2 loss: 6.7798828240484e-05\n",
      "  batch 3 loss: 0.00011617707787081599\n",
      "  batch 4 loss: 9.955689893104136e-05\n",
      "  batch 5 loss: 8.727231761440635e-05\n",
      "  batch 6 loss: 9.21936152735725e-05\n",
      "  batch 7 loss: 6.38064811937511e-05\n",
      "  batch 8 loss: 8.625214832136407e-05\n",
      "  batch 9 loss: 0.00010372443648520857\n",
      "  batch 10 loss: 6.426098843803629e-05\n",
      "  batch 11 loss: 9.160606714431196e-05\n",
      "  batch 12 loss: 0.0001086202246369794\n",
      "  batch 13 loss: 0.00014450622256845236\n",
      "  batch 14 loss: 0.00020780939667019993\n",
      "  batch 15 loss: 0.00014245251077227294\n",
      "  batch 16 loss: 7.358225411735475e-05\n",
      "  batch 17 loss: 7.29832099750638e-05\n",
      "  batch 18 loss: 8.393536700168625e-05\n",
      "  batch 19 loss: 0.0001751385716488585\n",
      "  batch 20 loss: 0.000136202885187231\n",
      "  batch 21 loss: 0.0001608866296010092\n",
      "  batch 22 loss: 9.227175178239122e-05\n",
      "  batch 23 loss: 0.00014829967403784394\n",
      "  batch 24 loss: 0.00013529288116842508\n",
      "  batch 25 loss: 0.00019198637164663523\n",
      "  batch 26 loss: 0.000171478241099976\n",
      "  batch 27 loss: 6.869203934911638e-05\n",
      "  batch 28 loss: 9.487444185651839e-05\n",
      "  batch 29 loss: 6.29286005278118e-05\n",
      "  batch 30 loss: 5.5098360462579876e-05\n",
      "  batch 31 loss: 5.4652904509566724e-05\n",
      "  batch 32 loss: 9.697054338175803e-05\n",
      "  batch 33 loss: 9.23806510400027e-05\n",
      "  batch 34 loss: 0.00011752767750294879\n",
      "  batch 35 loss: 5.5499702284578234e-05\n",
      "  batch 36 loss: 5.169206269783899e-05\n",
      "  batch 37 loss: 7.352573447860777e-05\n",
      "  batch 38 loss: 7.611098408233374e-05\n",
      "  batch 39 loss: 9.633699664846063e-05\n",
      "  batch 40 loss: 0.00010738613491412252\n",
      "  batch 41 loss: 5.6396995205432177e-05\n",
      "  batch 42 loss: 8.624471956864e-05\n",
      "  batch 43 loss: 6.158721953397617e-05\n",
      "  batch 44 loss: 4.0923805499915034e-05\n",
      "  batch 45 loss: 4.883408473688178e-05\n",
      "  batch 46 loss: 4.590653406921774e-05\n",
      "  batch 47 loss: 6.293304613791406e-05\n",
      "  batch 48 loss: 6.852750084362924e-05\n",
      "  batch 49 loss: 7.714543608017266e-05\n",
      "  batch 50 loss: 8.81370942806825e-05\n",
      "  batch 51 loss: 8.503538992954418e-05\n",
      "  batch 52 loss: 7.764267502352595e-05\n",
      "  batch 53 loss: 7.194677164079621e-05\n",
      "  batch 54 loss: 7.235765951918438e-05\n",
      "  batch 55 loss: 9.892568050418049e-05\n",
      "  batch 56 loss: 6.398405821528286e-05\n",
      "  batch 57 loss: 5.6644101277925074e-05\n",
      "  batch 58 loss: 6.707610737066716e-05\n",
      "  batch 59 loss: 6.185397796798497e-05\n",
      "  batch 60 loss: 0.00013476675667334348\n",
      "  batch 61 loss: 3.808706969721243e-05\n",
      "  batch 62 loss: 9.017217962536961e-05\n",
      "  batch 63 loss: 6.525068602059036e-05\n",
      "  batch 64 loss: 5.2572653657989576e-05\n",
      "  batch 65 loss: 6.488289363915101e-05\n",
      "  batch 66 loss: 7.829144306015223e-05\n",
      "  batch 67 loss: 6.267470598686486e-05\n",
      "  batch 68 loss: 7.857618038542569e-05\n",
      "  batch 69 loss: 7.304016617126763e-05\n",
      "  batch 70 loss: 0.00012288671860005707\n",
      "  batch 71 loss: 7.778839790262282e-05\n",
      "  batch 72 loss: 7.166068826336414e-05\n",
      "  batch 73 loss: 7.609381282236427e-05\n",
      "  batch 74 loss: 8.331504068337381e-05\n",
      "  batch 75 loss: 0.00011209138028789312\n",
      "  batch 76 loss: 7.174437632784247e-05\n",
      "  batch 77 loss: 8.455389615846798e-05\n",
      "  batch 78 loss: 7.403588824672624e-05\n",
      "  batch 79 loss: 5.6050510465865955e-05\n",
      "  batch 80 loss: 7.077127520460635e-05\n",
      "  batch 81 loss: 8.252252882812172e-05\n",
      "  batch 82 loss: 8.353272278327495e-05\n",
      "  batch 83 loss: 0.00010159027442568913\n",
      "  batch 84 loss: 7.64637952670455e-05\n",
      "  batch 85 loss: 8.00246125436388e-05\n",
      "  batch 86 loss: 8.755722956266254e-05\n",
      "  batch 87 loss: 8.120860729832202e-05\n",
      "  batch 88 loss: 0.00013211375335231423\n",
      "  batch 89 loss: 8.199735020752996e-05\n",
      "  batch 90 loss: 0.0001369383535347879\n",
      "  batch 91 loss: 7.778123836033046e-05\n",
      "  batch 92 loss: 0.0001000176853267476\n",
      "  batch 93 loss: 9.85904480330646e-05\n",
      "  batch 94 loss: 0.00010129738075193018\n",
      "  batch 95 loss: 0.00015229247219394892\n",
      "LOSS train 0.00015229247219394892 valid 0.0013405231293290854\n",
      "LOSS train 0.00015229247219394892 valid 0.0015445902245119214\n",
      "LOSS train 0.00015229247219394892 valid 0.0014536025701090693\n",
      "LOSS train 0.00015229247219394892 valid 0.001386034768074751\n",
      "LOSS train 0.00015229247219394892 valid 0.001376128289848566\n",
      "LOSS train 0.00015229247219394892 valid 0.0012727691791951656\n",
      "LOSS train 0.00015229247219394892 valid 0.0013445114018395543\n",
      "LOSS train 0.00015229247219394892 valid 0.0014139876002445817\n",
      "LOSS train 0.00015229247219394892 valid 0.0013721635332331061\n",
      "LOSS train 0.00015229247219394892 valid 0.001405640970915556\n",
      "LOSS train 0.00015229247219394892 valid 0.0014590121572837234\n",
      "LOSS train 0.00015229247219394892 valid 0.0014647694770246744\n",
      "LOSS train 0.00015229247219394892 valid 0.0014597062254324555\n",
      "LOSS train 0.00015229247219394892 valid 0.0014908828306943178\n",
      "LOSS train 0.00015229247219394892 valid 0.0015925487969070673\n",
      "LOSS train 0.00015229247219394892 valid 0.0016547164414077997\n",
      "LOSS train 0.00015229247219394892 valid 0.0016642352566123009\n",
      "LOSS train 0.00015229247219394892 valid 0.0016798803117126226\n",
      "LOSS train 0.00015229247219394892 valid 0.0016855064313858747\n",
      "LOSS train 0.00015229247219394892 valid 0.0016801735619083047\n",
      "LOSS train 0.00015229247219394892 valid 0.0016542266821488738\n",
      "LOSS train 0.00015229247219394892 valid 0.0016801467863842845\n",
      "LOSS train 0.00015229247219394892 valid 0.0016692336648702621\n",
      "LOSS train 0.00015229247219394892 valid 0.0016764065949246287\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS =120\n",
    "writer_step = 2\n",
    "\n",
    "\n",
    "def main(execute):\n",
    "    \n",
    "    if execute:\n",
    "        model = Unet(num_channels=NUM_CHANNELS).to(device)\n",
    "        # metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CHANNELS)\n",
    "        # metric.to(device)\n",
    "        #summary(model, (3,256,256))\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        #loss_fn = torch.nn.Sigmoid()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        softmax = nn.Softmax2d()\n",
    "\n",
    "        def train_one_epoch(epoch_index, tb_writer):\n",
    "\n",
    "            running_loss = 0\n",
    "            last_loss = 0\n",
    "\n",
    "            for i, batch in enumerate(train_dataloader):\n",
    "                inputs, labels, _ = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(inputs)\n",
    "                loss = loss_fn(preds, labels)\n",
    "                # acc = metric(preds, labels)\n",
    "                # print(f'batch {i} accuracy = {acc}')\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                last_loss = running_loss / writer_step # loss per batch\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "                tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                running_loss = 0.\n",
    "            return last_loss\n",
    "\n",
    "\n",
    "\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "        epoch_number = 0\n",
    "\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "            model.train(True)\n",
    "            avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "            model.train(False)\n",
    "            with torch.no_grad():\n",
    "                running_vloss = 0.0\n",
    "                for i, vdata in enumerate(validation_dataloader):\n",
    "                    vinputs, vlabels, _ = vdata\n",
    "                    vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "                    voutputs = model(vinputs)\n",
    "                    vloss = loss_fn(voutputs, vlabels)\n",
    "                    running_vloss += vloss\n",
    "                    avg_vloss = running_vloss / (i + 1)\n",
    "                    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "                    # Log the running loss averaged per batch\n",
    "                    # for both training and validation\n",
    "                    writer.add_scalars('Training vs. Validation Loss',\n",
    "                                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                                    epoch_number + 1)\n",
    "                    writer.flush()\n",
    "            epoch_number += 1\n",
    "\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        tree = os.walk(cwd, topdown=True)\n",
    "\n",
    "        for root, dirs, files in tree:\n",
    "            for name in files:\n",
    "                if '.pth' in os.path.join(cwd, name):\n",
    "                    os.remove(os.path.join(cwd, name))\n",
    "                    break\n",
    "            break\n",
    "\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path + '.pth')\n",
    "\n",
    "main(EXECUTION_MODEL_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions on single Image #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "tree = os.walk(cwd, topdown=True)\n",
    "for root, dirs, files in tree:\n",
    "    for name in files:\n",
    "        if '.pth' in os.path.join(cwd, name):\n",
    "            model_path = os.path.join(cwd, name)\n",
    "            break\n",
    "    break\n",
    "\n",
    "model = Unet(num_channels=NUM_CHANNELS).to('cuda')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "def decode_to_RGB2(model, image_path, colormap): #gray\n",
    "    \"\"\"segmentation predicton for 2 class segmentation\"\"\"\n",
    "    image_test_path = os.path.join(cwd, image_path) \n",
    "    image_test = Image.open(image_test_path)\n",
    "    image_test = ImageOps.grayscale(image_test)\n",
    "    image_test = transform_image(image_test)\n",
    "    image_test = image_test.clone().detach().to(device)\n",
    "    image_test .to(device)\n",
    "\n",
    "    output = model(image_test .unsqueeze(0))\n",
    "\n",
    "    sm = nn.Softmax2d()\n",
    "    output = sm(output).squeeze(0)\n",
    "    output = torch.permute(output, (1,2,0)) #swap axes of tensor\n",
    "    output = torch.argmax(output, dim=2)\n",
    "    image_classes = output.cpu().detach().numpy()\n",
    "    image_output = np.zeros(shape=(TARGET_IMAGE_WIDTH,TARGET_IMAGE_HEIGHT)).astype(np.uint8)\n",
    "    for W in range(image_classes.shape[0]):\n",
    "        for H in range(image_classes.shape[1]):\n",
    "            image_output[W,H] = [k for k, v in colormap.items() if v == image_classes[W,H]][0]\n",
    "    image_output = Image.fromarray(image_output.astype(np.uint8))\n",
    "    image_output.save('test_mask_predicted.png')\n",
    "\n",
    "\n",
    "\n",
    "def decode_to_RGB12(model, image_path, colormap):\n",
    "    \"\"\"segmentation predicton for 2 class segmentation (for testing purposes)\"\"\"\n",
    "    image_test_path = os.path.join(cwd, image_path) \n",
    "\n",
    "    image_test = Image.open(image_test_path)\n",
    "    image_test = transform_image(image_test)\n",
    "    image_test = image_test.clone().detach().to(device)\n",
    "\n",
    "    output = model(image_test .unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    sm = nn.Softmax2d()\n",
    "    output = sm(output).squeeze(0)\n",
    "    output = torch.permute(output, dims=(1,2,0))\n",
    "    output = torch.argmax(output, axis=2)\n",
    "    image_classes = output.cpu().detach().numpy().astype(np.uint8)\n",
    "\n",
    "    image_output = np.zeros((TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT,3))\n",
    "    for W in range(image_classes.shape[0]):\n",
    "        for H in range(image_classes.shape[1]):\n",
    "            image_output[W,H] = [k for k, v in colormap.items() if v == image_classes[W,H]][0]\n",
    "    image_output = Image.fromarray(image_output.astype(np.uint8)).save('test_mask_predicted.png')\n",
    "\n",
    "if NUM_CHANNELS == 2:\n",
    "    decode_to_RGB2(model, 'test_image.jpg', colormap2_gray)\n",
    "else:\n",
    "    decode_to_RGB12(model, 'test_image.jpg', colormap12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of windows #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество окон - 44\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The marching squares algorithm for counting windows\"\"\"\n",
    "\n",
    "test_image_path = os.path.join(cwd, 'test_mask_predicted.png')\n",
    "\n",
    "def count_windows(test_img_path):\n",
    "    if os.path.exists(test_image_path):\n",
    "        imgray = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        imgrgb = cv2.imread(test_image_path)\n",
    "\n",
    "        # get contours\n",
    "        contours = measure.find_contours(imgray)\n",
    "\n",
    "        # get contours length\n",
    "        contour_length_list = []\n",
    "        for contour in contours:\n",
    "            contour_length = 0\n",
    "            for i in range(len(contour)-1):\n",
    "                contour_length += cv2.norm(contour[i], contour[i+1], cv2.NORM_L2)\n",
    "            contour_length_list.append(contour_length)\n",
    "\n",
    "        max_countour_length = max(contour_length_list)\n",
    "        # remove countours with length less than 20% of mean length of all countours\n",
    "        windows_count = len([length for length in contour_length_list if length > max_countour_length/3])\n",
    "        return windows_count\n",
    "\n",
    "windows = count_windows(test_image_path)\n",
    "print(f\"Количество окон - {windows}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "876187d4a2c995a73645928887c465872e44a13f37aecb236f2b7209f9d185bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
