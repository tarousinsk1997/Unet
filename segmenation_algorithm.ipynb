{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import cv2 \n",
    "from skimage import measure\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode segmentation masks to classes #\n",
    "\n",
    "CMP_facade_DB_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IMAGE_WIDTH = 512\n",
    "TARGET_IMAGE_HEIGHT = 512\n",
    "NUM_CHANNELS = 2\n",
    "BATCH_SIZE = 4\n",
    "EXECUTION_IMAGE_DATA = True\n",
    "EXECUTION_MODEL_TRAIN = True\n",
    "\n",
    "colormap12 = {(0,0,170) : 0, #background\n",
    "            (0,0,255) : 1, #facade\n",
    "            (0,85,255): 2, #window\n",
    "            (0,170,255): 3, #door\n",
    "            (255,85,0): 4, #cornice\n",
    "            (85,255,170): 5, #sill\n",
    "            (170,255,85): 6, #balcony\n",
    "            (255,255,0): 7, #blind\n",
    "            (255,170,0): 8, #deco\n",
    "            (0,255,255): 9, #molding\n",
    "            (255,0,0): 10, #pillar\n",
    "            (170,0,0): 11 #shop \n",
    "                            }\n",
    "colormap2 = {   (0,0,170) : 0, #background\n",
    "                (0,85,255): 1 #window    \n",
    "                           }     \n",
    "cwd = os.getcwd()\n",
    "img_trainA_path = os.path.join(cwd, r'dataset\\trainA')\n",
    "img_testA_path = os.path.join(cwd, r'dataset\\testA')\n",
    "img_trainB_path = os.path.join(cwd, r'dataset\\trainB')\n",
    "img_testB_path = os.path.join(cwd, r'dataset\\testB')\n",
    "npy_path = os.path.join(cwd, r'dataset\\np.array_targets')\n",
    "img_test_trans = cwd\n",
    "\n",
    "def encode_to_classes2(img_array, colormap):\n",
    "\n",
    "    \"\"\"\n",
    "    prepare data for 2 class segmentation\n",
    "    \"\"\"\n",
    "    class_array = np.zeros((img_array.shape[0], img_array.shape[1], 2))\n",
    "    image_trans_array = np.zeros((img_array.shape[0], img_array.shape[1], 3))\n",
    "\n",
    "    for W in range(img_array.shape[0]):\n",
    "        for H in range(img_array.shape[1]):\n",
    "            pixel_color = (img_array[W,H,0], img_array[W,H,1], img_array[W,H,2])\n",
    "            if pixel_color == (0,85,255):\n",
    "                class_array[W,H,1] = 1\n",
    "                image_trans_array[W,H,0] = 255\n",
    "                image_trans_array[W,H,1] = 0\n",
    "                image_trans_array[W,H,2] = 0\n",
    "            else:\n",
    "                class_array[W,H,0] = 1\n",
    "                image_trans_array[W,H,0] = 0\n",
    "                image_trans_array[W,H,1] = 0\n",
    "                image_trans_array[W,H,2] = 255\n",
    "    return class_array.astype(np.uint8), image_trans_array.astype(np.uint8)\n",
    "\n",
    "\n",
    "def encode_to_classes12(img_array, colormap):\n",
    "\n",
    "    \"\"\"\n",
    "    prepare data for 12 class segmentation\n",
    "    \"\"\"\n",
    "    class_array = np.zeros((img_array.shape[0], img_array.shape[1], 12))\n",
    "    image_trans_array = np.zeros((img_array.shape[0], img_array.shape[1], 3))\n",
    "\n",
    "    for W in range(img_array.shape[0]):\n",
    "        for H in range(img_array.shape[1]):\n",
    "            pixel_color = (img_array[W,H,0], img_array[W,H,1], img_array[W,H,2])\n",
    "            if pixel_color in colormap.keys():\n",
    "                class_array[W,H,colormap[pixel_color]] = 1\n",
    "            else:\n",
    "                class_array[W,H,1] = 1\n",
    "\n",
    "            image_trans_array[W,H,0] = img_array[W,H,0]\n",
    "            image_trans_array[W,H,1] = img_array[W,H,1]\n",
    "            image_trans_array[W,H,2] = img_array[W,H,2]\n",
    "    return class_array.astype(np.uint8), image_trans_array.astype(np.uint8)\n",
    "    \n",
    "\n",
    "def create_image_data(execute):\n",
    "    \n",
    "    if execute:\n",
    "        for i in range(303):\n",
    "            image = Image.open(os.path.join(img_trainB_path, \"cmp_b\" + f\"{i+1}\".zfill(4) + '.png')).convert('RGB')\n",
    "            image = np.array(image.resize((TARGET_IMAGE_WIDTH,TARGET_IMAGE_WIDTH)))\n",
    "            if NUM_CHANNELS == 2:\n",
    "                class_array, img_trans_array = encode_to_classes2(image, colormap2)\n",
    "            else:\n",
    "                class_array, img_trans_array = encode_to_classes12(image, colormap12)\n",
    "            np.save(os.path.join(npy_path, r'npy_trainB', \"cmp_b\" +  f\"{i+1}\".zfill(4)+ '.npy'), class_array)\n",
    "\n",
    "\n",
    "        for i in range(75):\n",
    "            image = Image.open(os.path.join(img_testB_path, \"cmp_b\" + f\"{i+304}\".zfill(4) + '.png')).convert('RGB')\n",
    "            image = np.array(image.resize((TARGET_IMAGE_WIDTH,TARGET_IMAGE_WIDTH)))\n",
    "            if NUM_CHANNELS == 2:\n",
    "                class_array, img_trans_array = encode_to_classes2(image, colormap2)\n",
    "            else:\n",
    "                class_array, img_trans_array = encode_to_classes12(image, colormap12)\n",
    "            np.save(os.path.join(npy_path, r'npy_testB', \"cmp_b\" +  f\"{i+304}\".zfill(4)+ '.npy'), class_array)\n",
    "\n",
    "create_image_data(EXECUTION_IMAGE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset class and instances #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_image = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]\n",
    ")\n",
    "\n",
    "transform_label = transforms.Compose(\n",
    "    [   \n",
    "        transforms.ToTensor()])\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform_image, transform_label, data_index):\n",
    "        self.img_labels = pd.read_csv(annotations_file, sep=';')\n",
    "        self.img_dir = img_dir\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        self.data_index = data_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        img_label_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, self.data_index])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = image.resize((TARGET_IMAGE_WIDTH,TARGET_IMAGE_WIDTH))\n",
    "        image_label = np.load(img_label_path)\n",
    "        text_label = self.img_labels.iloc[idx,3]\n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label:\n",
    "            image_label = self.transform_label(image_label)\n",
    "        return image, image_label, text_label\n",
    "\n",
    "\n",
    "training_dataset = CustomImageDataset(annotations_file=os.path.join(cwd, r'dataset\\annotation_train.csv'),\n",
    "                                   img_dir=os.path.join(cwd, r'dataset'),\n",
    "                                   transform_image=transform_image,\n",
    "                                   transform_label=transform_label,\n",
    "                                   data_index=2,\n",
    "                                   )\n",
    "\n",
    "validation_dataset = CustomImageDataset(annotations_file=os.path.join(cwd, r'dataset\\annotation_validation.csv'),\n",
    "                                   img_dir=os.path.join(cwd, r'dataset'),\n",
    "                                   transform_image=transform_image,\n",
    "                                   transform_label=transform_label,\n",
    "                                   data_index=2,\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader instances #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Model implementation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1) #1\n",
    "        self.bn1 = nn.BatchNorm2d(out_c) #2 reduces internal covariance shift\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, num_channels, kernel_size=1, padding=0)\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and run model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 2 loss: 0.0025511536514386535\n",
      "  batch 4 loss: 0.002240907517261803\n",
      "  batch 6 loss: 0.0018963073380291462\n",
      "  batch 8 loss: 0.0017554272199049592\n",
      "  batch 10 loss: 0.0017239355947822332\n",
      "  batch 12 loss: 0.0017004050314426422\n",
      "  batch 14 loss: 0.0016208774177357554\n",
      "  batch 16 loss: 0.0018541037570685148\n",
      "  batch 18 loss: 0.0019820391898974776\n",
      "  batch 20 loss: 0.001980458211619407\n",
      "  batch 22 loss: 0.0014002337702549994\n",
      "  batch 24 loss: 0.0013130876468494534\n",
      "  batch 26 loss: 0.0012819321127608418\n",
      "  batch 28 loss: 0.0011595396790653467\n",
      "  batch 30 loss: 0.0011166086187586188\n",
      "  batch 32 loss: 0.0013962205848656595\n",
      "  batch 34 loss: 0.0010292159277014434\n",
      "  batch 36 loss: 0.0010258627007715404\n",
      "  batch 38 loss: 0.0011938430252484977\n",
      "  batch 40 loss: 0.00104225252289325\n",
      "  batch 42 loss: 0.001208837958984077\n",
      "  batch 44 loss: 0.001160282059572637\n",
      "  batch 46 loss: 0.0010373763798270375\n",
      "  batch 48 loss: 0.0013159841764718294\n",
      "  batch 50 loss: 0.0012945111375302076\n",
      "  batch 52 loss: 0.001203904626891017\n",
      "  batch 54 loss: 0.0010258265247102827\n",
      "  batch 56 loss: 0.0009407711331732571\n",
      "  batch 58 loss: 0.0007525358232669532\n",
      "  batch 60 loss: 0.000926250999327749\n",
      "  batch 62 loss: 0.0010062342626042664\n",
      "  batch 64 loss: 0.0010873014107346535\n",
      "  batch 66 loss: 0.0008233081316575408\n",
      "  batch 68 loss: 0.0007356001005973667\n",
      "  batch 70 loss: 0.0010259908740408719\n",
      "  batch 72 loss: 0.0009270429727621377\n",
      "  batch 74 loss: 0.0010493972804397345\n",
      "  batch 76 loss: 0.000881161744473502\n",
      "LOSS train 0.000881161744473502 valid 0.0017757894238457084\n",
      "LOSS train 0.000881161744473502 valid 0.0019225417636334896\n",
      "LOSS train 0.000881161744473502 valid 0.0018450263887643814\n",
      "LOSS train 0.000881161744473502 valid 0.0016756823752075434\n",
      "LOSS train 0.000881161744473502 valid 0.0015858336118981242\n",
      "LOSS train 0.000881161744473502 valid 0.0015356712974607944\n",
      "LOSS train 0.000881161744473502 valid 0.0014943376882001758\n",
      "LOSS train 0.000881161744473502 valid 0.001448635826818645\n",
      "LOSS train 0.000881161744473502 valid 0.0014625150943174958\n",
      "LOSS train 0.000881161744473502 valid 0.001441663596779108\n",
      "LOSS train 0.000881161744473502 valid 0.00145191908814013\n",
      "LOSS train 0.000881161744473502 valid 0.0014702258631587029\n",
      "LOSS train 0.000881161744473502 valid 0.0014667423674836755\n",
      "LOSS train 0.000881161744473502 valid 0.0014387060655280948\n",
      "LOSS train 0.000881161744473502 valid 0.0014523979043588042\n",
      "LOSS train 0.000881161744473502 valid 0.0014761664206162095\n",
      "LOSS train 0.000881161744473502 valid 0.0014683693880215287\n",
      "LOSS train 0.000881161744473502 valid 0.0015125867212191224\n",
      "LOSS train 0.000881161744473502 valid 0.0015531514072790742\n",
      "EPOCH 2:\n",
      "  batch 2 loss: 0.0010099645587615669\n",
      "  batch 4 loss: 0.0009693984175100923\n",
      "  batch 6 loss: 0.0007206283917184919\n",
      "  batch 8 loss: 0.0011053826310671866\n",
      "  batch 10 loss: 0.001521727943327278\n",
      "  batch 12 loss: 0.0013787714997306466\n",
      "  batch 14 loss: 0.001362448325380683\n",
      "  batch 16 loss: 0.0016353231039829552\n",
      "  batch 18 loss: 0.001568760140798986\n",
      "  batch 20 loss: 0.001813831040635705\n",
      "  batch 22 loss: 0.001082074420992285\n",
      "  batch 24 loss: 0.0010270214406773448\n",
      "  batch 26 loss: 0.001072477549314499\n",
      "  batch 28 loss: 0.0009183685469906777\n",
      "  batch 30 loss: 0.0008376672922167927\n",
      "  batch 32 loss: 0.0011448780714999884\n",
      "  batch 34 loss: 0.0008162608719430864\n",
      "  batch 36 loss: 0.0008233249245677143\n",
      "  batch 38 loss: 0.000982090801699087\n",
      "  batch 40 loss: 0.0008005308045540005\n",
      "  batch 42 loss: 0.0009399570990353823\n",
      "  batch 44 loss: 0.000852645025588572\n",
      "  batch 46 loss: 0.0007340286101680249\n",
      "  batch 48 loss: 0.0010145674459636211\n",
      "  batch 50 loss: 0.0009139228495769203\n",
      "  batch 52 loss: 0.0008181949669960886\n",
      "  batch 54 loss: 0.0009137939778156579\n",
      "  batch 56 loss: 0.0007822442858014256\n",
      "  batch 58 loss: 0.0005940732662566006\n",
      "  batch 60 loss: 0.0007735148828942329\n",
      "  batch 62 loss: 0.0007836722652427852\n",
      "  batch 64 loss: 0.0009226249530911446\n",
      "  batch 66 loss: 0.0007371222891379148\n",
      "  batch 68 loss: 0.0006604685622733086\n",
      "  batch 70 loss: 0.0007883392390795052\n",
      "  batch 72 loss: 0.0008401937084272504\n",
      "  batch 74 loss: 0.0009857631521299481\n",
      "  batch 76 loss: 0.0007987135322764516\n",
      "LOSS train 0.0007987135322764516 valid 0.0010693774092942476\n",
      "LOSS train 0.0007987135322764516 valid 0.0009814011864364147\n",
      "LOSS train 0.0007987135322764516 valid 0.000994243542663753\n",
      "LOSS train 0.0007987135322764516 valid 0.0009527236688882113\n",
      "LOSS train 0.0007987135322764516 valid 0.0009827461326494813\n",
      "LOSS train 0.0007987135322764516 valid 0.001035698689520359\n",
      "LOSS train 0.0007987135322764516 valid 0.0010703683365136385\n",
      "LOSS train 0.0007987135322764516 valid 0.0010842822957783937\n",
      "LOSS train 0.0007987135322764516 valid 0.0011147550540044904\n",
      "LOSS train 0.0007987135322764516 valid 0.0011381390504539013\n",
      "LOSS train 0.0007987135322764516 valid 0.0011473584454506636\n",
      "LOSS train 0.0007987135322764516 valid 0.0011988440528512\n",
      "LOSS train 0.0007987135322764516 valid 0.0012022799346596003\n",
      "LOSS train 0.0007987135322764516 valid 0.0011942476266995072\n",
      "LOSS train 0.0007987135322764516 valid 0.0012410922208800912\n",
      "LOSS train 0.0007987135322764516 valid 0.0012929670047014952\n",
      "LOSS train 0.0007987135322764516 valid 0.0013557183556258678\n",
      "LOSS train 0.0007987135322764516 valid 0.0014086830196902156\n",
      "LOSS train 0.0007987135322764516 valid 0.001404685084708035\n",
      "EPOCH 3:\n",
      "  batch 2 loss: 0.0011651781387627125\n",
      "  batch 4 loss: 0.0010824364726431668\n",
      "  batch 6 loss: 0.0006452938250731677\n",
      "  batch 8 loss: 0.000802279042545706\n",
      "  batch 10 loss: 0.0012623812071979046\n",
      "  batch 12 loss: 0.0013527292176149786\n",
      "  batch 14 loss: 0.0012517363065853715\n",
      "  batch 16 loss: 0.0016857622540555894\n",
      "  batch 18 loss: 0.0014949874021112919\n",
      "  batch 20 loss: 0.0016105714021250606\n",
      "  batch 22 loss: 0.0010606691939756274\n",
      "  batch 24 loss: 0.0008614945109002292\n",
      "  batch 26 loss: 0.0008158687269315124\n",
      "  batch 28 loss: 0.0007475719321519136\n",
      "  batch 30 loss: 0.000677961070323363\n",
      "  batch 32 loss: 0.0008605415932834148\n",
      "  batch 34 loss: 0.0007618839445058256\n",
      "  batch 36 loss: 0.0006794781802454963\n",
      "  batch 38 loss: 0.0006957242439966649\n",
      "  batch 40 loss: 0.0005872377660125494\n",
      "  batch 42 loss: 0.0006988138775341213\n",
      "  batch 44 loss: 0.0006285135459620506\n",
      "  batch 46 loss: 0.000587916758377105\n",
      "  batch 48 loss: 0.0008195159316528589\n",
      "  batch 50 loss: 0.0007235642115119845\n",
      "  batch 52 loss: 0.0005938492831774056\n",
      "  batch 54 loss: 0.0006002380105201155\n",
      "  batch 56 loss: 0.000666393869323656\n",
      "  batch 58 loss: 0.0004567757132463157\n",
      "  batch 60 loss: 0.0006429637141991407\n",
      "  batch 62 loss: 0.0006382877472788095\n",
      "  batch 64 loss: 0.0006924242479726672\n",
      "  batch 66 loss: 0.0005960218841210008\n",
      "  batch 68 loss: 0.0005620328593067825\n",
      "  batch 70 loss: 0.0007756561099085957\n",
      "  batch 72 loss: 0.0008034875791054219\n",
      "  batch 74 loss: 0.0009214558522216976\n",
      "  batch 76 loss: 0.0007240342965815216\n",
      "LOSS train 0.0007240342965815216 valid 0.0007622917182743549\n",
      "LOSS train 0.0007240342965815216 valid 0.000649660883937031\n",
      "LOSS train 0.0007240342965815216 valid 0.0006500377785414457\n",
      "LOSS train 0.0007240342965815216 valid 0.0006307526491582394\n",
      "LOSS train 0.0007240342965815216 valid 0.0006365666631609201\n",
      "LOSS train 0.0007240342965815216 valid 0.0006463391473516822\n",
      "LOSS train 0.0007240342965815216 valid 0.000682889309246093\n",
      "LOSS train 0.0007240342965815216 valid 0.0006919715669937432\n",
      "LOSS train 0.0007240342965815216 valid 0.0007194674690254033\n",
      "LOSS train 0.0007240342965815216 valid 0.0007205367437563837\n",
      "LOSS train 0.0007240342965815216 valid 0.0007373177795670927\n",
      "LOSS train 0.0007240342965815216 valid 0.0007798493606969714\n",
      "LOSS train 0.0007240342965815216 valid 0.000802394060883671\n",
      "LOSS train 0.0007240342965815216 valid 0.000791059632319957\n",
      "LOSS train 0.0007240342965815216 valid 0.0008339387713931501\n",
      "LOSS train 0.0007240342965815216 valid 0.0008444688282907009\n",
      "LOSS train 0.0007240342965815216 valid 0.0008540569106116891\n",
      "LOSS train 0.0007240342965815216 valid 0.0008783268858678639\n",
      "LOSS train 0.0007240342965815216 valid 0.0008752872818149626\n",
      "EPOCH 4:\n",
      "  batch 2 loss: 0.0008400576480198652\n",
      "  batch 4 loss: 0.000841303903143853\n",
      "  batch 6 loss: 0.0005969236372038722\n",
      "  batch 8 loss: 0.0006935423298273236\n",
      "  batch 10 loss: 0.0011726290686056018\n",
      "  batch 12 loss: 0.0011846885026898235\n",
      "  batch 14 loss: 0.0009612736175768077\n",
      "  batch 16 loss: 0.0014021501410752535\n",
      "  batch 18 loss: 0.0013552908785641193\n",
      "  batch 20 loss: 0.0014190039364621043\n",
      "  batch 22 loss: 0.0009855494427029043\n",
      "  batch 24 loss: 0.0008782832010183483\n",
      "  batch 26 loss: 0.0007510217255912721\n",
      "  batch 28 loss: 0.0007070558203849941\n",
      "  batch 30 loss: 0.0005833306058775634\n",
      "  batch 32 loss: 0.0008312321151606739\n",
      "  batch 34 loss: 0.0006966381915844977\n",
      "  batch 36 loss: 0.0005716359883081168\n",
      "  batch 38 loss: 0.0005534840311156586\n",
      "  batch 40 loss: 0.0004396499280119315\n",
      "  batch 42 loss: 0.0006075989222154021\n",
      "  batch 44 loss: 0.0005403714021667838\n",
      "  batch 46 loss: 0.0005115354433655739\n",
      "  batch 48 loss: 0.0008449656888842583\n",
      "  batch 50 loss: 0.0005879426607862115\n",
      "  batch 52 loss: 0.0005509845359483734\n",
      "  batch 54 loss: 0.0005515728844329715\n",
      "  batch 56 loss: 0.0005699116154573858\n",
      "  batch 58 loss: 0.00040880139567889273\n",
      "  batch 60 loss: 0.0005813799944007769\n",
      "  batch 62 loss: 0.0006250477454159409\n",
      "  batch 64 loss: 0.0006023534224368632\n",
      "  batch 66 loss: 0.0005188762006582692\n",
      "  batch 68 loss: 0.0004909662675345317\n",
      "  batch 70 loss: 0.0007822493789717555\n",
      "  batch 72 loss: 0.0007494563469663262\n",
      "  batch 74 loss: 0.0007994610641617328\n",
      "  batch 76 loss: 0.0006480196316260844\n",
      "LOSS train 0.0006480196316260844 valid 0.000706101767718792\n",
      "LOSS train 0.0006480196316260844 valid 0.0005926524172537029\n",
      "LOSS train 0.0006480196316260844 valid 0.0005718761822208762\n",
      "LOSS train 0.0006480196316260844 valid 0.0005613435059785843\n",
      "LOSS train 0.0006480196316260844 valid 0.0005848185392096639\n",
      "LOSS train 0.0006480196316260844 valid 0.0005975377280265093\n",
      "LOSS train 0.0006480196316260844 valid 0.0006303094560280442\n",
      "LOSS train 0.0006480196316260844 valid 0.000636704673524946\n",
      "LOSS train 0.0006480196316260844 valid 0.0006823432631790638\n",
      "LOSS train 0.0006480196316260844 valid 0.0006753936177119613\n",
      "LOSS train 0.0006480196316260844 valid 0.0006832962390035391\n",
      "LOSS train 0.0006480196316260844 valid 0.0007320913719013333\n",
      "LOSS train 0.0006480196316260844 valid 0.0007554481271654367\n",
      "LOSS train 0.0006480196316260844 valid 0.0007403617491945624\n",
      "LOSS train 0.0006480196316260844 valid 0.00078429194400087\n",
      "LOSS train 0.0006480196316260844 valid 0.00078483612742275\n",
      "LOSS train 0.0006480196316260844 valid 0.0007802928448654711\n",
      "LOSS train 0.0006480196316260844 valid 0.0007938088965602219\n",
      "LOSS train 0.0006480196316260844 valid 0.0007930686697363853\n",
      "EPOCH 5:\n",
      "  batch 2 loss: 0.0008247351215686649\n",
      "  batch 4 loss: 0.0008163934107869864\n",
      "  batch 6 loss: 0.0006129238463472575\n",
      "  batch 8 loss: 0.0005828012654092163\n",
      "  batch 10 loss: 0.001132406381657347\n",
      "  batch 12 loss: 0.0011842201929539442\n",
      "  batch 14 loss: 0.0009411695937160403\n",
      "  batch 16 loss: 0.0012689100112766027\n",
      "  batch 18 loss: 0.0012220066273584962\n",
      "  batch 20 loss: 0.001298458082601428\n",
      "  batch 22 loss: 0.0008644927584100515\n",
      "  batch 24 loss: 0.0007824096828699112\n",
      "  batch 26 loss: 0.000731258827727288\n",
      "  batch 28 loss: 0.0007195377838797867\n",
      "  batch 30 loss: 0.0005827613931614906\n",
      "  batch 32 loss: 0.0007230114424601197\n",
      "  batch 34 loss: 0.0006043906614650041\n",
      "  batch 36 loss: 0.0004782362375408411\n",
      "  batch 38 loss: 0.000559120875550434\n",
      "  batch 40 loss: 0.0004032798751723021\n",
      "  batch 42 loss: 0.0005462232074933127\n",
      "  batch 44 loss: 0.0004891032440355048\n",
      "  batch 46 loss: 0.00047705453471280634\n",
      "  batch 48 loss: 0.000801329966634512\n",
      "  batch 50 loss: 0.0005680066533386707\n",
      "  batch 52 loss: 0.0004506081313593313\n",
      "  batch 54 loss: 0.0005231968825682998\n",
      "  batch 56 loss: 0.0005203021573834121\n",
      "  batch 58 loss: 0.00037087133387103677\n",
      "  batch 60 loss: 0.0005296142335282639\n",
      "  batch 62 loss: 0.0005561373545788229\n",
      "  batch 64 loss: 0.0005432041070889682\n",
      "  batch 66 loss: 0.00048363856330979615\n",
      "  batch 68 loss: 0.00044220182462595403\n",
      "  batch 70 loss: 0.0007464099617209285\n",
      "  batch 72 loss: 0.0007021900382824242\n",
      "  batch 74 loss: 0.0007738705608062446\n",
      "  batch 76 loss: 0.0006238401401787996\n",
      "LOSS train 0.0006238401401787996 valid 0.0006568505195900798\n",
      "LOSS train 0.0006238401401787996 valid 0.0005491802585311234\n",
      "LOSS train 0.0006238401401787996 valid 0.0005360273644328117\n",
      "LOSS train 0.0006238401401787996 valid 0.0005335686728358269\n",
      "LOSS train 0.0006238401401787996 valid 0.0005451430333778262\n",
      "LOSS train 0.0006238401401787996 valid 0.0005527780740521848\n",
      "LOSS train 0.0006238401401787996 valid 0.0005753643345087767\n",
      "LOSS train 0.0006238401401787996 valid 0.0005783546948805451\n",
      "LOSS train 0.0006238401401787996 valid 0.000601326406467706\n",
      "LOSS train 0.0006238401401787996 valid 0.0005907244049012661\n",
      "LOSS train 0.0006238401401787996 valid 0.0006003769813105464\n",
      "LOSS train 0.0006238401401787996 valid 0.0006480516749434173\n",
      "LOSS train 0.0006238401401787996 valid 0.000672683585435152\n",
      "LOSS train 0.0006238401401787996 valid 0.0006608415860682726\n",
      "LOSS train 0.0006238401401787996 valid 0.0007069444400258362\n",
      "LOSS train 0.0006238401401787996 valid 0.0007084705866873264\n",
      "LOSS train 0.0006238401401787996 valid 0.0007054723682813346\n",
      "LOSS train 0.0006238401401787996 valid 0.0007189789321273565\n",
      "LOSS train 0.0006238401401787996 valid 0.0007211328484117985\n",
      "EPOCH 6:\n",
      "  batch 2 loss: 0.0007524296524934471\n",
      "  batch 4 loss: 0.0007635379442945123\n",
      "  batch 6 loss: 0.0005534594820346683\n",
      "  batch 8 loss: 0.0005635226843878627\n",
      "  batch 10 loss: 0.0011087695020250976\n",
      "  batch 12 loss: 0.0011262849438935518\n",
      "  batch 14 loss: 0.0008751680143177509\n",
      "  batch 16 loss: 0.001233562477864325\n",
      "  batch 18 loss: 0.0011741347843781114\n",
      "  batch 20 loss: 0.0012269553844816983\n",
      "  batch 22 loss: 0.0008068044844549149\n",
      "  batch 24 loss: 0.0007630806358065456\n",
      "  batch 26 loss: 0.0007261535502038896\n",
      "  batch 28 loss: 0.0006996930751483887\n",
      "  batch 30 loss: 0.0005681532202288508\n",
      "  batch 32 loss: 0.0007562735700048506\n",
      "  batch 34 loss: 0.0005715905281249434\n",
      "  batch 36 loss: 0.00043952339910902083\n",
      "  batch 38 loss: 0.0005049911414971575\n",
      "  batch 40 loss: 0.0003835119277937338\n",
      "  batch 42 loss: 0.0005042479751864448\n",
      "  batch 44 loss: 0.0004647022287826985\n",
      "  batch 46 loss: 0.0004227550671203062\n",
      "  batch 48 loss: 0.0007516936457250267\n",
      "  batch 50 loss: 0.000518127650138922\n",
      "  batch 52 loss: 0.0004198320966679603\n",
      "  batch 54 loss: 0.0004384482163004577\n",
      "  batch 56 loss: 0.00046788960753474385\n",
      "  batch 58 loss: 0.000364536652341485\n",
      "  batch 60 loss: 0.0004910801508231089\n",
      "  batch 62 loss: 0.0004928958369418979\n",
      "  batch 64 loss: 0.0004875691665802151\n",
      "  batch 66 loss: 0.00044600991532206535\n",
      "  batch 68 loss: 0.0004105889529455453\n",
      "  batch 70 loss: 0.0006466264458140358\n",
      "  batch 72 loss: 0.0007014332804828882\n",
      "  batch 74 loss: 0.0006776996306143701\n",
      "  batch 76 loss: 0.0006174091249704361\n",
      "LOSS train 0.0006174091249704361 valid 0.0006600468186661601\n",
      "LOSS train 0.0006174091249704361 valid 0.0005282016936689615\n",
      "LOSS train 0.0006174091249704361 valid 0.0005205841735005379\n",
      "LOSS train 0.0006174091249704361 valid 0.0005099912523292005\n",
      "LOSS train 0.0006174091249704361 valid 0.0005133814993314445\n",
      "LOSS train 0.0006174091249704361 valid 0.0005257491138763726\n",
      "LOSS train 0.0006174091249704361 valid 0.0005448833107948303\n",
      "LOSS train 0.0006174091249704361 valid 0.0005472557968460023\n",
      "LOSS train 0.0006174091249704361 valid 0.0005754848243668675\n",
      "LOSS train 0.0006174091249704361 valid 0.0005682139890268445\n",
      "LOSS train 0.0006174091249704361 valid 0.0005799767095595598\n",
      "LOSS train 0.0006174091249704361 valid 0.0006136767915450037\n",
      "LOSS train 0.0006174091249704361 valid 0.0006324168643914163\n",
      "LOSS train 0.0006174091249704361 valid 0.0006214382010512054\n",
      "LOSS train 0.0006174091249704361 valid 0.0006515810964629054\n",
      "LOSS train 0.0006174091249704361 valid 0.0006538355373777449\n",
      "LOSS train 0.0006174091249704361 valid 0.0006519895396195352\n",
      "LOSS train 0.0006174091249704361 valid 0.0006637309561483562\n",
      "LOSS train 0.0006174091249704361 valid 0.000668373191729188\n",
      "EPOCH 7:\n",
      "  batch 2 loss: 0.000751024461351335\n",
      "  batch 4 loss: 0.0007380419410765171\n",
      "  batch 6 loss: 0.0006068108486942947\n",
      "  batch 8 loss: 0.0005485386645887047\n",
      "  batch 10 loss: 0.0009817408863455057\n",
      "  batch 12 loss: 0.0010761340672615916\n",
      "  batch 14 loss: 0.0008001297828741372\n",
      "  batch 16 loss: 0.0011191069497726858\n",
      "  batch 18 loss: 0.0011688292725011706\n",
      "  batch 20 loss: 0.0011872872128151357\n",
      "  batch 22 loss: 0.0008017715881578624\n",
      "  batch 24 loss: 0.0007372272666543722\n",
      "  batch 26 loss: 0.0006785517034586519\n",
      "  batch 28 loss: 0.0006603184156119823\n",
      "  batch 30 loss: 0.0005173768731765449\n",
      "  batch 32 loss: 0.0007221541600301862\n",
      "  batch 34 loss: 0.0005478832754306495\n",
      "  batch 36 loss: 0.00039903735159896314\n",
      "  batch 38 loss: 0.0004459423653315753\n",
      "  batch 40 loss: 0.00035230175126343966\n",
      "  batch 42 loss: 0.0004936526092933491\n",
      "  batch 44 loss: 0.00042703174403868616\n",
      "  batch 46 loss: 0.00041005258390214294\n",
      "  batch 48 loss: 0.0007756576524116099\n",
      "  batch 50 loss: 0.000527538824826479\n",
      "  batch 52 loss: 0.00039411681063938886\n",
      "  batch 54 loss: 0.0004264926246833056\n",
      "  batch 56 loss: 0.0004489258863031864\n",
      "  batch 58 loss: 0.0003364436561241746\n",
      "  batch 60 loss: 0.00048086800961755216\n",
      "  batch 62 loss: 0.0004773128457600251\n",
      "  batch 64 loss: 0.0004745586193166673\n",
      "  batch 66 loss: 0.00042161974124610424\n",
      "  batch 68 loss: 0.00038854383456055075\n",
      "  batch 70 loss: 0.0006518091249745339\n",
      "  batch 72 loss: 0.0006903400935698301\n",
      "  batch 74 loss: 0.0006695927586406469\n",
      "  batch 76 loss: 0.0006263367831707001\n",
      "LOSS train 0.0006263367831707001 valid 0.0006854814710095525\n",
      "LOSS train 0.0006263367831707001 valid 0.0005355728790163994\n",
      "LOSS train 0.0006263367831707001 valid 0.0005255010328255594\n",
      "LOSS train 0.0006263367831707001 valid 0.0005290501867420971\n",
      "LOSS train 0.0006263367831707001 valid 0.0005291143315844238\n",
      "LOSS train 0.0006263367831707001 valid 0.0005369747523218393\n",
      "LOSS train 0.0006263367831707001 valid 0.0005618117284029722\n",
      "LOSS train 0.0006263367831707001 valid 0.0005636903224512935\n",
      "LOSS train 0.0006263367831707001 valid 0.0005898235249333084\n",
      "LOSS train 0.0006263367831707001 valid 0.0005822741659358144\n",
      "LOSS train 0.0006263367831707001 valid 0.0005905696307308972\n",
      "LOSS train 0.0006263367831707001 valid 0.0006293522310443223\n",
      "LOSS train 0.0006263367831707001 valid 0.0006509359227493405\n",
      "LOSS train 0.0006263367831707001 valid 0.000640149402897805\n",
      "LOSS train 0.0006263367831707001 valid 0.0006716122734360397\n",
      "LOSS train 0.0006263367831707001 valid 0.0006698611541651189\n",
      "LOSS train 0.0006263367831707001 valid 0.0006661853985860944\n",
      "LOSS train 0.0006263367831707001 valid 0.0006783881108276546\n",
      "LOSS train 0.0006263367831707001 valid 0.0006831785431131721\n",
      "EPOCH 8:\n",
      "  batch 2 loss: 0.0007280859281308949\n",
      "  batch 4 loss: 0.0007621136901434511\n",
      "  batch 6 loss: 0.0005815074546262622\n",
      "  batch 8 loss: 0.0005155317048775032\n",
      "  batch 10 loss: 0.001033025560900569\n",
      "  batch 12 loss: 0.0010412610427010804\n",
      "  batch 14 loss: 0.0007841011101845652\n",
      "  batch 16 loss: 0.0011336836032569408\n",
      "  batch 18 loss: 0.001207886089105159\n",
      "  batch 20 loss: 0.0011496697552502155\n",
      "  batch 22 loss: 0.0007241943676490337\n",
      "  batch 24 loss: 0.000677080184686929\n",
      "  batch 26 loss: 0.0006658080383203924\n",
      "  batch 28 loss: 0.0007053719600662589\n",
      "  batch 30 loss: 0.0005415261548478156\n",
      "  batch 32 loss: 0.0006743058911524713\n",
      "  batch 34 loss: 0.000539687171112746\n",
      "  batch 36 loss: 0.00039446301525458694\n",
      "  batch 38 loss: 0.0004799987655133009\n",
      "  batch 40 loss: 0.00037003068428020924\n",
      "  batch 42 loss: 0.0005134547827765346\n",
      "  batch 44 loss: 0.0004306957853259519\n",
      "  batch 46 loss: 0.00041584296559449285\n",
      "  batch 48 loss: 0.0007803105108905584\n",
      "  batch 50 loss: 0.000493525352794677\n",
      "  batch 52 loss: 0.0003806923486990854\n",
      "  batch 54 loss: 0.00039474383811466396\n",
      "  batch 56 loss: 0.000429765903390944\n",
      "  batch 58 loss: 0.0003297787916380912\n",
      "  batch 60 loss: 0.0004393411218188703\n",
      "  batch 62 loss: 0.0004537045897450298\n",
      "  batch 64 loss: 0.00045257710735313594\n",
      "  batch 66 loss: 0.00040575777529738843\n",
      "  batch 68 loss: 0.00037675377097912133\n",
      "  batch 70 loss: 0.0006132912531029433\n",
      "  batch 72 loss: 0.0006395368836820126\n",
      "  batch 74 loss: 0.0006777945382054895\n",
      "  batch 76 loss: 0.0005910892941756174\n",
      "LOSS train 0.0005910892941756174 valid 0.0006433314410969615\n",
      "LOSS train 0.0005910892941756174 valid 0.0005094970692880452\n",
      "LOSS train 0.0005910892941756174 valid 0.000511820602696389\n",
      "LOSS train 0.0005910892941756174 valid 0.0005086843157187104\n",
      "LOSS train 0.0005910892941756174 valid 0.0005068423342891037\n",
      "LOSS train 0.0005910892941756174 valid 0.0005169556243345141\n",
      "LOSS train 0.0005910892941756174 valid 0.0005363858072087169\n",
      "LOSS train 0.0005910892941756174 valid 0.000541109184268862\n",
      "LOSS train 0.0005910892941756174 valid 0.0005695209838449955\n",
      "LOSS train 0.0005910892941756174 valid 0.0005644274060614407\n",
      "LOSS train 0.0005910892941756174 valid 0.0005742398207075894\n",
      "LOSS train 0.0005910892941756174 valid 0.000602464540861547\n",
      "LOSS train 0.0005910892941756174 valid 0.0006198538467288017\n",
      "LOSS train 0.0005910892941756174 valid 0.0006094423588365316\n",
      "LOSS train 0.0005910892941756174 valid 0.0006395276868715882\n",
      "LOSS train 0.0005910892941756174 valid 0.0006432405207306147\n",
      "LOSS train 0.0005910892941756174 valid 0.0006430362700484693\n",
      "LOSS train 0.0005910892941756174 valid 0.0006535688880831003\n",
      "LOSS train 0.0005910892941756174 valid 0.0006575025618076324\n",
      "EPOCH 9:\n",
      "  batch 2 loss: 0.0006848748307675123\n",
      "  batch 4 loss: 0.0007863540085963905\n",
      "  batch 6 loss: 0.0006096793804317713\n",
      "  batch 8 loss: 0.0004969528963556513\n",
      "  batch 10 loss: 0.0009718387445900589\n",
      "  batch 12 loss: 0.0009345887810923159\n",
      "  batch 14 loss: 0.0007600535755045712\n",
      "  batch 16 loss: 0.0010021695925388485\n",
      "  batch 18 loss: 0.0010714450327213854\n",
      "  batch 20 loss: 0.0010902812937274575\n",
      "  batch 22 loss: 0.0006759393436368555\n",
      "  batch 24 loss: 0.0006584127258975059\n",
      "  batch 26 loss: 0.0006367157911881804\n",
      "  batch 28 loss: 0.000671722402330488\n",
      "  batch 30 loss: 0.0005367574049159884\n",
      "  batch 32 loss: 0.000638869620161131\n",
      "  batch 34 loss: 0.0005536871613003314\n",
      "  batch 36 loss: 0.0003962235787184909\n",
      "  batch 38 loss: 0.0004081934748683125\n",
      "  batch 40 loss: 0.000329015267197974\n",
      "  batch 42 loss: 0.00045237936137709767\n",
      "  batch 44 loss: 0.0004205049917800352\n",
      "  batch 46 loss: 0.00039793072210159153\n",
      "  batch 48 loss: 0.0007627718732692301\n",
      "  batch 50 loss: 0.0004655261291190982\n",
      "  batch 52 loss: 0.00034382233570795506\n",
      "  batch 54 loss: 0.000382626531063579\n",
      "  batch 56 loss: 0.0004019269108539447\n",
      "  batch 58 loss: 0.00030892102222424\n",
      "  batch 60 loss: 0.0004305543698137626\n",
      "  batch 62 loss: 0.0004434561706148088\n",
      "  batch 64 loss: 0.00044303981121629477\n",
      "  batch 66 loss: 0.0004041345964651555\n",
      "  batch 68 loss: 0.0003584798105293885\n",
      "  batch 70 loss: 0.0005398580979090184\n",
      "  batch 72 loss: 0.0006268801807891577\n",
      "  batch 74 loss: 0.0006396294047590345\n",
      "  batch 76 loss: 0.000591317075304687\n",
      "LOSS train 0.000591317075304687 valid 0.0006483051693066955\n",
      "LOSS train 0.000591317075304687 valid 0.0005153624224476516\n",
      "LOSS train 0.000591317075304687 valid 0.0005091620841994882\n",
      "LOSS train 0.000591317075304687 valid 0.0005103993462398648\n",
      "LOSS train 0.000591317075304687 valid 0.0005137062980793417\n",
      "LOSS train 0.000591317075304687 valid 0.000519986730068922\n",
      "LOSS train 0.000591317075304687 valid 0.0005375233013182878\n",
      "LOSS train 0.000591317075304687 valid 0.0005384625401347876\n",
      "LOSS train 0.000591317075304687 valid 0.0005650303792208433\n",
      "LOSS train 0.000591317075304687 valid 0.0005600994918495417\n",
      "LOSS train 0.000591317075304687 valid 0.0005691871047019958\n",
      "LOSS train 0.000591317075304687 valid 0.0005970929050818086\n",
      "LOSS train 0.000591317075304687 valid 0.0006118253222666681\n",
      "LOSS train 0.000591317075304687 valid 0.0006023996393196285\n",
      "LOSS train 0.000591317075304687 valid 0.0006252143066376448\n",
      "LOSS train 0.000591317075304687 valid 0.0006247275741770864\n",
      "LOSS train 0.000591317075304687 valid 0.0006234601023606956\n",
      "LOSS train 0.000591317075304687 valid 0.0006340710679069161\n",
      "LOSS train 0.000591317075304687 valid 0.0006385335582308471\n",
      "EPOCH 10:\n",
      "  batch 2 loss: 0.0006744104321114719\n",
      "  batch 4 loss: 0.0007490937714464962\n",
      "  batch 6 loss: 0.0006052757380530238\n",
      "  batch 8 loss: 0.00045874934585299343\n",
      "  batch 10 loss: 0.0009095118439290673\n",
      "  batch 12 loss: 0.000954870687564835\n",
      "  batch 14 loss: 0.0007586012652609497\n",
      "  batch 16 loss: 0.0009898049174807966\n",
      "  batch 18 loss: 0.0011129408376291394\n",
      "  batch 20 loss: 0.0011296499869786203\n",
      "  batch 22 loss: 0.0006657237536273897\n",
      "  batch 24 loss: 0.0006205393874552101\n",
      "  batch 26 loss: 0.0005795675097033381\n",
      "  batch 28 loss: 0.0006457369308918715\n",
      "  batch 30 loss: 0.0004971504822606221\n",
      "  batch 32 loss: 0.0006263683026190847\n",
      "  batch 34 loss: 0.0005540002894122154\n",
      "  batch 36 loss: 0.0004023735091323033\n",
      "  batch 38 loss: 0.00040868834184948355\n",
      "  batch 40 loss: 0.00033355303457938135\n",
      "  batch 42 loss: 0.00044284010073170066\n",
      "  batch 44 loss: 0.00039039464900270104\n",
      "  batch 46 loss: 0.0003859508578898385\n",
      "  batch 48 loss: 0.0007767596689518541\n",
      "  batch 50 loss: 0.00048000647802837193\n",
      "  batch 52 loss: 0.00034814867831300944\n",
      "  batch 54 loss: 0.0003575546870706603\n",
      "  batch 56 loss: 0.0003959091700380668\n",
      "  batch 58 loss: 0.0003018588467966765\n",
      "  batch 60 loss: 0.0004125681152800098\n",
      "  batch 62 loss: 0.0004187015147181228\n",
      "  batch 64 loss: 0.00044446621905080974\n",
      "  batch 66 loss: 0.00039285598904825747\n",
      "  batch 68 loss: 0.00036091392394155264\n",
      "  batch 70 loss: 0.000496451641083695\n",
      "  batch 72 loss: 0.0006139080505818129\n",
      "  batch 74 loss: 0.0006330507749225944\n",
      "  batch 76 loss: 0.0005828580760862678\n",
      "LOSS train 0.0005828580760862678 valid 0.0006426515756174922\n",
      "LOSS train 0.0005828580760862678 valid 0.0005129435448907316\n",
      "LOSS train 0.0005828580760862678 valid 0.0005185769405215979\n",
      "LOSS train 0.0005828580760862678 valid 0.0005127437761984766\n",
      "LOSS train 0.0005828580760862678 valid 0.0005079071270301938\n",
      "LOSS train 0.0005828580760862678 valid 0.0005173740792088211\n",
      "LOSS train 0.0005828580760862678 valid 0.0005324477679096162\n",
      "LOSS train 0.0005828580760862678 valid 0.0005314720328897238\n",
      "LOSS train 0.0005828580760862678 valid 0.0005578540731221437\n",
      "LOSS train 0.0005828580760862678 valid 0.0005537588149309158\n",
      "LOSS train 0.0005828580760862678 valid 0.0005636464338749647\n",
      "LOSS train 0.0005828580760862678 valid 0.0005867013242095709\n",
      "LOSS train 0.0005828580760862678 valid 0.0006004396127536893\n",
      "LOSS train 0.0005828580760862678 valid 0.0005914019420742989\n",
      "LOSS train 0.0005828580760862678 valid 0.0006192454602569342\n",
      "LOSS train 0.0005828580760862678 valid 0.0006232949090190232\n",
      "LOSS train 0.0005828580760862678 valid 0.0006250757724046707\n",
      "LOSS train 0.0005828580760862678 valid 0.000635009550023824\n",
      "LOSS train 0.0005828580760862678 valid 0.0006399717531166971\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "writer_step = 2\n",
    "\n",
    "\n",
    "def main(execute):\n",
    "    \n",
    "    if execute:\n",
    "        model = build_unet(num_channels=NUM_CHANNELS).to(device)\n",
    "        #summary(model, (3,256,256))\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "        def train_one_epoch(epoch_index, tb_writer):\n",
    "            \n",
    "            running_loss = 0\n",
    "            last_loss = 0\n",
    "\n",
    "            for i, batch in enumerate(train_dataloader):\n",
    "                inputs, labels, _ = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(inputs)\n",
    "                loss = loss_fn(preds, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if i % writer_step == 1:\n",
    "                    last_loss = running_loss / writer_step # loss per batch\n",
    "                    print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                    tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "                    tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                    running_loss = 0.\n",
    "            return last_loss\n",
    "\n",
    "\n",
    "\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "        epoch_number = 0\n",
    "\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "            model.train(True)\n",
    "            avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "            model.train(False)\n",
    "            with torch.no_grad():\n",
    "                running_vloss = 0.0\n",
    "                for i, vdata in enumerate(validation_dataloader):\n",
    "                    vinputs, vlabels, _ = vdata\n",
    "                    vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "                    voutputs = model(vinputs)\n",
    "                    vloss = loss_fn(voutputs, vlabels)\n",
    "                    running_vloss += vloss\n",
    "\n",
    "                    avg_vloss = running_vloss / (i + 1)\n",
    "                    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "                    # Log the running loss averaged per batch\n",
    "                    # for both training and validation\n",
    "                    writer.add_scalars('Training vs. Validation Loss',\n",
    "                                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                                    epoch_number + 1)\n",
    "                    writer.flush()\n",
    "            epoch_number += 1\n",
    "\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        tree = os.walk(cwd, topdown=True)\n",
    "\n",
    "        for root, dirs, files in tree:\n",
    "            for name in files:\n",
    "                if '.pth' in os.path.join(cwd, name):\n",
    "                    os.remove(os.path.join(cwd, name))\n",
    "                    break\n",
    "            break\n",
    "\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path + '.pth')\n",
    "\n",
    "main(EXECUTION_MODEL_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "colormap12 = {(0,0,170) : 0, #background\n",
    "            (0,0,255) : 1, #facade\n",
    "            (0,85,255): 2, #window\n",
    "            (0,170,255): 3, #door\n",
    "            (255,85,0): 4, #cornice\n",
    "            (85,255,170): 5, #sill\n",
    "            (170,255,85): 6, #balcony\n",
    "            (255,255,0): 7, #blind\n",
    "            (255,170,0): 8, #deco\n",
    "            (0,255,255): 9, #molding\n",
    "            (255,0,0): 10, #pillar\n",
    "            (170,0,0): 11 #shop \n",
    "                            }\n",
    "colormap2 = {   (0,0,170) : 0, #background\n",
    "                (0,85,255): 1 #window    \n",
    "                            }\n",
    "\n",
    "tree = os.walk(cwd, topdown=True)\n",
    "for root, dirs, files in tree:\n",
    "    for name in files:\n",
    "        if '.pth' in os.path.join(cwd, name):\n",
    "            model_path = os.path.join(cwd, name)\n",
    "            break\n",
    "    break\n",
    "\n",
    "model = build_unet(num_channels=NUM_CHANNELS).to('cuda')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "def decode_to_RGB2(model, image_path):\n",
    "    image_test_path = os.path.join(cwd, image_path) \n",
    "    my_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                                        ])\n",
    "    image_test = Image.open(image_test_path).convert('RGB')\n",
    "    image_test = np.array(image_test.resize((TARGET_IMAGE_WIDTH,TARGET_IMAGE_HEIGHT)))\n",
    "    image_test  = my_transforms(image_test)\n",
    "    image_test  = image_test.clone().detach().cuda()\n",
    "\n",
    "    image_test .to(device)\n",
    "\n",
    "    output = model(image_test .unsqueeze(0))\n",
    "\n",
    "    sm = nn.Softmax2d()\n",
    "    output = sm(output).squeeze(0)\n",
    "    #output = torch.argmax(output, dim=0)\n",
    "    output = torch.permute(output, (1,2,0)) #swap axes of tensor\n",
    "    image_classes = output.cpu().detach().numpy()\n",
    "\n",
    "    image_output = np.zeros(shape=(TARGET_IMAGE_WIDTH,TARGET_IMAGE_HEIGHT,3)).astype(np.uint8)\n",
    "    for W in range(image_classes.shape[0]):\n",
    "        for H in range(image_classes.shape[1]):\n",
    "            if image_classes[W,H,0] >= 0.9:\n",
    "                image_output[W,H,0] = 0\n",
    "                image_output[W,H,1] = 0\n",
    "                image_output[W,H,2] = 255\n",
    "                #image[W,H,0] = [k for k, v in colormap.items() if v == pixel_class+1][0][0]\n",
    "            else:\n",
    "                image_output[W,H,0] = 255\n",
    "                image_output[W,H,1] = 0\n",
    "                image_output[W,H,2] = 0\n",
    "\n",
    "    image_output = Image.fromarray(image_output)\n",
    "    image_output.save('test_predicted_mask.png')\n",
    "\n",
    "\n",
    "def decode_to_RGB12(model, image_path, colormap):\n",
    "    image_test_path = os.path.join(cwd, image_path) \n",
    "    my_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                                        ])\n",
    "\n",
    "    image_test = Image.open(image_test_path).convert('RGB')\n",
    "    image_test = np.array(image_test.resize((TARGET_IMAGE_WIDTH,TARGET_IMAGE_HEIGHT)))\n",
    "    image_test  = my_transforms(image_test)\n",
    "    image_test  = image_test.clone().detach().cuda()\n",
    "\n",
    "    image_test .to(device)\n",
    "\n",
    "    output = model(image_test .unsqueeze(0))\n",
    "\n",
    "    sm = nn.Softmax2d()\n",
    "    output = sm(output).squeeze(0)\n",
    "    #output = torch.argmax(output, dim=0)\n",
    "    output = torch.permute(output, (1,2,0)) #swap axes of tensor\n",
    "    image_classes = output.cpu().detach().numpy()\n",
    "\n",
    "    image_output = np.zeros(shape=(TARGET_IMAGE_WIDTH,TARGET_IMAGE_HEIGHT,3)).astype(np.uint8)\n",
    "    for W in range(image_classes.shape[0]):\n",
    "        for H in range(image_classes.shape[1]):\n",
    "            for C in range(image_classes.shape[2]):\n",
    "                if image_classes[H,W,C] >= 0.9:\n",
    "                    image_output[H,W,0] = [k[0] for k, v in colormap.items() if v == C][0]\n",
    "                    image_output[H,W,1] = [k[1] for k, v in colormap.items() if v == C][0]\n",
    "                    image_output[H,W,2] = [k[2] for k, v in colormap.items() if v == C][0]\n",
    "\n",
    "    image_output = Image.fromarray(image_output)\n",
    "    image_output.save('test_predicted_mask.png')\n",
    "\n",
    "if NUM_CHANNELS == 2:\n",
    "    decode_to_RGB2(model, 'test_image.jpg')\n",
    "else:\n",
    "    decode_to_RGB12(model, 'test_image.jpg', colormap12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of windows #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "test_image_path = os.path.join(cwd, 'test_predicted_mask.png')\n",
    "\n",
    "\n",
    "def count_windows(test_img_path):\n",
    "    if os.path.exists(test_image_path):\n",
    "        imgray = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        imgrgb = cv2.imread(test_image_path)\n",
    "\n",
    "        # get contours\n",
    "        contours = measure.find_contours(imgray, fully_connected='high')\n",
    "\n",
    "        # get contours length\n",
    "        contour_length_list = []\n",
    "        for contour in contours:\n",
    "            contour_length = 0\n",
    "            for i in range(len(contour)-1):\n",
    "                contour_length += cv2.norm(contour[i], contour[i+1], cv2.NORM_L2)\n",
    "            contour_length_list.append(contour_length)\n",
    "\n",
    "        max_countour_length = max(contour_length_list)\n",
    "        # remove countours with length less than 20% of mean length of all countours\n",
    "        windows_count = len([length for length in contour_length_list if length > max_countour_length/2])\n",
    "        return windows_count\n",
    "\n",
    "windows = count_windows(test_image_path)\n",
    "print(windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axarr = plt.subplots(2, sharex=True)\n",
    "# axarr[0].imshow(np.array(cv2.imread(os.path.join(cwd, 'test_image.jpg'))))\n",
    "# axarr[1].imshow(np.array(cv2.imread(os.path.join(cwd, 'test_predicted_mask.jpg'))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "876187d4a2c995a73645928887c465872e44a13f37aecb236f2b7209f9d185bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
